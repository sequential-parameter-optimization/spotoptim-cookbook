{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Physics-Informed Neural Networks (PINNs) Demo 1\"\n",
        "subtitle: \"Solving ODEs with SpotOptim's LinearRegressor\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Overview\n",
        "\n",
        "This tutorial demonstrates how to use Physics-Informed Neural Networks (PINNs) to solve ordinary differential equations (ODEs) using SpotOptim's `LinearRegressor` class. We'll solve a first-order ODE with an initial condition, combining data-driven learning with physics constraints.\n",
        "\n",
        "## The Differential Equation\n",
        "\n",
        "We want to solve the following ODE:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dt} + 0.1 y - \\sin\\left(\\frac{\\pi t}{2}\\right) = 0\n",
        "$$\n",
        "\n",
        "with initial condition:\n",
        "\n",
        "$$\n",
        "y(0) = 0\n",
        "$$\n",
        "\n",
        "# Setup\n",
        "\n",
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Neural Network\n",
        "\n",
        "We'll use SpotOptim's `LinearRegressor` class to create a neural network with:\n",
        "\n",
        "- 1 input (time t)\n",
        "- 1 output (solution y)\n",
        "- 32 neurons per hidden layer\n",
        "- 3 hidden layers\n",
        "- Tanh activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = LinearRegressor(\n",
        "    input_dim=1,\n",
        "    output_dim=1,\n",
        "    l1=32,\n",
        "    num_hidden_layers=3,\n",
        "    activation=\"Tanh\",\n",
        "    lr=1.0\n",
        ")\n",
        "\n",
        "# Get optimizer with custom learning rate\n",
        "optimizer = model.get_optimizer(\"Adam\", lr=3.0)  # 3.0 * 0.001 = 0.003\n",
        "\n",
        "print(f\"Model architecture:\")\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Training Data\n",
        "\n",
        "We'll generate exact solution data using a numerical solver (RK2 method):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def oscillator(\n",
        "    n_steps: int = 3000,\n",
        "    t_min: float = 0.0,\n",
        "    t_max: float = 30.0,\n",
        "    y0: float = 0.0,\n",
        "    alpha: float = 0.1,\n",
        "    omega: float = np.pi / 2\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Solve ODE: dy/dt + alpha*y - sin(omega*t) = 0\n",
        "    using RK2 (midpoint method).\n",
        "    \n",
        "    Args:\n",
        "        n_steps: Number of time steps\n",
        "        t_min: Start time\n",
        "        t_max: End time\n",
        "        y0: Initial condition y(t_min)\n",
        "        alpha: Damping coefficient\n",
        "        omega: Forcing frequency\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (time points, solution values) as torch tensors\n",
        "    \"\"\"\n",
        "    t_step = (t_max - t_min) / n_steps\n",
        "    t_points = np.arange(t_min, t_min + n_steps * t_step, t_step)[:n_steps]\n",
        "    \n",
        "    y = [y0]\n",
        "    \n",
        "    for t_current_step_end in t_points[1:]:\n",
        "        t_midpoint = t_current_step_end - t_step / 2.0\n",
        "        y_prev = y[-1]\n",
        "        \n",
        "        # Stage 1: intermediate value\n",
        "        slope_at_t_mid = -alpha * y_prev + np.sin(omega * t_midpoint)\n",
        "        y_intermediate = y_prev + (t_step / 2.0) * slope_at_t_mid\n",
        "        \n",
        "        # Stage 2: final value\n",
        "        slope_at_t_end = -alpha * y_intermediate + np.sin(omega * t_current_step_end)\n",
        "        y_next = y_prev + t_step * slope_at_t_end\n",
        "        y.append(y_next)\n",
        "    \n",
        "    t_tensor = torch.tensor(t_points, dtype=torch.float32).view(-1, 1)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "    \n",
        "    return t_tensor, y_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the exact solution and sample training data points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate exact solution (3000 points)\n",
        "x, y = oscillator()\n",
        "\n",
        "# Sample training data (every 119th point, giving ~25 points)\n",
        "x_data = x[0:3000:119]\n",
        "y_data = y[0:3000:119]\n",
        "\n",
        "print(f\"Exact solution points: {len(x)}\")\n",
        "print(f\"Training data points: {len(x_data)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the exact solution and training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x.numpy(), y.numpy(), 'b-', linewidth=2, label=\"Exact solution y(t)\")\n",
        "plt.scatter(x_data.numpy(), y_data.numpy(), color=\"tab:orange\", \n",
        "            s=50, label=\"Training data\", zorder=5)\n",
        "plt.xlabel(\"Time t\", fontsize=12)\n",
        "plt.ylabel(\"Solution y(t)\", fontsize=12)\n",
        "plt.title(\"ODE Solution and Training Data\", fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collocation Points\n",
        "\n",
        "Create collocation points where we'll enforce the physics (ODE) constraints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 50 evenly spaced points in [0, 30]\n",
        "x_physics = torch.linspace(0, 30, 50).view(-1, 1).requires_grad_(True)\n",
        "\n",
        "print(f\"Collocation points: {len(x_physics)}\")\n",
        "print(f\"Range: [{x_physics.min().item():.2f}, {x_physics.max().item():.2f}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PINN Training\n",
        "\n",
        "Now we'll train the neural network using two loss components:\n",
        "\n",
        "1. **Data Loss (Loss1)**: Ensures the network fits the training data\n",
        "2. **Physics Loss (Loss2)**: Ensures the network satisfies the ODE\n",
        "\n",
        "## Loss Components Explained\n",
        "\n",
        "### Neural Network Prediction on Data Points\n",
        "\n",
        "The network predicts values at the training data points:\n",
        "\n",
        "```python\n",
        "yh = model(x_data)\n",
        "```\n",
        "\n",
        "### Data Loss (Loss1)\n",
        "\n",
        "Mean squared error between predictions and actual data:\n",
        "\n",
        "```python\n",
        "loss1 = torch.mean((yh - y_data)**2)\n",
        "```\n",
        "\n",
        "### Neural Network Prediction on Collocation Points\n",
        "\n",
        "The network predicts values at the physics enforcement points:\n",
        "\n",
        "```python\n",
        "yhp = model(x_physics)\n",
        "```\n",
        "\n",
        "### Computing Derivatives for the ODE\n",
        "\n",
        "We compute dy/dt using automatic differentiation:\n",
        "\n",
        "```python\n",
        "dyhp_dxphysics = torch.autograd.grad(\n",
        "    yhp, x_physics, \n",
        "    torch.ones_like(yhp), \n",
        "    create_graph=True\n",
        ")[0]\n",
        "```\n",
        "\n",
        "### Physics Loss (Loss2)\n",
        "\n",
        "The ODE residual at collocation points:\n",
        "\n",
        "$$\n",
        "\\text{residual} = \\frac{dy}{dt} + 0.1 y - \\sin\\left(\\frac{\\pi t}{2}\\right)\n",
        "$$\n",
        "\n",
        "```python\n",
        "physics = dyhp_dxphysics + 0.1 * yhp - torch.sin(np.pi * x_physics / 2)\n",
        "loss2 = torch.mean(physics**2)\n",
        "```\n",
        "\n",
        "### Total Loss\n",
        "\n",
        "Combined loss with weighting factor α:\n",
        "\n",
        "```python\n",
        "loss = loss1 + alpha * loss2\n",
        "```\n",
        "\n",
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_history_pinn = []\n",
        "loss2_history_pinn = []\n",
        "plot_data_points_pinn = []\n",
        "\n",
        "alpha = 6e-2  # Weight for physics loss\n",
        "n_epochs = 48000\n",
        "\n",
        "print(\"Training Physics-Informed Neural Network...\")\n",
        "print(f\"Total epochs: {n_epochs}\")\n",
        "print(f\"Physics loss weight (alpha): {alpha}\")\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Data Loss: Fit the training data\n",
        "    yh = model(x_data)\n",
        "    loss1 = torch.mean((yh - y_data)**2)\n",
        "    \n",
        "    # Physics Loss: Satisfy the ODE at collocation points\n",
        "    yhp = model(x_physics)\n",
        "    dyhp_dxphysics = torch.autograd.grad(\n",
        "        yhp, x_physics, \n",
        "        torch.ones_like(yhp), \n",
        "        create_graph=True\n",
        "    )[0]\n",
        "    physics = dyhp_dxphysics + 0.1 * yhp - torch.sin(np.pi * x_physics / 2)\n",
        "    loss2 = torch.mean(physics**2)\n",
        "    \n",
        "    # Total Loss\n",
        "    loss = loss1 + alpha * loss2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Store history every 100 steps\n",
        "    if (i + 1) % 100 == 0:\n",
        "        loss_history_pinn.append(loss.detach())\n",
        "        loss2_history_pinn.append(loss2.detach())\n",
        "    \n",
        "    # Store snapshots for visualization every 10000 steps\n",
        "    if (i + 1) % 10000 == 0:\n",
        "        current_yh_full = model(x).detach()\n",
        "        plot_data_points_pinn.append({\n",
        "            'yh': current_yh_full, \n",
        "            'step': i + 1\n",
        "        })\n",
        "        print(f\"Epoch {i+1}/{n_epochs}: Total Loss = {loss.item():.6f}, \"\n",
        "              f\"Physics Loss = {loss2.item():.6f}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Visualization\n",
        "\n",
        "## Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot total loss\n",
        "ax1.plot(loss_history_pinn, 'b-', linewidth=1.5)\n",
        "ax1.set_xlabel('Iteration (×100)', fontsize=11)\n",
        "ax1.set_ylabel('Total Loss', fontsize=11)\n",
        "ax1.set_title('Training Progress: Total Loss', fontsize=12)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Plot physics loss\n",
        "ax2.plot(loss2_history_pinn, 'r-', linewidth=1.5)\n",
        "ax2.set_xlabel('Iteration (×100)', fontsize=11)\n",
        "ax2.set_ylabel('Physics Loss', fontsize=11)\n",
        "ax2.set_title('Training Progress: Physics Loss (ODE Residual)', fontsize=12)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution Evolution During Training\n",
        "\n",
        "Visualize how the neural network solution evolves during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xp_plot = x_physics.detach()\n",
        "\n",
        "for plot_info in plot_data_points_pinn:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot exact solution\n",
        "    plt.plot(x.numpy(), y.numpy(), 'b-', linewidth=2, \n",
        "             label='Exact solution', alpha=0.7)\n",
        "    \n",
        "    # Plot training data\n",
        "    plt.scatter(x_data.numpy(), y_data.numpy(), color='tab:orange', \n",
        "                s=50, label='Training data', zorder=5)\n",
        "    \n",
        "    # Plot neural network prediction\n",
        "    plt.plot(x.numpy(), plot_info['yh'].numpy(), 'r--', linewidth=2,\n",
        "             label='PINN prediction', alpha=0.8)\n",
        "    \n",
        "    # Plot collocation points\n",
        "    plt.scatter(xp_plot.numpy(), \n",
        "                model(xp_plot).detach().numpy(),\n",
        "                color='green', marker='x', s=30, \n",
        "                label='Collocation points', alpha=0.6, zorder=4)\n",
        "    \n",
        "    plt.xlabel('Time t', fontsize=12)\n",
        "    plt.ylabel('Solution y(t)', fontsize=12)\n",
        "    plt.title(f'PINN Solution at Epoch {plot_info[\"step\"]}', fontsize=14)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Solution Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_final = model(x)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot exact solution\n",
        "plt.plot(x.numpy(), y.numpy(), 'b-', linewidth=2.5, \n",
        "         label='Exact solution', alpha=0.8)\n",
        "\n",
        "# Plot PINN prediction\n",
        "plt.plot(x.numpy(), y_final.numpy(), 'r--', linewidth=2, \n",
        "         label='PINN prediction', alpha=0.8)\n",
        "\n",
        "# Plot training data\n",
        "plt.scatter(x_data.numpy(), y_data.numpy(), color='tab:orange', \n",
        "            s=80, label='Training data', zorder=5, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Plot collocation points\n",
        "plt.scatter(x_physics.detach().numpy(), \n",
        "            model(x_physics).detach().numpy(),\n",
        "            color='green', marker='x', s=50, \n",
        "            label='Collocation points', alpha=0.7, zorder=4)\n",
        "\n",
        "plt.xlabel('Time t', fontsize=13)\n",
        "plt.ylabel('Solution y(t)', fontsize=13)\n",
        "plt.title('Final PINN Solution vs Exact Solution', fontsize=15, fontweight='bold')\n",
        "plt.legend(fontsize=12, loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute absolute error\n",
        "error = torch.abs(y_final - y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x.numpy(), error.numpy(), 'r-', linewidth=2)\n",
        "plt.xlabel('Time t', fontsize=12)\n",
        "plt.ylabel('Absolute Error |y_exact - y_PINN|', fontsize=12)\n",
        "plt.title('PINN Approximation Error', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nError Statistics:\")\n",
        "print(f\"Maximum absolute error: {error.max().item():.6f}\")\n",
        "print(f\"Mean absolute error: {error.mean().item():.6f}\")\n",
        "print(f\"Root mean squared error: {torch.sqrt(torch.mean(error**2)).item():.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "This tutorial demonstrated how to use SpotOptim's `LinearRegressor` class to implement a Physics-Informed Neural Network (PINN) for solving ordinary differential equations. Key takeaways:\n",
        "\n",
        "1. **Network Architecture**: Used a 3-layer neural network with 32 neurons per layer and Tanh activation\n",
        "2. **Dual Loss Function**: Combined data fitting loss with physics constraint loss\n",
        "3. **Automatic Differentiation**: Leveraged PyTorch's autograd to compute derivatives for the ODE\n",
        "4. **Collocation Method**: Enforced the ODE at specific points in the domain\n",
        "5. **Training Strategy**: Balanced data-driven and physics-informed learning with weight α\n",
        "\n",
        "The PINN successfully learned to approximate the ODE solution using only sparse training data (~25 points) by incorporating the underlying physics through the differential equation constraint.\n",
        "\n",
        "## Key Advantages of PINNs\n",
        "\n",
        "- **Data Efficiency**: Can learn with very few data points\n",
        "- **Physics Consistency**: Solutions automatically satisfy the governing equations\n",
        "- **Generalization**: Better extrapolation beyond training data\n",
        "- **Flexibility**: Can handle complex geometries and boundary conditions\n",
        "\n",
        "## Using SpotOptim for Hyperparameter Optimization\n",
        "\n",
        "The `LinearRegressor` class integrates seamlessly with SpotOptim for hyperparameter tuning:\n",
        "\n",
        "```python\n",
        "from spotoptim import SpotOptim\n",
        "\n",
        "def train_pinn(X):\n",
        "    \"\"\"Objective function for hyperparameter optimization.\"\"\"\n",
        "    results = []\n",
        "    for params in X:\n",
        "        l1 = int(params[0])           # Hidden layer size\n",
        "        num_layers = int(params[1])   # Number of layers\n",
        "        lr_unified = 10 ** params[2]  # Learning rate (log scale)\n",
        "        \n",
        "        # Create model\n",
        "        model = LinearRegressor(\n",
        "            input_dim=1, output_dim=1,\n",
        "            l1=l1, num_hidden_layers=num_layers,\n",
        "            activation=\"Tanh\", lr=lr_unified\n",
        "        )\n",
        "        \n",
        "        # Train PINN and compute validation error\n",
        "        # ... training code ...\n",
        "        \n",
        "        results.append(validation_error)\n",
        "    return np.array(results)\n",
        "\n",
        "# Optimize hyperparameters\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_pinn,\n",
        "    bounds=[(16, 128), (1, 5), (-4, 0)],\n",
        "    var_type=[\"int\", \"int\", \"num\"],\n",
        "    var_name=[\"layer_size\", \"num_layers\", \"log_lr\"],\n",
        "    max_iter=50\n",
        ")\n",
        "result = optimizer.optimize()\n",
        "```\n",
        "\n",
        "This approach allows you to systematically find the best network architecture and learning rate for your specific PINN problem."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}