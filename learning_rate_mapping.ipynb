{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Learning Rate Mapping for Unified Optimizer Interface\n",
        "sidebar_position: 5\n",
        "eval: true\n",
        "---\n",
        "\n",
        "\n",
        "SpotOptim provides a sophisticated learning rate mapping system through the `map_lr()` function, enabling a unified interface for learning rates across different PyTorch optimizers. This solves the challenge that different optimizers operate on vastly different learning rate scales.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Different PyTorch optimizers use different default learning rates and optimal ranges:\n",
        "\n",
        "- **Adam**: default 0.001, typical range 0.0001-0.01\n",
        "- **SGD**: default 0.01, typical range 0.001-0.1\n",
        "- **RMSprop**: default 0.01, typical range 0.001-0.1\n",
        "\n",
        "This makes it difficult to compare optimizer performance fairly or optimize learning rates across different optimizers. The `map_lr()` function provides a unified scale where **`lr_unified=1.0` corresponds to each optimizer's PyTorch default**.\n",
        "\n",
        "**Module**: `spotoptim.utils.mapping`\n",
        "\n",
        "**Key Features**:\n",
        "\n",
        "- Unified learning rate scale across all optimizers\n",
        "- Fair comparison when evaluating different optimizers\n",
        "- Simplified hyperparameter optimization\n",
        "- Based on official PyTorch default learning rates\n",
        "- Supports 13 major PyTorch optimizers\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "### Basic Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: basic-usage\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "\n",
        "# Get optimizer-specific learning rate from unified scale\n",
        "lr_adam = map_lr(1.0, \"Adam\")      # Returns 0.001 (Adam's default)\n",
        "lr_sgd = map_lr(1.0, \"SGD\")        # Returns 0.01 (SGD's default)\n",
        "lr_rmsprop = map_lr(1.0, \"RMSprop\")  # Returns 0.01 (RMSprop's default)\n",
        "\n",
        "print(f\"Unified lr=1.0:\")\n",
        "print(f\"  Adam:    {lr_adam}\")\n",
        "print(f\"  SGD:     {lr_sgd}\")\n",
        "print(f\"  RMSprop: {lr_rmsprop}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: scaling-learning-rates\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "\n",
        "# Scale all learning rates by the same factor\n",
        "unified_lr = 0.5\n",
        "\n",
        "lr_adam = map_lr(unified_lr, \"Adam\")      # 0.5 * 0.001 = 0.0005\n",
        "lr_sgd = map_lr(unified_lr, \"SGD\")        # 0.5 * 0.01 = 0.005\n",
        "lr_rmsprop = map_lr(unified_lr, \"RMSprop\")  # 0.5 * 0.01 = 0.005\n",
        "\n",
        "print(f\"Unified lr={unified_lr}:\")\n",
        "print(f\"  Adam:    {lr_adam}\")\n",
        "print(f\"  SGD:     {lr_sgd}\")\n",
        "print(f\"  RMSprop: {lr_rmsprop}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integration with LinearRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: linear-regressor-integration\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "# Create model with unified learning rate\n",
        "model = LinearRegressor(\n",
        "    input_dim=10, \n",
        "    output_dim=1, \n",
        "    l1=32, \n",
        "    num_hidden_layers=2,\n",
        "    lr=1.0  # Unified learning rate\n",
        ")\n",
        "\n",
        "# Get optimizer - automatically uses mapped learning rate\n",
        "optimizer_adam = model.get_optimizer(\"Adam\")     # Gets 1.0 * 0.001 = 0.001\n",
        "optimizer_sgd = model.get_optimizer(\"SGD\")       # Gets 1.0 * 0.01 = 0.01\n",
        "\n",
        "# Verify the actual learning rates\n",
        "print(f\"Adam actual lr: {optimizer_adam.param_groups[0]['lr']}\")\n",
        "print(f\"SGD actual lr: {optimizer_sgd.param_groups[0]['lr']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function Reference\n",
        "\n",
        "### `map_lr(lr_unified, optimizer_name, use_default_scale=True)`\n",
        "\n",
        "Maps a unified learning rate to an optimizer-specific learning rate.\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "- `lr_unified` (float): Unified learning rate multiplier. A value of 1.0 corresponds to the optimizer's default learning rate. Typical range: [0.001, 100.0].\n",
        "- `optimizer_name` (str): Name of the PyTorch optimizer. Must be one of: \"Adadelta\", \"Adagrad\", \"Adam\", \"AdamW\", \"SparseAdam\", \"Adamax\", \"ASGD\", \"LBFGS\", \"NAdam\", \"RAdam\", \"RMSprop\", \"Rprop\", \"SGD\".\n",
        "- `use_default_scale` (bool, optional): Whether to scale by the optimizer's default learning rate. If `True` (default), `lr_unified` is multiplied by the default lr. If `False`, returns `lr_unified` directly.\n",
        "\n",
        "**Returns**:\n",
        "\n",
        "- `float`: The optimizer-specific learning rate.\n",
        "\n",
        "**Raises**:\n",
        "\n",
        "- `ValueError`: If `optimizer_name` is not supported.\n",
        "- `ValueError`: If `lr_unified` is not positive.\n",
        "\n",
        "**Example**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: map-lr-example\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "\n",
        "# Get default learning rates (unified lr = 1.0)\n",
        "lr = map_lr(1.0, \"Adam\")      # 0.001\n",
        "lr = map_lr(1.0, \"SGD\")       # 0.01\n",
        "lr = map_lr(1.0, \"RMSprop\")   # 0.01\n",
        "\n",
        "# Scale learning rates\n",
        "lr = map_lr(0.5, \"Adam\")      # 0.0005\n",
        "lr = map_lr(2.0, \"SGD\")       # 0.02\n",
        "\n",
        "# Without default scaling\n",
        "lr = map_lr(0.01, \"Adam\", use_default_scale=False)  # 0.01 (direct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supported Optimizers\n",
        "\n",
        "All major PyTorch optimizers are supported with their default learning rates:\n",
        "\n",
        "| Optimizer | Default LR | Typical Range | Notes |\n",
        "|-----------|------------|---------------|-------|\n",
        "| **Adam** | 0.001 | 0.0001-0.01 | Most popular, good default |\n",
        "| **AdamW** | 0.001 | 0.0001-0.01 | Adam with weight decay |\n",
        "| **Adamax** | 0.002 | 0.0001-0.01 | Adam variant with infinity norm |\n",
        "| **NAdam** | 0.002 | 0.0001-0.01 | Adam with Nesterov momentum |\n",
        "| **RAdam** | 0.001 | 0.0001-0.01 | Rectified Adam |\n",
        "| **SparseAdam** | 0.001 | 0.0001-0.01 | For sparse gradients |\n",
        "| **SGD** | 0.01 | 0.001-0.1 | Classic, needs momentum |\n",
        "| **RMSprop** | 0.01 | 0.001-0.1 | Good for RNNs |\n",
        "| **Adagrad** | 0.01 | 0.001-0.1 | Adaptive learning rate |\n",
        "| **Adadelta** | 1.0 | 0.1-10.0 | Extension of Adagrad |\n",
        "| **ASGD** | 0.01 | 0.001-0.1 | Averaged SGD |\n",
        "| **LBFGS** | 1.0 | 0.1-10.0 | Second-order optimizer |\n",
        "| **Rprop** | 0.01 | 0.001-0.1 | Resilient backpropagation |\n",
        "\n",
        "## Use Cases\n",
        "\n",
        "### Comparing Different Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: optimizer-comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# Load data\n",
        "train_loader, test_loader, _ = get_diabetes_dataloaders(batch_size=32, random_state=42)\n",
        "\n",
        "# Test different optimizers with unified learning rate\n",
        "unified_lr = 1.0\n",
        "optimizers_to_test = [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"]\n",
        "results = {}\n",
        "\n",
        "for opt_name in optimizers_to_test:\n",
        "    # Reset for fair comparison\n",
        "    torch.manual_seed(42)\n",
        "    model = LinearRegressor(input_dim=10, output_dim=1, l1=32, \n",
        "                           num_hidden_layers=2, lr=unified_lr)\n",
        "    \n",
        "    # Create optimizer with mapped learning rate\n",
        "    if opt_name == \"SGD\":\n",
        "        optimizer = model.get_optimizer(opt_name, momentum=0.9)\n",
        "    else:\n",
        "        optimizer = model.get_optimizer(opt_name)\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    for epoch in range(50):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_X)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            predictions = model(batch_X)\n",
        "            test_loss += criterion(predictions, batch_y).item()\n",
        "    \n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    results[opt_name] = avg_test_loss\n",
        "    \n",
        "    print(f\"{opt_name:10s}: Test MSE = {avg_test_loss:.4f} \"\n",
        "          f\"(actual lr = {optimizer.param_groups[0]['lr']:.6f})\")\n",
        "\n",
        "# Find best optimizer\n",
        "best_opt = min(results, key=results.get)\n",
        "print(f\"\\nBest optimizer: {best_opt} with MSE = {results[best_opt]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Optimization with SpotOptim\n",
        "\n",
        "Note, `N_INITIAL` and `MAX_ITER` are kept small for demonstration; increase for real use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: hyperparameter-optimization\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "MAX_ITER = 10\n",
        "N_INITIAL = 5\n",
        "\n",
        "def train_and_evaluate(X):\n",
        "    \"\"\"Objective function for hyperparameter optimization.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Load data once\n",
        "    train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
        "        batch_size=32, random_state=42\n",
        "    )\n",
        "    \n",
        "    for params in X:\n",
        "        # Extract hyperparameters\n",
        "        lr_unified = 10 ** params[0]  # Log scale\n",
        "        optimizer_name = params[1]     # Factor variable\n",
        "        l1 = int(params[2])           # Integer\n",
        "        num_layers = int(params[3])   # Integer\n",
        "        \n",
        "        # Create model with unified learning rate\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=l1,\n",
        "            num_hidden_layers=num_layers,\n",
        "            lr=lr_unified  # Automatically mapped per optimizer\n",
        "        )\n",
        "        \n",
        "        # Get optimizer (lr already mapped internally)\n",
        "        if optimizer_name == \"SGD\":\n",
        "            optimizer = model.get_optimizer(optimizer_name, momentum=0.9)\n",
        "        else:\n",
        "            optimizer = model.get_optimizer(optimizer_name)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # Train\n",
        "        model.train()\n",
        "        for epoch in range(30):\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "        results.append(avg_test_loss)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Optimize learning rate, optimizer choice, and architecture\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_and_evaluate,\n",
        "    bounds=[\n",
        "        (-4, 0),                           # log10(lr_unified): [0.0001, 1.0]\n",
        "        (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),  # Optimizer choice\n",
        "        (16, 128),                         # Layer size\n",
        "        (1, 3)                             # Number of hidden layers\n",
        "    ],\n",
        "    var_type=[\"num\", \"factor\", \"int\", \"int\"],\n",
        "    max_iter=MAX_ITER,\n",
        "    n_initial=N_INITIAL,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "\n",
        "# Display results\n",
        "print(\"\\nOptimization Results:\")\n",
        "print(f\"Best unified lr: {10**result.x[0]:.6f}\")\n",
        "print(f\"Best optimizer: {result.x[1]}\")\n",
        "print(f\"Best layer size: {int(result.x[2])}\")\n",
        "print(f\"Best num layers: {int(result.x[3])}\")\n",
        "print(f\"Best test MSE: {result.fun:.4f}\")\n",
        "\n",
        "# Show actual learning rate used\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "actual_lr = map_lr(10**result.x[0], result.x[1])\n",
        "print(f\"Actual {result.x[1]} learning rate: {actual_lr:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log-Scale Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: log-scale-search\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "import numpy as np\n",
        "\n",
        "# Common pattern: sample unified lr from log scale\n",
        "log_lr_range = np.linspace(-4, 0, 10)  # [-4, -3.56, ..., 0]\n",
        "optimizers = [\"Adam\", \"SGD\", \"RMSprop\"]\n",
        "\n",
        "print(\"Log-scale learning rate search:\")\n",
        "print()\n",
        "print(f\"{'log_lr':<10} {'unified_lr':<12} {'Adam':<12} {'SGD':<12} {'RMSprop':<12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for log_lr in log_lr_range:\n",
        "    lr_unified = 10 ** log_lr\n",
        "    lr_adam = map_lr(lr_unified, \"Adam\")\n",
        "    lr_sgd = map_lr(lr_unified, \"SGD\")\n",
        "    lr_rmsprop = map_lr(lr_unified, \"RMSprop\")\n",
        "    \n",
        "    print(f\"{log_lr:<10.2f} {lr_unified:<12.6f} {lr_adam:<12.8f} \"\n",
        "          f\"{lr_sgd:<12.8f} {lr_rmsprop:<12.8f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Learning Rate Schedules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: custom-lr-schedules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "\n",
        "# Create model with unified lr\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n",
        "\n",
        "# Get initial optimizer\n",
        "optimizer = model.get_optimizer(\"Adam\")\n",
        "initial_lr = optimizer.param_groups[0]['lr']\n",
        "print(f\"Initial learning rate: {initial_lr}\")\n",
        "\n",
        "# Use PyTorch learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training with scheduler\n",
        "for epoch in range(100):\n",
        "    # ... training code ...\n",
        "    scheduler.step()\n",
        "    \n",
        "    if (epoch + 1) % 30 == 0:\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}: lr = {current_lr:.8f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Direct Usage Without LinearRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: direct-usage-without-linearregressor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "\n",
        "# Define your own model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "# Use map_lr to get optimizer-specific learning rate\n",
        "unified_lr = 2.0\n",
        "optimizer_name = \"Adam\"\n",
        "\n",
        "actual_lr = map_lr(unified_lr, optimizer_name)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=actual_lr)\n",
        "\n",
        "print(f\"Unified lr: {unified_lr}\")\n",
        "print(f\"Actual {optimizer_name} lr: {actual_lr}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### Choosing Unified Learning Rate\n",
        "\n",
        "**For initial experiments:**\n",
        "\n",
        "- Start with `lr=1.0` (gives defaults for all optimizers)\n",
        "- Test with `lr=0.1`, `lr=1.0`, `lr=10.0` to get a sense of scale\n",
        "\n",
        "**For hyperparameter optimization:**\n",
        "\n",
        "- Use log scale: sample from `[-4, 0]` or `[-3, 1]`\n",
        "- Convert with `lr_unified = 10 ** log_lr`\n",
        "- This gives reasonable ranges for all optimizers\n",
        "\n",
        "**For fine-tuning:**\n",
        "\n",
        "- If training is unstable: try smaller `lr` (e.g., 0.1 or 0.5)\n",
        "- If training is too slow: try larger `lr` (e.g., 2.0 or 5.0)\n",
        "- Monitor loss curves to adjust\n",
        "\n",
        "### Optimizer Selection Guidelines\n",
        "\n",
        "**Adam family (Adam, AdamW, NAdam, RAdam):**\n",
        "\n",
        "- ✅ Good default choice for most tasks\n",
        "- ✅ Adaptive learning rates per parameter\n",
        "- ✅ Works well out of the box\n",
        "- Use `lr=1.0` as starting point\n",
        "\n",
        "**SGD:**\n",
        "\n",
        "- ✅ Good for large datasets\n",
        "- ✅ Often achieves better generalization\n",
        "- ⚠️ Requires momentum (e.g., 0.9)\n",
        "- Use `lr=1.0` with momentum=0.9\n",
        "\n",
        "**RMSprop:**\n",
        "\n",
        "- ✅ Good for recurrent networks\n",
        "- ✅ Handles non-stationary objectives\n",
        "- Use `lr=1.0` as starting point\n",
        "\n",
        "**Others (Adadelta, Adagrad, etc.):**\n",
        "\n",
        "- Specialized use cases\n",
        "- Start with `lr=1.0` and adjust\n",
        "\n",
        "### Common Patterns\n",
        "\n",
        "```python\n",
        "# Pattern 1: Quick optimizer comparison\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n",
        "for opt in [\"Adam\", \"SGD\", \"RMSprop\"]:\n",
        "    optimizer = model.get_optimizer(opt)\n",
        "    # ... train and compare ...\n",
        "\n",
        "# Pattern 2: Hyperparameter optimization\n",
        "def objective(X):\n",
        "    lr_unified = 10 ** X[:, 0]  # Log scale\n",
        "    optimizer_name = X[:, 1]     # Factor\n",
        "    # ... use unified lr ...\n",
        "\n",
        "# Pattern 3: Override model's lr\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n",
        "optimizer = model.get_optimizer(\"Adam\", lr=2.0)  # Override with 2.0\n",
        "\n",
        "# Pattern 4: Direct mapping\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "lr_actual = map_lr(unified_lr, optimizer_name)\n",
        "optimizer = torch.optim.Adam(params, lr=lr_actual)\n",
        "```\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Issue: Training is unstable (loss explodes)\n",
        "\n",
        "**Solution**: Learning rate is too high. Try:\n",
        "```python\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=0.1)  # Reduce from 1.0\n",
        "```\n",
        "\n",
        "### Issue: Training is too slow (loss decreases very slowly)\n",
        "\n",
        "**Solution**: Learning rate is too low. Try:\n",
        "```python\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=5.0)  # Increase from 1.0\n",
        "```\n",
        "\n",
        "### Issue: Different results across optimizer runs\n",
        "\n",
        "**Solution**: Set random seed for reproducibility:\n",
        "```python\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "```\n",
        "\n",
        "### Issue: Want to use raw learning rate without mapping\n",
        "\n",
        "**Solution**: Use `use_default_scale=False`:\n",
        "```python\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "lr = map_lr(0.001, \"Adam\", use_default_scale=False)  # Returns 0.001 directly\n",
        "```\n",
        "\n",
        "### Issue: Optimizer not supported\n",
        "\n",
        "**Solution**: Check supported optimizers:\n",
        "```python\n",
        "from spotoptim.utils.mapping import OPTIMIZER_DEFAULT_LR\n",
        "print(\"Supported optimizers:\", list(OPTIMIZER_DEFAULT_LR.keys()))\n",
        "```\n",
        "\n",
        "## Technical Details\n",
        "\n",
        "### How It Works\n",
        "\n",
        "The mapping is simple but effective:\n",
        "\n",
        "```\n",
        "actual_lr = lr_unified * default_lr[optimizer_name]\n",
        "```\n",
        "\n",
        "For example:\n",
        "\n",
        "- `map_lr(1.0, \"Adam\")` → `1.0 * 0.001` = `0.001`\n",
        "- `map_lr(0.5, \"SGD\")` → `0.5 * 0.01` = `0.005`\n",
        "- `map_lr(2.0, \"RMSprop\")` → `2.0 * 0.01` = `0.02`\n",
        "\n",
        "This ensures that the same unified learning rate gives optimizer-specific learning rates in their typical working ranges.\n",
        "\n",
        "### Design Rationale\n",
        "\n",
        "**Why use defaults as scaling factors?**\n",
        "\n",
        "PyTorch's default learning rates are carefully chosen to work well for typical use cases. By using them as scaling factors:\n",
        "\n",
        "1. `lr=1.0` always gives sensible defaults\n",
        "2. Scaling preserves the relative relationships between optimizers\n",
        "3. Each optimizer stays in its optimal range\n",
        "4. Easy to understand and explain\n",
        "\n",
        "**Comparison with spotPython's approach:**\n",
        "\n",
        "spotPython uses `lr = lr_mult * default_lr` in `optimizer_handler()`. Our implementation:\n",
        "\n",
        "- ✅ Separates mapping logic (testable, reusable)\n",
        "- ✅ Provides standalone function (`map_lr()`)\n",
        "- ✅ Comprehensive error handling and validation\n",
        "- ✅ Extensive documentation and examples\n",
        "- ✅ Full integration with `LinearRegressor`\n",
        "\n",
        "### Default Learning Rates\n",
        "\n",
        "All values verified against [PyTorch documentation](https://pytorch.org/docs/stable/optim.html):\n",
        "\n",
        "```python\n",
        "OPTIMIZER_DEFAULT_LR = {\n",
        "    \"Adadelta\": 1.0,\n",
        "    \"Adagrad\": 0.01,\n",
        "    \"Adam\": 0.001,\n",
        "    \"AdamW\": 0.001,\n",
        "    \"SparseAdam\": 0.001,\n",
        "    \"Adamax\": 0.002,\n",
        "    \"ASGD\": 0.01,\n",
        "    \"LBFGS\": 1.0,\n",
        "    \"NAdam\": 0.002,\n",
        "    \"RAdam\": 0.001,\n",
        "    \"RMSprop\": 0.01,\n",
        "    \"Rprop\": 0.01,\n",
        "    \"SGD\": 0.01,\n",
        "}\n",
        "```\n",
        "\n",
        "## Examples\n",
        "\n",
        "### Complete Example: Optimizer Comparison Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Complete example: Compare optimizers with unified learning rate interface.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load data\n",
        "train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
        "    batch_size=32, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Test configurations\n",
        "optimizers = [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"]\n",
        "unified_lrs = [0.5, 1.0, 2.0]\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "print(\"Training models with different optimizers and learning rates...\")\n",
        "print()\n",
        "\n",
        "for unified_lr in unified_lrs:\n",
        "    results[unified_lr] = {}\n",
        "    \n",
        "    for opt_name in optimizers:\n",
        "        # Reset model for fair comparison\n",
        "        torch.manual_seed(42)\n",
        "        \n",
        "        # Create model with unified lr\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10, \n",
        "            output_dim=1, \n",
        "            l1=32, \n",
        "            num_hidden_layers=2,\n",
        "            lr=unified_lr\n",
        "        )\n",
        "        \n",
        "        # Get optimizer\n",
        "        if opt_name == \"SGD\":\n",
        "            optimizer = model.get_optimizer(opt_name, momentum=0.9)\n",
        "        else:\n",
        "            optimizer = model.get_optimizer(opt_name)\n",
        "        \n",
        "        actual_lr = optimizer.param_groups[0]['lr']\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # Track training loss\n",
        "        train_losses = []\n",
        "        \n",
        "        # Train\n",
        "        model.train()\n",
        "        for epoch in range(50):\n",
        "            epoch_loss = 0.0\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "            train_losses.append(avg_epoch_loss)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "        results[unified_lr][opt_name] = {\n",
        "            'train_losses': train_losses,\n",
        "            'test_loss': avg_test_loss,\n",
        "            'actual_lr': actual_lr\n",
        "        }\n",
        "        \n",
        "        print(f\"Unified lr={unified_lr:.1f}, {opt_name:10s}: \"\n",
        "              f\"actual_lr={actual_lr:.6f}, test_MSE={avg_test_loss:.4f}\")\n",
        "\n",
        "# Display summary\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"Summary: Best configurations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for unified_lr in unified_lrs:\n",
        "    best_opt = min(results[unified_lr].items(), \n",
        "                   key=lambda x: x[1]['test_loss'])\n",
        "    opt_name, metrics = best_opt\n",
        "    \n",
        "    print(f\"Unified lr={unified_lr:.1f}: {opt_name:10s} \"\n",
        "          f\"(test MSE={metrics['test_loss']:.4f}, \"\n",
        "          f\"actual lr={metrics['actual_lr']:.6f})\")\n",
        "\n",
        "# Find overall best\n",
        "best_overall = None\n",
        "best_overall_loss = float('inf')\n",
        "\n",
        "for unified_lr in unified_lrs:\n",
        "    for opt_name, metrics in results[unified_lr].items():\n",
        "        if metrics['test_loss'] < best_overall_loss:\n",
        "            best_overall_loss = metrics['test_loss']\n",
        "            best_overall = (unified_lr, opt_name, metrics['actual_lr'])\n",
        "\n",
        "print()\n",
        "print(f\"Overall best: unified_lr={best_overall[0]:.1f}, \"\n",
        "      f\"optimizer={best_overall[1]}, \"\n",
        "      f\"test_MSE={best_overall_loss:.4f}\")\n",
        "print(f\"Actual learning rate used: {best_overall[2]:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See Also\n",
        "\n",
        "- [LinearRegressor Documentation](../api/linear_regressor.md) - Neural network class with lr parameter\n",
        "- [Diabetes Dataset Utilities](diabetes_dataset.md) - Data loading for examples\n",
        "- [Hyperparameter Optimization](../tutorials/hyperparameter_optimization.md) - Using map_lr with SpotOptim\n",
        "- [PyTorch Optimizer Documentation](https://pytorch.org/docs/stable/optim.html) - Official PyTorch reference\n",
        "\n",
        "## References\n",
        "\n",
        "- Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv:1412.6980.\n",
        "- Loshchilov, I., & Hutter, F. (2017). Decoupled weight decay regularization. arXiv:1711.05101.\n",
        "- PyTorch Team. (2023). PyTorch Optimizer Documentation. https://pytorch.org/docs/stable/optim.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}