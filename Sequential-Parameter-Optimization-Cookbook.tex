% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  10pt,
  a4paperpaper,
]{scrbook}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{mdframed}
\usepackage{pseudo}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Sequential Parameter Optimization Cookbook},
  pdfauthor={Thomas Bartz-Beielstein},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Sequential Parameter Optimization Cookbook}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{A guide for scikit-learn and PyTorch}
\author{Thomas Bartz-Beielstein}
\date{Nov 16, 2025}
\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\part{Optimization}

\chapter{Aircraft Wing Weight Example}\label{sec-awwe}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  This section is based on chapter 1.3 ``A ten-variable weight
  function'' in (\textbf{Forr08a?}).
\item
  The following Python packages are imported:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ mpl\_toolkits.axes\_grid1 }\ImportTok{import}\NormalTok{ ImageGrid}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{AWWE Equation}\label{awwe-equation}

\begin{itemize}
\tightlist
\item
  Example from (\textbf{Forr08a?})
\item
  Understand the \textbf{weight} of an unpainted light aircraft wing as
  a function of nine design and operational parameters:
\end{itemize}

\[ W = 0.036 S_W^{0.758} \times W_{fw}^{0.0035} \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times  q^{0.006}  \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49}\]

\section{AWWE Parameters and Equations (Part
1)}\label{awwe-parameters-and-equations-part-1}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1392}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.5063}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1266}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1139}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1139}}@{}}
\caption{Aircraft Wing Weight Parameters}\label{tbl-awwe}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Baseline
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Minimum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maximum
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(S_W\) & Wing area (\(ft^2\)) & 174 & 150 & 200 \\
\(W_{fw}\) & Weight of fuel in wing (lb) & 252 & 220 & 300 \\
\(A\) & Aspect ratio & 7.52 & 6 & 10 \\
\(\Lambda\) & Quarter-chord sweep (deg) & 0 & -10 & 10 \\
\(q\) & Dynamic pressure at cruise (\(lb/ft^2\)) & 34 & 16 & 45 \\
\(\lambda\) & Taper ratio & 0.672 & 0.5 & 1 \\
\(R_{tc}\) & Aerofoil thickness to chord ratio & 0.12 & 0.08 & 0.18 \\
\(N_z\) & Ultimate load factor & 3.8 & 2.5 & 6 \\
\(W_{dg}\) & Flight design gross weight (lb) & 2000 & 1700 & 2500 \\
\(W_p\) & paint weight (lb/ft\^{}2) & 0.064 & 0.025 & 0.08 \\
\end{longtable}

The study begins with a baseline Cessna C172 Skyhawk Aircraft as its
reference point. It aims to investigate the impact of wing area and fuel
weight on the overall weight of the aircraft. Two crucial parameters in
this analysis are the aspect ratio (\(A\)), defined as the ratio of the
wing's length to the average chord (thickness of the airfoil), and the
taper ratio (\(\lambda\)), which represents the ratio of the maximum to
the minimum thickness of the airfoil or the maximum to minimum chord.

It's important to note that the equation used in this context is not a
computer simulation but will be treated as one for the purpose of
illustration. This approach involves employing a true mathematical
equation, even if it's considered unknown, as a useful tool for
generating realistic settings to test the methodology. The functional
form of this equation was derived by ``calibrating'' known physical
relationships to curves obtained from existing aircraft data, as
referenced in (\textbf{raym06a?}). Essentially, it acts as a surrogate
for actual measurements of aircraft weight.

Examining the mathematical properties of the AWWE (Aircraft Weight With
Wing Area and Fuel Weight Equation), it is evident that the response is
highly nonlinear concerning its inputs. While it's common to apply the
logarithm to simplify equations with complex exponents, even when
modeling the logarithm, which transforms powers into slope coefficients
and products into sums, the response remains nonlinear due to the
presence of trigonometric terms. Given the combination of nonlinearity
and high input dimension, simple linear and quadratic response surface
approximations are likely to be inadequate for this analysis.

\section{Goals: Understanding and
Optimization}\label{goals-understanding-and-optimization}

The primary goals of this study revolve around understanding and
optimization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Understanding}: One of the straightforward objectives is to
  gain a deep understanding of the input-output relationships in this
  context. Given the global perspective implied by this setting, it
  becomes evident that a more sophisticated model is almost necessary.
  At this stage, let's focus on this specific scenario to establish a
  clear understanding.
\item
  \textbf{Optimization}: Another application of this analysis could be
  optimization. There may be an interest in minimizing the weight of the
  aircraft, but it's likely that there will be constraints in place. For
  example, the presence of wings with a nonzero area is essential for
  the aircraft to be capable of flying. In situations involving
  (constrained) optimization, a global perspective and, consequently,
  the use of flexible modeling are vital.
\end{enumerate}

The provided Python code serves as a genuine computer implementation
that ``solves'' a mathematical model. It accepts arguments encoded in
the unit cube, with defaults used to represent baseline settings, as
detailed in the table labeled as Table~\ref{tbl-awwe}. To map values
from the interval \([a, b]\) to the interval \([0, 1]\), the following
formula can be employed:

\begin{equation}\phantomsection\label{eq-mapping}{
y = f(x) = \frac{x - a}{b - a}.
}\end{equation} To reverse this mapping and obtain the original values,
the formula \begin{equation}\phantomsection\label{eq-reverse}{
g(y) = a + (b - a) y
}\end{equation} can be used. The function \texttt{wingwt()} expects
inputs from the unit cube, which are then transformed back to their
original scales using Equation~\ref{eq-reverse}. The function is defined
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ wingwt(Sw}\OperatorTok{=}\FloatTok{0.48}\NormalTok{, Wfw}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, A}\OperatorTok{=}\FloatTok{0.38}\NormalTok{, L}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, q}\OperatorTok{=}\FloatTok{0.62}\NormalTok{, l}\OperatorTok{=}\FloatTok{0.344}\NormalTok{,  Rtc}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, Nz}\OperatorTok{=}\FloatTok{0.37}\NormalTok{, Wdg}\OperatorTok{=}\FloatTok{0.38}\NormalTok{):}
    \CommentTok{\# put coded inputs back on natural scale}
\NormalTok{    Sw }\OperatorTok{=}\NormalTok{ Sw }\OperatorTok{*}\NormalTok{ (}\DecValTok{200} \OperatorTok{{-}} \DecValTok{150}\NormalTok{) }\OperatorTok{+} \DecValTok{150} 
\NormalTok{    Wfw }\OperatorTok{=}\NormalTok{ Wfw }\OperatorTok{*}\NormalTok{ (}\DecValTok{300} \OperatorTok{{-}} \DecValTok{220}\NormalTok{) }\OperatorTok{+} \DecValTok{220} 
\NormalTok{    A }\OperatorTok{=}\NormalTok{ A }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}} \DecValTok{6}\NormalTok{) }\OperatorTok{+} \DecValTok{6} 
\NormalTok{    L }\OperatorTok{=}\NormalTok{ (L }\OperatorTok{*}\NormalTok{ (}\DecValTok{10} \OperatorTok{{-}}\NormalTok{ (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{)) }\OperatorTok{{-}} \DecValTok{10}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.pi}\OperatorTok{/}\DecValTok{180}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ q }\OperatorTok{*}\NormalTok{ (}\DecValTok{45} \OperatorTok{{-}} \DecValTok{16}\NormalTok{) }\OperatorTok{+} \DecValTok{16} 
\NormalTok{    l }\OperatorTok{=}\NormalTok{ l }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+} \FloatTok{0.5}  
\NormalTok{    Rtc }\OperatorTok{=}\NormalTok{ Rtc }\OperatorTok{*}\NormalTok{ (}\FloatTok{0.18} \OperatorTok{{-}} \FloatTok{0.08}\NormalTok{) }\OperatorTok{+} \FloatTok{0.08}
\NormalTok{    Nz }\OperatorTok{=}\NormalTok{ Nz }\OperatorTok{*}\NormalTok{ (}\DecValTok{6} \OperatorTok{{-}} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+} \FloatTok{2.5}
\NormalTok{    Wdg }\OperatorTok{=}\NormalTok{ Wdg}\OperatorTok{*}\NormalTok{(}\DecValTok{2500} \OperatorTok{{-}} \DecValTok{1700}\NormalTok{) }\OperatorTok{+} \DecValTok{1700}
    \CommentTok{\# calculation on natural scale}
\NormalTok{    W }\OperatorTok{=} \FloatTok{0.036} \OperatorTok{*}\NormalTok{ Sw}\OperatorTok{**}\FloatTok{0.758} \OperatorTok{*}\NormalTok{ Wfw}\OperatorTok{**}\FloatTok{0.0035} \OperatorTok{*}\NormalTok{ (A}\OperatorTok{/}\NormalTok{np.cos(L)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\FloatTok{0.6} \OperatorTok{*}\NormalTok{ q}\OperatorTok{**}\FloatTok{0.006} 
\NormalTok{    W }\OperatorTok{=}\NormalTok{ W }\OperatorTok{*}\NormalTok{ l}\OperatorTok{**}\FloatTok{0.04} \OperatorTok{*}\NormalTok{ (}\DecValTok{100}\OperatorTok{*}\NormalTok{Rtc}\OperatorTok{/}\NormalTok{np.cos(L))}\OperatorTok{**}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.3}\NormalTok{) }\OperatorTok{*}\NormalTok{ (Nz}\OperatorTok{*}\NormalTok{Wdg)}\OperatorTok{**}\NormalTok{(}\FloatTok{0.49}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{(W)}
\end{Highlighting}
\end{Shaded}

\section{Properties of the Python
``Solver''}\label{properties-of-the-python-solver}

The compute time required by the ``wingwt'' solver is extremely short
and can be considered trivial in terms of computational resources. The
approximation error is exceptionally small, effectively approaching
machine precision, which indicates the high accuracy of the solver's
results.

To simulate time-consuming evaluations, a deliberate delay is introduced
by incorporating a \texttt{sleep(3600)} command, which effectively
synthesizes a one-hour execution time for a particular evaluation.

Moving on to the AWWE visualization, plotting in two dimensions is
considerably simpler than dealing with nine dimensions. To aid in
creating visual representations, the code provided below establishes a
grid within the unit square to facilitate the generation of sliced
visuals. This involves generating a ``meshgrid'' as outlined in the
code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zp }\OperatorTok{=} \BuiltInTok{zip}\NormalTok{(np.ravel(X), np.ravel(Y))}
\BuiltInTok{list}\NormalTok{(zp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[(np.float64(0.0), np.float64(0.0)),
 (np.float64(0.5), np.float64(0.0)),
 (np.float64(1.0), np.float64(0.0)),
 (np.float64(0.0), np.float64(0.5)),
 (np.float64(0.5), np.float64(0.5)),
 (np.float64(1.0), np.float64(0.5)),
 (np.float64(0.0), np.float64(1.0)),
 (np.float64(0.5), np.float64(1.0)),
 (np.float64(1.0), np.float64(1.0))]
\end{verbatim}

The coding used to transform inputs from natural units is largely a
matter of taste, so long as it's easy to undo for reporting back on
original scales

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{matplotlib inline}
\CommentTok{\# plt.style.use(\textquotesingle{}seaborn{-}white\textquotesingle{})}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Plot 1: Load Factor (\(N_z\)) and Aspect Ratio
(\(A\))}{Plot 1: Load Factor (N\_z) and Aspect Ratio (A)}}\label{plot-1-load-factor-n_z-and-aspect-ratio-a}

We will vary \(N_z\) and \(A\), with other inputs fixed at their
baseline values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(A }\OperatorTok{=}\NormalTok{ X, Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{7.}\NormalTok{, }\FloatTok{5.}\NormalTok{))}
\NormalTok{plt.contourf(X, Y, z, }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Load factor (Nz) vs. Aspect Ratio (A)"}\NormalTok{)}
\NormalTok{plt.colorbar()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-6-output-1.pdf}}

Contour plots can be refined, e.g., by adding explicit contour lines as
shown in the following figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"A"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Nz"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-7-output-1.pdf}}

The interpretation of the AWWE plot can be summarized as follows:

\begin{itemize}
\tightlist
\item
  The figure displays the weight response as a function of two
  variables, \(N_z\) and \(A\), using an image-contour plot.
\item
  The slight curvature observed in the contours suggests an interaction
  between these two variables.
\item
  Notably, the range of outputs depicted in the figure, spanning from
  approximately 160 to 320, nearly encompasses the entire range of
  outputs observed from various input settings within the full
  9-dimensional input space.
\item
  The plot indicates that aircraft wings tend to be heavier when the
  aspect ratios (\(A\)) are high.
\item
  This observation aligns with the idea that wings are designed to
  withstand and accommodate high gravitational forces (\(g\)-forces,
  large \(N_z\)), and there may be a compounding effect where larger
  values of \(N_z\) contribute to increased wing weight.
\item
  It's plausible that this phenomenon is related to the design
  considerations of fighter jets, which cannot have the efficient and
  lightweight glider-like wings typically found in other types of
  aircraft.
\end{itemize}

\section{Plot 2: Taper Ratio and Fuel
Weight}\label{plot-2-taper-ratio-and-fuel-weight}

\begin{itemize}
\tightlist
\item
  The same experiment for two other inputs, e.g., taper ratio
  \(\lambda\) and fuel weight \(W_{fw}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ wingwt(Wfw }\OperatorTok{=}\NormalTok{ X,  Nz }\OperatorTok{=}\NormalTok{ Y)}
\NormalTok{contours }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, z, }\DecValTok{4}\NormalTok{, colors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.clabel(contours, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"WfW"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"l"}\NormalTok{)}

\NormalTok{plt.imshow(z, extent}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], origin}\OperatorTok{=}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{,}
\NormalTok{           cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
\NormalTok{plt.colorbar()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-8-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Interpretation of Taper Ratio (\(l\)) and Fuel Weight (\(W_{fw}\))

  \begin{itemize}
  \tightlist
  \item
    Apparently, neither input has much effect on wing weight:

    \begin{itemize}
    \tightlist
    \item
      with \(\lambda\) having a marginally greater effect, covering less
      than 4 percent of the span of weights observed in the
      \(A \times N_z\) plane
    \end{itemize}
  \item
    There's no interaction evident in \(\lambda \times W_{fw}\)
  \end{itemize}
\end{itemize}

\section{The Big Picture: Combining all
Variables}\label{the-big-picture-combining-all-variables}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pl }\OperatorTok{=}\NormalTok{ [}\StringTok{"Sw"}\NormalTok{, }\StringTok{"Wfw"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"q"}\NormalTok{, }\StringTok{"l"}\NormalTok{,  }\StringTok{"Rtc"}\NormalTok{, }\StringTok{"Nz"}\NormalTok{, }\StringTok{"Wdg"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Z }\OperatorTok{=}\NormalTok{ []}
\NormalTok{Zlab }\OperatorTok{=}\NormalTok{ []}
\NormalTok{l }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(pl)}
\CommentTok{\# lc = math.comb(l,2)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(l):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, l):}
    \CommentTok{\# for j in range(l):}
        \CommentTok{\# print(pl[i], pl[j])}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ \{pl[i]: X, pl[j]: Y\}}
\NormalTok{        Z.append(wingwt(}\OperatorTok{**}\NormalTok{d))}
\NormalTok{        Zlab.append([pl[i],pl[j]])}
\end{Highlighting}
\end{Shaded}

Now we can generate all 36 combinations, e.g., our first example is
combination \texttt{p\ =\ 19}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{19}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['A', 'Nz']
\end{verbatim}

To help interpret outputs from experiments such as this one---to level
the playing field when comparing outputs from other pairs of
inputs---code below sets up a color palette that can be re-used from one
experiment to the next. We use the arguments \texttt{vmin=180} and
\texttt{vmax\ =360} to implement comparibility

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-12-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Let's plot the second example, taper ratio \(\lambda\) and fuel weight
  \(W_{fw}\)
\item
  This is combination \texttt{11}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{=} \DecValTok{11}
\NormalTok{Zlab[p]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['Wfw', 'l']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.contourf(X, Y, Z[p], }\DecValTok{20}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}jet\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{180}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{360}\NormalTok{)}
\NormalTok{plt.xlabel(Zlab[p][}\DecValTok{0}\NormalTok{])}
\NormalTok{plt.ylabel(Zlab[p][}\DecValTok{1}\NormalTok{])}
\NormalTok{plt.colorbar()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-14-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  Using a global colormap indicates that these variables have minor
  effects on the wing weight.
\item
  Important factors can be detected by visual inspection
\item
  Plotting the Big Picture: we can plot all 36 combinations in one
  figure.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\FloatTok{20.}\NormalTok{, }\FloatTok{20.}\NormalTok{))}
\NormalTok{grid }\OperatorTok{=}\NormalTok{ ImageGrid(fig, }\DecValTok{111}\NormalTok{,  }\CommentTok{\# similar to subplot(111)}
\NormalTok{                 nrows\_ncols}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{),  }\CommentTok{\# creates 2x2 grid of axes}
\NormalTok{                 axes\_pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,  }\CommentTok{\# pad between axes in inch.}
\NormalTok{                 share\_all}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{                 label\_mode}\OperatorTok{=}\StringTok{"all"}\NormalTok{,}
\NormalTok{                 ) }
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ ax, im }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(grid, Z):}
    \CommentTok{\# Iterating over the grid returns the Axes.}
\NormalTok{    ax.set\_xlabel(Zlab[i][}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(Zlab[i][}\DecValTok{1}\NormalTok{])}
    \CommentTok{\# ax.set\_title(Zlab[i][1] + " vs. " + Zlab[i][0])}
\NormalTok{    ax.contourf(X, Y, im, }\DecValTok{30}\NormalTok{, cmap }\OperatorTok{=} \StringTok{"jet"}\NormalTok{,  vmin }\OperatorTok{=} \DecValTok{180}\NormalTok{, vmax }\OperatorTok{=} \DecValTok{360}\NormalTok{)}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ i }\OperatorTok{+} \DecValTok{1}
       
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{002_awwe_files/figure-pdf/cell-15-output-1.pdf}}

\section{AWWE Landscape}\label{awwe-landscape}

\begin{itemize}
\tightlist
\item
  Our Observations

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The load factor \(N_z\), which determines the magnitude of the
    maximum aerodynamic load on the wing, is very active and involved in
    interactions with other variables.
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Classic example: the interaction of \(N_z\) with the aspect ratio
    \(A\) indicates a heavy wing for high aspect ratios and large
    \(g\)-forces
  \item
    This is the reaon why highly manoeuvrable fighter jets cannot have
    very efficient, glider wings)
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Aspect ratio \(A\) and airfoil thickness to chord ratio \(R_{tc}\)
    have nonlinear interactions.
  \item
    Most important variables:
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    Ultimate load factor \(N_z\), wing area \(S_w\), and flight design
    gross weight\(W_{dg}\).
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Little impact: dynamic pressure \(q\), taper ratio \(l\), and
    quarter-chord sweep \(L\).
  \end{enumerate}
\item
  Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Aircraft designers know that the overall weight of the aircraft and
    the wing area must be kept to a minimum
  \item
    the latter usually dictated by constraints such as required stall
    speed, landing distance, turn rate, etc.
  \end{itemize}
\end{itemize}

\section{Summary of the First
Experiments}\label{summary-of-the-first-experiments}

\begin{itemize}
\tightlist
\item
  First, we considered two pairs of inputs, out of 36 total pairs
\item
  Then, the ``Big Picture'':

  \begin{itemize}
  \tightlist
  \item
    For each pair we evaluated \texttt{wingwt} 10,000 times
  \end{itemize}
\item
  Doing the same for all pairs would require 360K evaluations:

  \begin{itemize}
  \tightlist
  \item
    not a reasonable number with a real computer simulation that takes
    any non-trivial amount of time to evaluate
  \item
    Only 1s per evaluation: \(>100\) hours
  \end{itemize}
\item
  Many solvers take minutes/hours/days to execute a single run
\item
  And: three-way interactions?
\item
  Consequence: a different strategy is needed
\end{itemize}

\section{Exercise}\label{exercise}

\subsection{Adding Paint Weight}\label{adding-paint-weight}

\begin{itemize}
\tightlist
\item
  Paint weight is not considered.
\item
  Add Paint Weight \(W_p\) to formula (the updated formula is shown
  below) and update the functions and plots in the notebook.
\end{itemize}

\[ W = 0.036S_W^{0.758} \times W_{fw}^{0.0035} \times \left( \frac{A}{\cos^2 \Lambda} \right)^{0.6} \times q^{0.006} \times \lambda^{0.04} \]
\[ \times \left( \frac{100 R_{tc}}{\cos \Lambda} \right)^{-0.3} \times (N_z W_{dg})^{0.49} + S_w W_p\]

\section{Jupyter Notebook}\label{jupyter-notebook}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/002_awwe.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\part{Numerical Methods}

\chapter{Simulation and Surrogate
Modeling}\label{simulation-and-surrogate-modeling}

\begin{itemize}
\tightlist
\item
  We will consider the interplay between

  \begin{itemize}
  \tightlist
  \item
    mathematical models,
  \item
    numerical approximation,
  \item
    simulation,
  \item
    computer experiments, and
  \item
    field data
  \end{itemize}
\item
  Experimental design will play a key role in our developments, but not
  in the classical regression and response surface methodology sense
\item
  Challenging real-data/real-simulation examples benefiting from modern
  surrogate modeling methodology
\item
  We will consider the classical, response surface methodology (RSM)
  approach, and then move on to more modern approaches
\item
  All approaches are based on surrogates
\end{itemize}

\section{Surrogates}\label{surrogates}

\begin{itemize}
\tightlist
\item
  Gathering data is \textbf{expensive}, and sometimes getting exactly
  the data you want is impossible or unethical
\item
  \textbf{Surrogate}: substitute for the real thing
\item
  In statistics, draws from predictive equations derived from a fitted
  model can act as a surrogate for the data-generating mechanism
\item
  Benefits of the surrogate approach:

  \begin{itemize}
  \tightlist
  \item
    Surrogate could represent a cheaper way to explore relationships,
    and entertain ``what ifs?''
  \item
    Surrogates favor faithful yet pragmatic reproduction of dynamics:

    \begin{itemize}
    \tightlist
    \item
      interpretation,
    \item
      establishing causality, or
    \item
      identification
    \end{itemize}
  \item
    Many numerical simulators are \textbf{deterministic}, whereas field
    observations are noisy or have measurement error
  \end{itemize}
\end{itemize}

\subsection{Costs of Simulation}\label{costs-of-simulation}

\begin{itemize}
\tightlist
\item
  Computer simulations are generally cheaper (but not always!) than
  physical observation
\item
  Some computer simulations can be just as expensive as field
  experimentation, but computer modeling is regarded as easier because:

  \begin{itemize}
  \tightlist
  \item
    the experimental apparatus is better understood
  \item
    more aspects may be controlled.
  \end{itemize}
\end{itemize}

\subsection{Mathematical Models and
Meta-Models}\label{mathematical-models-and-meta-models}

\begin{itemize}
\tightlist
\item
  Use of mathematical models leveraging numerical solvers has been
  commonplace for some time
\item
  Mathematical models became more complex, requiring more resources to
  simulate/solve numerically
\item
  Practitioners increasingly relied on \textbf{meta-models} built off of
  limited simulation campaigns
\end{itemize}

\subsection{Surrogates = Trained
Meta-models}\label{surrogates-trained-meta-models}

\begin{itemize}
\tightlist
\item
  Data collected via expensive computer evaluations tuned flexible
  functional forms that could be used in lieu of further simulation to

  \begin{itemize}
  \tightlist
  \item
    save money or computational resources;
  \item
    cope with an inability to perform future runs (expired licenses,
    off-line or over-impacted supercomputers)
  \end{itemize}
\item
  Trained meta-models became known as \textbf{surrogates}
\end{itemize}

\subsection{Computer Experiments}\label{computer-experiments}

\begin{itemize}
\tightlist
\item
  \textbf{Computer experiment}: design, running, and fitting
  meta-models.

  \begin{itemize}
  \tightlist
  \item
    Like an ordinary statistical experiment, except the data are
    generated by computer codes rather than physical or field
    observations, or surveys
  \end{itemize}
\item
  \textbf{Surrogate modeling} is statistical modeling of computer
  experiments
\end{itemize}

\subsection{Limits of Mathematical
Modeling}\label{limits-of-mathematical-modeling}

\begin{itemize}
\tightlist
\item
  Mathematical biologists, economists and others had reached the limit
  of equilibrium-based mathematical modeling with cute closed-form
  solutions
\item
  \textbf{Stochastic simulations replace deterministic solvers} based on
  FEM, Navier--Stokes or Euler methods
\item
  Agent-based simulation models are used to explore predator-prey
  (Lotka--Voltera) dynamics, spread of disease, management of inventory
  or patients in health insurance markets
\item
  Consequence: the distinction between surrogate and statistical model
  is all but gone
\end{itemize}

\subsection{Why Computer Simulations are
Necessary}\label{why-computer-simulations-are-necessary}

\begin{itemize}
\tightlist
\item
  You can't seed a real community with Ebola and watch what happens
\item
  If there's (real) field data, say on a historical epidemic, further
  experimentation may be almost entirely limited to the mathematical and
  computer modeling side
\item
  Classical statistical methods offer little guidance
\end{itemize}

\subsection{Simulation Requirements}\label{simulation-requirements}

\begin{itemize}
\tightlist
\item
  Simulation should

  \begin{itemize}
  \tightlist
  \item
    enable rich \textbf{diagnostics} to help criticize that models
  \item
    \textbf{understanding} its sensitivity to inputs and other
    configurations
  \item
    providing the ability to \textbf{optimize} and
  \item
    refine both \textbf{automatically} and with expert intervention
  \end{itemize}
\item
  And it has to do all that while remaining \textbf{computationally
  tractable}
\item
  One perspective is so-called \textbf{response surface methods} (RSMs):
\item
  a poster child from industrial statistics' heyday, well before
  information technology became a dominant industry
\end{itemize}

\section{Applications of Surrogate
Models}\label{applications-of-surrogate-models}

The four most common usages of surrogate models are:

\begin{itemize}
\tightlist
\item
  Augmenting Expensive Simulations: Surrogate models act as a `curve
  fit' to approximate the results of expensive simulation codes,
  enabling predictions without rerunning the primary source. This
  provides significant speed improvements while maintaining useful
  accuracy.
\item
  Calibration of Predictive Codes: Surrogates bridge the gap between
  simpler, faster but less accurate models and more accurate, slower
  models. This multi-fidelity approach allows for improved accuracy
  without the full computational expense.
\item
  Handling Noisy or Missing Data: Surrogates smooth out random or
  systematic errors in experimental or computational data, filling gaps
  and revealing overall trends while filtering out extraneous details.
\item
  Data Mining and Insight Generation: Surrogates help identify
  functional relationships between variables and their impact on
  results. They enable engineers to focus on critical variables and
  visualize data trends effectively.
\end{itemize}

\section{DACE and RSM}\label{dace-and-rsm}

Mathematical models implemented in computer codes are used to circumvent
the need for expensive field data collection. These models are
particularly useful when dealing with highly nonlinear response
surfaces, high signal-to-noise ratios (which often involve deterministic
evaluations), and a global scope. As a result, a new approach is
required in comparison to Response Surface Methodology (RSM), which is
discussed in Section~\ref{sec-rsm-intro}.

With the improvement in computing power and simulation fidelity,
researchers gain higher confidence and a better understanding of the
dynamics in physical, biological, and social systems. However, the
expansion of configuration spaces and increasing input dimensions
necessitates more extensive designs. High-performance computing (HPC)
allows for thousands of runs, whereas previously only tens were
possible. This shift towards larger models and training data presents
new computational challenges.

Research questions for DACE (Design and Analysis of Computer
Experiments) include how to design computer experiments that make
efficient use of computation and how to meta-model computer codes to
save on simulation effort. The choice of surrogate model for computer
codes significantly impacts the optimal experiment design, and the
preferred model-design pairs can vary depending on the specific goal.

The combination of computer simulation, design, and modeling with field
data from similar real-world experiments introduces a new category of
computer model tuning problems. The ultimate goal is to automate these
processes to the greatest extent possible, allowing for the deployment
of HPC with minimal human intervention.

One of the remaining differences between RSM and DACE lies in how they
handle noise. DACE employs replication, a technique that would not be
used in a deterministic setting, to separate signal from noise.
Traditional RSM is best suited for situations where a substantial
proportion of the variability in the data is due to noise, and where the
acquisition of data values can be severely limited. Consequently, RSM is
better suited for a different class of problems, aligning with its
intended purposes.

Two very good texts on computer experiments and surrogate modeling are
(\textbf{Sant03a?}) and (\textbf{Forr08a?}). The former is the canonical
reference in the statistics literature and the latter is perhaps more
popular in engineering.

\begin{example}[Example: DACE and
RSM]\protect\hypertarget{exm-dace-rsm}{}\label{exm-dace-rsm}

Imagine you are a chemical engineer tasked with optimizing a chemical
process to maximize yield. You can control temperature and pressure, but
repeated experiments show variability in yield due to inconsistencies in
raw materials.

\begin{itemize}
\item
  Using RSM: You would use RSM to design a series of experiments varying
  temperature and pressure. You would then fit a response surface (a
  mathematical model) to the data, helping you understand how changes in
  temperature and pressure affect yield. Using this model, you can
  identify optimal conditions for maximizing yield despite the noise.
\item
  Using DACE: If instead you use a computational model to simulate the
  chemical process and want to account for numerical noise or
  uncertainty in model parameters, you might use DACE. You would run
  simulations at different conditions, possibly repeating them to assess
  variability and build a surrogate model that accurately predicts
  yields, which can be optimized to find the best conditions.
\end{itemize}

\end{example}

\subsection{Noise Handling in RSM and
DACE}\label{noise-handling-in-rsm-and-dace}

Noise in RSM: In experimental settings, noise often arises due to
variability in experimental conditions, measurement errors, or other
uncontrollable factors. This noise can significantly affect the response
variable, \(Y\). Replication is a standard procedure for handling noise
in RSM. In the context of computer experiments, noise might not be
present in the traditional sense since simulations can be deterministic.
However, variability can arise from uncertainty in input parameters or
model inaccuracies. DACE predominantly utilizes advanced interpolation
to construct accurate models of deterministic data, sometimes
considering statistical noise modeling if needed.

\section{Updating a Surrogate Model}\label{updating-a-surrogate-model}

A surrogate model is updated by incorporating new data points, known as
infill points, into the model to improve its accuracy and predictive
capabilities. This process is iterative and involves the following
steps:

\begin{itemize}
\tightlist
\item
  Identify Regions of Interest: The surrogate model is analyzed to
  determine areas where it is inaccurate or where further exploration is
  needed. This could be regions with high uncertainty or areas where the
  model predicts promising results (e.g., potential optima).
\item
  Select Infill Points: Infill points are new data points chosen based
  on specific criteria, such as:
\item
  Exploitation: Sampling near predicted optima to refine the solution.
  Exploration: Sampling in regions of high uncertainty to improve the
  model globally. Balanced Approach: Combining exploitation and
  exploration to ensure both local and global improvements.
\item
  Evaluate the True Function: The true function (e.g., a simulation or
  experiment) is evaluated at the selected infill points to obtain their
  corresponding outputs.
\item
  Update the Surrogate Model: The surrogate model is retrained or
  updated using the new data, including the infill points, to improve
  its accuracy.
\item
  Repeat: The process is repeated until the model meets predefined
  accuracy criteria or the computational budget is exhausted.
\end{itemize}

\begin{definition}[Infill
Points]\protect\hypertarget{def-infill-points}{}\label{def-infill-points}

Infill points are strategically chosen new data points added to the
surrogate model. They are selected to:

\begin{itemize}
\tightlist
\item
  Reduce uncertainty in the model.
\item
  Improve predictions in regions of interest.
\item
  Enhance the model's ability to identify optima or trends.
\end{itemize}

\end{definition}

The selection of infill points is often guided by infill criteria, such
as:

\begin{itemize}
\tightlist
\item
  Expected Improvement (EI): Maximizing the expected improvement over
  the current best solution.
\item
  Uncertainty Reduction: Sampling where the model's predictions have
  high variance.
\item
  Probability of Improvement (PI): Sampling where the probability of
  improving the current best solution is highest.
\end{itemize}

The iterative infill-points updating process ensures that the surrogate
model becomes increasingly accurate and useful for optimization or
decision-making tasks.

\chapter{Sampling Plans}\label{sampling-plans}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  This section is based on chapter 1 in (\textbf{Forr08a?}).
\item
  The following Python packages are imported:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Tuple, Optional}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ rlh}
\ImportTok{from}\NormalTok{ spotpython.utils.effects }\ImportTok{import}\NormalTok{ screening\_plot, screeningplan}
\ImportTok{from}\NormalTok{ spotpython.fun.objectivefunctions }\ImportTok{import}\NormalTok{ Analytical}
\ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ (fullfactorial, bestlh,}
\NormalTok{     jd, mm, mmphi, mmsort, perturb, mmlhs, phisort, mmphi\_intensive)}
\ImportTok{from}\NormalTok{ spotpython.design.poor }\ImportTok{import}\NormalTok{ Poor}
\ImportTok{from}\NormalTok{ spotpython.design.clustered }\ImportTok{import}\NormalTok{ Clustered}
\ImportTok{from}\NormalTok{ spotpython.design.sobol }\ImportTok{import}\NormalTok{ Sobol}
\ImportTok{from}\NormalTok{ spotpython.design.spacefilling }\ImportTok{import}\NormalTok{ SpaceFilling}
\ImportTok{from}\NormalTok{ spotpython.design.random }\ImportTok{import}\NormalTok{ Random}
\ImportTok{from}\NormalTok{ spotpython.design.grid }\ImportTok{import}\NormalTok{ Grid}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Ideas and Concepts}\label{ideas-and-concepts}

\begin{definition}[Sampling
Plan]\protect\hypertarget{def-sampling-plan}{}\label{def-sampling-plan}

In the context of computer experiments, the term sampling plan refers to
the set of input values, say \(X\),at which the computer code is
evaluated.

\end{definition}

The goal of a sampling plan is to efficiently explore the input space to
understand the behavior of the computer code and build a surrogate model
that accurately represents the code's behavior. Traditionally, Response
Surface Methodology (RSM) has been used to design sampling plans for
computer experiments. These sampling plans are based on procedures that
generate points by means of a rectangular grid or a factorial design.

However, more recently, Design and Analysis of Computer Experiments
(DACE) has emerged as a more flexible and powerful approach for
designing sampling plans.

Engineering design often requires the construction of a surrogate model
\(\hat{f}\) to approximate the expensive response of a black-box
function \(f\). The function \(f(x)\) represents a continuous metric
(e.g., quality, cost, or performance) defined over a design space
\(D \subset \mathbb{R}^k\), where \(x\) is a \(k\)-dimensional vector of
design variables. Since evaluating \(f\) is costly, only a sparse set of
samples is used to construct \(\hat{f}\), which can then provide
inexpensive predictions for any \(x \in D\).

The process involves:

\begin{itemize}
\tightlist
\item
  Sampling discrete observations:
\item
  Using these samples to construct an approximation \(\hat{f}\).
\item
  Ensuring the surrogate model is well-posed, meaning it is
  mathematically valid and can generalize predictions effectively.
\end{itemize}

A sampling plan

\[
X =
\left\{
  x^{(i)} \in D | i = 1, \ldots, n 
\right\}
\]

determines the spatial arrangement of observations. While some models
require a minimum number of data points \(n\), once this threshold is
met, a surrogate model can be constructed to approximate \(f\)
efficiently.

A well-posed model does not always perform well because its ability to
generalize depends heavily on the sampling plan used to collect data. If
the sampling plan is poorly designed, the model may fail to capture
critical behaviors in the design space. For example:

\begin{itemize}
\tightlist
\item
  Extreme Sampling: Measuring performance only at the extreme values of
  parameters may miss important behaviors in the center of the design
  space, leading to incomplete understanding.
\item
  Uneven Sampling: Concentrating samples in certain regions while
  neglecting others forces the model to extrapolate over unsampled
  areas, potentially resulting in inaccurate or misleading predictions.
  Additionally, in some cases, the data may come from external sources
  or be limited in scope, leaving little control over the sampling plan.
  This can further restrict the model's ability to generalize
  effectively.
\end{itemize}

\subsection{The `Curse of Dimensionality' and How to Avoid
It}\label{the-curse-of-dimensionality-and-how-to-avoid-it}

The ``curse of dimensionality'' refers to the exponential increase in
computational complexity and data requirements as the number of
dimensions (variables) in a problem grows. For a one-dimensional space,
sampling \(n\) locations may suffice for accurate predictions. In
high-dimensional spaces, the amount of data needed to maintain the same
level of accuracy or coverage increases dramatically. For example, if a
one-dimensional space requires \(n\) samples for a certain accuracy, a
\(k\)-dimensional space would require \(n^k\) samples. This makes tasks
like optimization, sampling, and modeling computationally expensive and
often impractical in high-dimensional settings.

\begin{example}[Example: Curse of
Dimensionality]\protect\hypertarget{exm-curse-of-dim}{}\label{exm-curse-of-dim}

Consider a simple example where we want to model the cost of a car tire
based on its wheel diameter. If we have one variable (wheel diameter),
we might need 10 simulations to get a good estimate of the cost. Now, if
we add 8 more variables (e.g., tread pattern, rubber type, etc.), the
number of simulations required increases to \(10^8\) (10 million). This
is because the number of combinations of design variables grows
exponentially with the number of dimensions. This means that the
computational budget required to evaluate all combinations of design
variables becomes infeasible. In this case, it would take 11,416 years
to complete the simulations, making it impractical to explore the design
space fully.

\end{example}

\subsection{Physical versus Computational
Experiments}\label{physical-versus-computational-experiments}

Physical experiments are prone to experimental errors from three main
sources:

\begin{itemize}
\tightlist
\item
  Human error: Mistakes made by the experimenter.
\item
  Random error: Measurement inaccuracies that vary unpredictably.
\item
  Systematic error: Consistent bias due to flaws in the experimental
  setup.
\end{itemize}

The key distinction is repeatability: systematic errors remain constant
across repetitions, while random errors vary.

Computational experiments, on the other hand, are deterministic and free
from random errors. However, they are still affected by:

\begin{itemize}
\tightlist
\item
  Human error: Bugs in code or incorrect boundary conditions.
\item
  Systematic error: Biases from model simplifications (e.g., inviscid
  flow approximations) or finite resolution (e.g., insufficient mesh
  resolution).
\end{itemize}

The term ``noise'' is used differently in physical and computational
contexts. In physical experiments, it refers to random errors, while in
computational experiments, it often refers to systematic errors.

Understanding these differences is crucial for designing experiments and
applying techniques like Gaussian process-based approximations. For
physical experiments, replication mitigates random errors, but this is
unnecessary for deterministic computational experiments.

\subsection{Designing Preliminary Experiments
(Screening)}\label{designing-preliminary-experiments-screening}

Minimizing the number of design variables \(x_1, x_2, \dots, x_k\) is
crucial before modeling the objective function \(f\). This process,
called screening, aims to reduce dimensionality without compromising the
analysis. If \(f\) is at least once differentiable over the design
domain \(D\), the partial derivative \(\frac{\partial f}{\partial x_i}\)
can be used to classify variables:

\begin{itemize}
\tightlist
\item
  Negligible Variables: If
  \(\frac{\partial f}{\partial x_i} = 0, \, \forall x \in D\), the
  variable \(x_i\) can be safely neglected.
\item
  Linear Additive Variables: If
  \(\frac{\partial f}{\partial x_i} = \text{constant} \neq 0, \, \forall x \in D\),
  the effect of \(x_i\) is linear and additive.
\item
  Nonlinear Variables: If
  \(\frac{\partial f}{\partial x_i} = g(x_i), \, \forall x \in D\),
  where \(g(x_i)\) is a non-constant function, \(f\) is nonlinear in
  \(x_i\).
\item
  Interactive Nonlinear Variables: If
  \(\frac{\partial f}{\partial x_i} = g(x_i, x_j, \dots), /, \forall x \in D\),
  where \(g(x_i, x_j, \dots)\) is a function involving interactions with
  other variables, \(f\) is nonlinear in \(x_i\) and interacts with
  \(x_j\).
\end{itemize}

Measuring \(\frac{\partial f}{\partial x_i}\) across the entire design
space is often infeasible due to limited budgets. The percentage of time
allocated to screening depends on the problem: If many variables are
expected to be inactive, thorough screening can significantly improve
model accuracy by reducing dimensionality. If most variables are
believed to impact the objective, focus should shift to modeling
instead. Screening is a trade-off between computational cost and model
accuracy, and its effectiveness depends on the specific problem context.

\subsubsection{Estimating the Distribution of Elementary
Effects}\label{estimating-the-distribution-of-elementary-effects}

In order to simplify the presentation of what follows, we make, without
loss of generality, the assumption that the design space
\(D = [0, 1]^k\); that is, we normalize all variables into the unit
cube. We shall adhere to this convention for the rest of the book and
strongly urge the reader to do likewise when implementing any algorithms
described here, as this step not only yields clearer mathematics in some
cases but also safeguards against scaling issues.

Before proceeding with the description of the Morris algorithm, we need
to define an important statistical concept. Let us restrict our design
space \(D\) to a \(k\)-dimensional, \(p\)-level full factorial grid,
that is,

\[
x_i \in \{0, \frac{1}{p-1}, \frac{2}{p-1}, \dots, 1\}, \quad \text{ for } i = 1, \dots, k.
\]

\begin{definition}[Elementary
Effect]\protect\hypertarget{def-elementary-effect}{}\label{def-elementary-effect}

For a given baseline value \(x \in D\), let \(d_i(x)\) denote the
elementary effect of \(x_i\), where:

\begin{equation}\phantomsection\label{eq-eleffect}{
d_i(x) = \frac{f(x_1, \dots, x_i + \Delta, \dots, x_k) - f(x_1, \dots, x_i - \Delta, \dots, x_k)}{2\Delta}, \quad i = 1, \dots, k,
}\end{equation} where \(\Delta\) is the step size, which is defined as
the distance between two adjacent levels in the grid. In other words, we
have:

with
\[\Delta = \frac{\xi}{p-1}, \quad \xi \in \mathbb{N}^*, \quad \text{and} \quad x \in D , \text{ such that its components } x_i \leq 1 - \Delta.
\]

\(\Delta\) is the step size. The elementary effect \(d_i(x)\) measures
the sensitivity of the function \(f\) to changes in the variable \(x_i\)
at the point \(x\).

\end{definition}

Morris's method aims to estimate the parameters of the distribution of
elementary effects associated with each variable. A large measure of
central tendency indicates that a variable has a significant influence
on the objective function across the design space, while a large measure
of spread suggests that the variable is involved in interactions or
contributes to the nonlinearity of \(f\). In practice, the sample mean
and standard deviation of a set of \(d_i(x)\) values, calculated in
different parts of the design space, are used for this estimation.

To ensure efficiency, the preliminary sampling plan \(X\) should be
designed so that each evaluation of the objective function \(f\)
contributes to the calculation of two elementary effects, rather than
just one (as would occur with a naive random spread of baseline \(x\)
values and adding \(\Delta\) to one variable). Additionally, the
sampling plan should provide a specified number (e.g., \(r\)) of
elementary effects for each variable, independently drawn with
replacement. For a detailed discussion on constructing such a sampling
plan, readers are encouraged to consult Morris's original paper (Morris,
1991). Here, we focus on describing the process itself.

The random orientation of the sampling plan \(B\) can be constructed as
follows:

\begin{itemize}
\tightlist
\item
  Let \(B\) be a \((k+1) \times k\) matrix of 0s and 1s, where for each
  column \(i\), two rows differ only in their \(i\)-th entries.
\item
  Compute a random orientation of \(B\), denoted \(B^*\):
\end{itemize}

\[
B^* =
\left(
1_{k+1,k} x^* + (\Delta/2) 
\left[
(2B-1_{k+1,k})
D^* +
1_{k+1,k}
\right]
\right)
P^*,
\]

where:

\begin{itemize}
\tightlist
\item
  \(D^*\) is a \(k\)-dimensional diagonal matrix with diagonal elements
  \(\pm 1\) (equal probability),
\item
  \(\mathbf{1}\) is a matrix of 1s,
\item
  \(x^*\) is a randomly chosen point in the \(p\)-level design space
  (limited by \(\Delta\)),
\item
  \(P^*\) is a \(k \times k\) random permutation matrix with one 1 per
  column and row.
\end{itemize}

\texttt{spotpython} provides a \texttt{Python} implementation to compute
\(B^*\), see
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py}.

Here is the corresponding code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ randorient(k, p, xi, seed}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \CommentTok{\# Initialize random number generator with the provided seed}
    \ControlFlowTok{if}\NormalTok{ seed }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}

    \CommentTok{\# Step length}
\NormalTok{    Delta }\OperatorTok{=}\NormalTok{ xi }\OperatorTok{/}\NormalTok{ (p }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}

\NormalTok{    m }\OperatorTok{=}\NormalTok{ k }\OperatorTok{+} \DecValTok{1}

    \CommentTok{\# A truncated p{-}level grid in one dimension}
\NormalTok{    xs }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ Delta, }\DecValTok{1} \OperatorTok{/}\NormalTok{ (p }\OperatorTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{    xsl }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(xs)}
    \ControlFlowTok{if}\NormalTok{ xsl }\OperatorTok{\textless{}} \DecValTok{1}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"xi = }\SpecialCharTok{\{}\NormalTok{xi}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p = }\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Delta = }\SpecialCharTok{\{}\NormalTok{Delta}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p {-} 1 = }\SpecialCharTok{\{}\NormalTok{p }\OperatorTok{{-}} \DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\SpecialStringTok{f"The number of levels xsl is }\SpecialCharTok{\{}\NormalTok{xsl}\SpecialCharTok{\}}\SpecialStringTok{, but it must be greater than 0."}\NormalTok{)}

    \CommentTok{\# Basic sampling matrix}
\NormalTok{    B }\OperatorTok{=}\NormalTok{ np.vstack((np.zeros((}\DecValTok{1}\NormalTok{, k)), np.tril(np.ones((k, k)))))}

    \CommentTok{\# Randomization}

    \CommentTok{\# Matrix with +1s and {-}1s on the diagonal with equal probability}
\NormalTok{    Dstar }\OperatorTok{=}\NormalTok{ np.diag(}\DecValTok{2} \OperatorTok{*}\NormalTok{ rng.integers(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, size}\OperatorTok{=}\NormalTok{k) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}

    \CommentTok{\# Random base value}
\NormalTok{    xstar }\OperatorTok{=}\NormalTok{ xs[rng.integers(}\DecValTok{0}\NormalTok{, xsl, size}\OperatorTok{=}\NormalTok{k)]}

    \CommentTok{\# Permutation matrix}
\NormalTok{    Pstar }\OperatorTok{=}\NormalTok{ np.zeros((k, k))}
\NormalTok{    rp }\OperatorTok{=}\NormalTok{ rng.permutation(k)}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
\NormalTok{        Pstar[i, rp[i]] }\OperatorTok{=} \DecValTok{1}

    \CommentTok{\# A random orientation of the sampling matrix}
\NormalTok{    Bstar }\OperatorTok{=}\NormalTok{ (np.ones((m, }\DecValTok{1}\NormalTok{)) }\OperatorTok{@}\NormalTok{ xstar.reshape(}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{) }\OperatorTok{+}
\NormalTok{        (Delta }\OperatorTok{/} \DecValTok{2}\NormalTok{) }\OperatorTok{*}\NormalTok{ ((}\DecValTok{2} \OperatorTok{*}\NormalTok{ B }\OperatorTok{{-}}\NormalTok{ np.ones((m, k))) }\OperatorTok{@}\NormalTok{ Dstar }\OperatorTok{+}
\NormalTok{        np.ones((m, k)))) }\OperatorTok{@}\NormalTok{ Pstar}

    \ControlFlowTok{return}\NormalTok{ Bstar}
\end{Highlighting}
\end{Shaded}

The code following snippet generates a random orientation of a sampling
matrix \texttt{Bstar} using the \texttt{randorient()} function. The
input parameters are:

\begin{itemize}
\tightlist
\item
  k = 3: The number of design variables (dimensions).
\item
  p = 3: The number of levels in the grid for each variable.
\item
  xi = 1: A parameter used to calculate the step size Delta.
\end{itemize}

Step-size calculation is performed as follows:
\texttt{Delta\ =\ xi\ /\ (p\ -\ 1)\ =\ 1\ /\ (3\ -\ 1)\ =\ 0.5}, which
determines the spacing between levels in the grid.

Next, random sampling matrix construction is computed:

\begin{itemize}
\tightlist
\item
  A truncated grid is created with levels \texttt{{[}0,\ 0.5{]}} (based
  on Delta).
\item
  A basic sampling matrix B is constructed, which is a lower triangular
  matrix with 0s and 1s.
\end{itemize}

Then, randomization is applied:

\begin{itemize}
\tightlist
\item
  \texttt{Dstar}: A diagonal matrix with random entries of +1 or -1.
\item
  \texttt{xstar}: A random starting point from the grid.
\item
  \texttt{Pstar}: A random permutation matrix.
\end{itemize}

Random orientation is applied to the basic sampling matrix \texttt{B} to
create \texttt{Bstar}. This involves scaling, shifting, and permuting
the rows and columns of B.

The final output is the matrix \texttt{Bstar}, which represents a random
orientation of the sampling plan. Each row corresponds to a sampled
point in the design space, and each column corresponds to a design
variable.

\begin{example}[Random Orientation of the Sampling Matrix in
2-D]\protect\hypertarget{exm-randorient-2}{}\label{exm-randorient-2}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OperatorTok{=} \DecValTok{2}
\NormalTok{p }\OperatorTok{=} \DecValTok{3}
\NormalTok{xi }\OperatorTok{=} \DecValTok{1}
\NormalTok{Bstar }\OperatorTok{=}\NormalTok{ randorient(k, p, xi, seed}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Random orientation of the sampling matrix:}\CharTok{\textbackslash{}n}\SpecialCharTok{\{}\NormalTok{Bstar}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Random orientation of the sampling matrix:
[[0.5 0. ]
 [0.  0. ]
 [0.  0.5]]
\end{verbatim}

We can visualize the random orientation of the sampling matrix in 2-D as
shown in Figure~\ref{fig-randorient-2d}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plt.scatter(Bstar[:, }\DecValTok{0}\NormalTok{], Bstar[:, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{50}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Hypercube Points\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(Bstar.shape[}\DecValTok{0}\NormalTok{]):}
\NormalTok{    plt.text(Bstar[i, }\DecValTok{0}\NormalTok{] }\OperatorTok{+} \FloatTok{0.01}\NormalTok{, Bstar[i, }\DecValTok{1}\NormalTok{] }\OperatorTok{+} \FloatTok{0.01}\NormalTok{, }\BuiltInTok{str}\NormalTok{(i), fontsize}\OperatorTok{=}\DecValTok{9}\NormalTok{)}
\NormalTok{plt.xlim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
\NormalTok{plt.ylim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-randorient-2d-output-1.pdf}}

}

\caption{\label{fig-randorient-2d}Random orientation of the sampling
matrix in 2-D. The labels indicate the row index of the points.}

\end{figure}%

\end{example}

\begin{example}[Random Orientation of the Sampling
Matrix]\protect\hypertarget{exm-randorient-3}{}\label{exm-randorient-3}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OperatorTok{=} \DecValTok{3}
\NormalTok{p }\OperatorTok{=} \DecValTok{3}
\NormalTok{xi }\OperatorTok{=} \DecValTok{1}
\NormalTok{Bstar }\OperatorTok{=}\NormalTok{ randorient(k, p, xi)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Random orientation of the sampling matrix:}\CharTok{\textbackslash{}n}\SpecialCharTok{\{}\NormalTok{Bstar}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Random orientation of the sampling matrix:
[[0.  0.  0.5]
 [0.  0.  0. ]
 [0.  0.5 0. ]
 [0.5 0.5 0. ]]
\end{verbatim}

\end{example}

To obtain \(r\) elementary effects for each variable, the screening plan
is built from \(r\) random orientations:

\[
X = 
\begin{pmatrix}
B^*_1 \\
B^*_2 \\
\vdots \\
B^*_r
\end{pmatrix}
\]

The function \texttt{screeningplan()} generates a screening plan by
calling the \texttt{randorient()} function \texttt{r} times. It creates
a list of random orientations and then concatenates them into a single
array, which represents the screening plan. The screening plan
implementation in \texttt{Python} is as follows (see
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/effects.py}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ screeningplan(k, p, xi, r):}
    \CommentTok{\# Empty list to accumulate screening plan rows}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(r):}
\NormalTok{        X.append(randorient(k, p, xi))}
    \CommentTok{\# Concatenate list of arrays into a single array}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.vstack(X)}
    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

It works like follows:

\begin{itemize}
\tightlist
\item
  The value of the objective function \(f\) is computed for each row of
  the screening plan matrix \(X\). These values are stored in a column
  vector \(t\) of size \((r * (k + 1)) \times 1\), where:

  \begin{itemize}
  \tightlist
  \item
    \texttt{r} is the number of random orientations.
  \item
    \texttt{k} is the number of design variables.
  \end{itemize}
\end{itemize}

The elementary effects are calculated using the following formula:

\begin{itemize}
\tightlist
\item
  For each random orientation, adjacent rows of the screening plan
  matrix X and their corresponding function values from t are used.
\item
  These values are inserted into Equation~\ref{eq-eleffect} to compute
  elementary effects for each variable. An elementary effect measures
  the sensitivity of the objective function to changes in a specific
  variable.
\end{itemize}

Results can be used for a statistical analysis. After collecting a
sample of \(r\) elementary effects for each variable:

\begin{itemize}
\tightlist
\item
  The sample mean (central tendency) is computed to indicate the overall
  influence of the variable.
\item
  The sample standard deviation (spread) is computed to capture
  variability, which may indicate interactions or nonlinearity.
\end{itemize}

The results (sample means and standard deviations) are plotted on a
chart for comparison. This helps identify which variables have the most
significant impact on the objective function and whether their effects
are linear or involve interactions. This is implemented in the function
\texttt{screening\_plot()} in \texttt{Python}, which uses the helper
function \texttt{\_screening()} to calculate the elementary effects and
their statistics.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ \_screening(X, fun, xi, p, labels, bounds}\OperatorTok{=}\VariableTok{None}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{tuple}\NormalTok{:}
    \CommentTok{"""Helper function to calculate elementary effects for a screening design.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray): The screening plan matrix, typically structured}
\CommentTok{            within a [0,1]\^{}k box.}
\CommentTok{        fun (object): The objective function to evaluate at each}
\CommentTok{            design point in the screening plan.}
\CommentTok{        xi (float): The elementary effect step length factor.}
\CommentTok{        p (int): Number of discrete levels along each dimension.}
\CommentTok{        labels (list of str): A list of variable names corresponding to}
\CommentTok{            the design variables.}
\CommentTok{        bounds (np.ndarray): A 2xk matrix where the first row contains}
\CommentTok{            lower bounds and the second row contains upper bounds for}
\CommentTok{            each variable.}

\CommentTok{    Returns:}
\CommentTok{        tuple: A tuple containing two arrays:}
\CommentTok{            {-} sm: The mean of the elementary effects for each variable.}
\CommentTok{            {-} ssd: The standard deviation of the elementary effects for}
\CommentTok{            each variable.}
\CommentTok{    """}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    r }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{] }\OperatorTok{//}\NormalTok{ (k }\OperatorTok{+} \DecValTok{1}\NormalTok{)}

    \CommentTok{\# Scale each design point}
\NormalTok{    t }\OperatorTok{=}\NormalTok{ np.zeros(X.shape[}\DecValTok{0}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(X.shape[}\DecValTok{0}\NormalTok{]):}
        \ControlFlowTok{if}\NormalTok{ bounds }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            X[i, :] }\OperatorTok{=}\NormalTok{ bounds[}\DecValTok{0}\NormalTok{, :] }\OperatorTok{+}\NormalTok{ X[i, :] }\OperatorTok{*}\NormalTok{ (bounds[}\DecValTok{1}\NormalTok{, :] }\OperatorTok{{-}}\NormalTok{ bounds[}\DecValTok{0}\NormalTok{, :])}
\NormalTok{        t[i] }\OperatorTok{=}\NormalTok{ fun(X[i, :])}

    \CommentTok{\# Elementary effects}
\NormalTok{    F }\OperatorTok{=}\NormalTok{ np.zeros((k, r))}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(r):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i }\OperatorTok{*}\NormalTok{ (k }\OperatorTok{+} \DecValTok{1}\NormalTok{), i }\OperatorTok{*}\NormalTok{ (k }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ k):}
\NormalTok{            idx }\OperatorTok{=}\NormalTok{ np.where(X[j, :] }\OperatorTok{{-}}\NormalTok{ X[j }\OperatorTok{+} \DecValTok{1}\NormalTok{, :] }\OperatorTok{!=} \DecValTok{0}\NormalTok{)[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}
\NormalTok{            F[idx, i] }\OperatorTok{=}\NormalTok{ (t[j }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ t[j]) }\OperatorTok{/}\NormalTok{ (xi }\OperatorTok{/}\NormalTok{ (p }\OperatorTok{{-}} \DecValTok{1}\NormalTok{))}

    \CommentTok{\# Statistical measures (divide by n)}
\NormalTok{    ssd }\OperatorTok{=}\NormalTok{ np.std(F, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{, ddof}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{    sm }\OperatorTok{=}\NormalTok{ np.mean(F, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ sm, ssd}


\KeywordTok{def}\NormalTok{ screening\_plot(X, fun, xi, p, labels, bounds}\OperatorTok{=}\VariableTok{None}\NormalTok{, show}\OperatorTok{=}\VariableTok{True}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
    \CommentTok{"""Generates a plot with elementary effect screening metrics.}

\CommentTok{    This function calculates the mean and standard deviation of the}
\CommentTok{    elementary effects for a given set of design variables and plots}
\CommentTok{    the results.}

\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            The screening plan matrix, typically structured within a [0,1]\^{}k box.}
\CommentTok{        fun (object):}
\CommentTok{            The objective function to evaluate at each design point in the screening plan.}
\CommentTok{        xi (float):}
\CommentTok{            The elementary effect step length factor.}
\CommentTok{        p (int):}
\CommentTok{            Number of discrete levels along each dimension.}
\CommentTok{        labels (list of str):}
\CommentTok{            A list of variable names corresponding to the design variables.}
\CommentTok{        bounds (np.ndarray):}
\CommentTok{            A 2xk matrix where the first row contains lower bounds and}
\CommentTok{            the second row contains upper bounds for each variable.}
\CommentTok{        show (bool):}
\CommentTok{            If True, the plot is displayed. Defaults to True.}

\CommentTok{    Returns:}
\CommentTok{        None: The function generates a plot of the results.}
\CommentTok{    """}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    sm, ssd }\OperatorTok{=}\NormalTok{ \_screening(X}\OperatorTok{=}\NormalTok{X, fun}\OperatorTok{=}\NormalTok{fun, xi}\OperatorTok{=}\NormalTok{xi, p}\OperatorTok{=}\NormalTok{p, labels}\OperatorTok{=}\NormalTok{labels, bounds}\OperatorTok{=}\NormalTok{bounds)}
\NormalTok{    plt.figure()}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
\NormalTok{        plt.text(sm[i], ssd[i], labels[i], fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    plt.axis([}\BuiltInTok{min}\NormalTok{(sm), }\FloatTok{1.1} \OperatorTok{*} \BuiltInTok{max}\NormalTok{(sm), }\BuiltInTok{min}\NormalTok{(ssd), }\FloatTok{1.1} \OperatorTok{*} \BuiltInTok{max}\NormalTok{(ssd)])}
\NormalTok{    plt.xlabel(}\StringTok{"Sample means"}\NormalTok{)}
\NormalTok{    plt.ylabel(}\StringTok{"Sample standard deviations"}\NormalTok{)}
\NormalTok{    plt.gca().tick\_params(labelsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    plt.grid(}\VariableTok{True}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ show:}
\NormalTok{        plt.show()}
\end{Highlighting}
\end{Shaded}

\subsection{Special Considerations When Deploying Screening
Algorithms}\label{special-considerations-when-deploying-screening-algorithms}

When implementing the screening algorithm described above, two specific
scenarios require special attention:

\begin{itemize}
\tightlist
\item
  Duplicate Design Points: If the dimensionality \(k\) of the space is
  relatively low and you can afford a large number of elementary effects
  \(r\), we should be be aware of the increased probability of duplicate
  design points appearing in the sampling plan \(X\). *Since the
  responses at sample points are deterministic, there's no value in
  evaluating the same point multiple times. Fortunately, this issue is
  relatively uncommon in practice, as screening high-dimensional spaces
  typically requires large numbers of elementary effects, which
  naturally reduces the likelihood of duplicates.
\item
  Failed Simulations: Numerical simulation codes occasionally fail to
  return valid results due to meshing errors, non-convergence of partial
  differential equation solvers, numerical instabilities, or parameter
  combinations outside the stable operating range.
\end{itemize}

From a screening perspective, this is particularly problematic because
an entire random orientation \(B^*\) becomes compromised if even a
single point within it fails to evaluate properly. Implementing error
handling strategies or fallback methods to manage such cases should be
considered.

For robust screening studies, monitoring simulation success rates and
having contingency plans for failed evaluations are important aspects of
the experimental design process.

\section{Analyzing Variable Importance in Aircraft Wing
Weight}\label{analyzing-variable-importance-in-aircraft-wing-weight}

Let us consider the following analytical expression used as a conceptual
level estimate of the weight of a light aircraft wing as discussed in
Chapter~\ref{sec-awwe}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fun }\OperatorTok{=}\NormalTok{ Analytical()}
\NormalTok{k }\OperatorTok{=} \DecValTok{10}
\NormalTok{p }\OperatorTok{=} \DecValTok{10}
\NormalTok{xi }\OperatorTok{=} \DecValTok{1}
\NormalTok{r }\OperatorTok{=} \DecValTok{25}
\NormalTok{X }\OperatorTok{=}\NormalTok{ screeningplan(k}\OperatorTok{=}\NormalTok{k, p}\OperatorTok{=}\NormalTok{p, xi}\OperatorTok{=}\NormalTok{xi, r}\OperatorTok{=}\NormalTok{r)  }\CommentTok{\# shape (r x (k+1), k)}
\NormalTok{value\_range }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{    [}\DecValTok{150}\NormalTok{, }\DecValTok{220}\NormalTok{,   }\DecValTok{6}\NormalTok{, }\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{16}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{1700}\NormalTok{, }\FloatTok{0.025}\NormalTok{],}
\NormalTok{    [}\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{,  }\DecValTok{10}\NormalTok{,  }\DecValTok{10}\NormalTok{, }\DecValTok{45}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{6.0}\NormalTok{, }\DecValTok{2500}\NormalTok{, }\FloatTok{0.08}\NormalTok{ ],}
\NormalTok{])}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"S\_W"}\NormalTok{, }\StringTok{"W\_fw"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"Lambda"}\NormalTok{,}
    \StringTok{"q"}\NormalTok{,   }\StringTok{"lambda"}\NormalTok{, }\StringTok{"tc"}\NormalTok{, }\StringTok{"N\_z"}\NormalTok{,}
    \StringTok{"W\_dg"}\NormalTok{, }\StringTok{"W\_p"}
\NormalTok{]}
\NormalTok{screening\_plot(}
\NormalTok{    X}\OperatorTok{=}\NormalTok{X,}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{fun.fun\_wingwt,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{value\_range,}
\NormalTok{    xi}\OperatorTok{=}\NormalTok{xi,}
\NormalTok{    p}\OperatorTok{=}\NormalTok{p,}
\NormalTok{    labels}\OperatorTok{=}\NormalTok{labels,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-forre08a-1-2-output-1.pdf}}

}

\caption{\label{fig-forre08a-1-2}Estimated means and standard deviations
of the elementary effects for the 10 design variables of the wing weight
function. Example based on (\textbf{Forr08a?}).}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Nondeterministic Results}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

The code will generate a slightly different screening plan each time, as
it uses random orientations of the sampling matrix \(B\).

\end{tcolorbox}

Figure~\ref{fig-forre08a-1-2} provides valuable insights into variable
activity without requiring domain expertise. The screening study with
\(r = 25\) elementary effects reveals distinct patterns in how variables
affect wing weight:

\begin{itemize}
\tightlist
\item
  Variables with Minimal Impact: A clearly defined group of variables
  clusters around the origin - indicating their minimal impact on wing
  weight:

  \begin{itemize}
  \tightlist
  \item
    Paint weight (\(W_p\)) - as expected, contributes little to overall
    wing weight
  \item
    Dynamic pressure (\(q\)) - within our chosen range, this has limited
    effect (essentially representing different cruise altitudes at the
    same speed)
  \item
    Taper ratio (\(\lambda\)) and quarter-chord sweep (\(\Lambda\)) -
    these geometric parameters have minor influence within the narrow
    range (-10 to 10) typical of light aircraft
  \end{itemize}
\item
  Variables with Linear Effects:

  \begin{itemize}
  \tightlist
  \item
    While still close to the origin, fuel weight (\(W_{fw}\)) shows a
    slightly larger central tendency with very low standard deviation.
    This indicates moderate importance but minimal involvement in
    interactions with other variables.
  \end{itemize}
\item
  Variables with Nonlinear/Interactive Effects:

  \begin{itemize}
  \tightlist
  \item
    Aspect ratio (\(A\)) and airfoil thickness ratio (\(R_{tc}\)) show
    similar importance levels, but their high standard deviations
    suggest significant nonlinear behavior and interactions with other
    variables.
  \end{itemize}
\item
  Dominant Variables: The most significant impacts come from:

  \begin{itemize}
  \tightlist
  \item
    Flight design gross weight (\(W_{dg}\))
  \item
    Wing area (\(S_W\))
  \item
    Ultimate load factor (\(N_z\))
  \end{itemize}
\end{itemize}

These variables show both large central tendency values and high
standard deviations, indicating strong direct effects and complex
interactions. The interaction between aspect ratio and load factor is
particularly important - high values of both create extremely heavy
wings, explaining why highly maneuverable fighter jets cannot use
glider-like wing designs.

What makes this screening approach valuable is its ability to identify
critical variables without requiring engineering knowledge or expensive
modeling. In real-world applications, we rarely have the luxury of
creating comprehensive parameter space visualizations, which is
precisely why surrogate modeling is needed. After identifying the active
variables through screening, we can design a focused sampling plan for
these key variables. This forms the foundation for building an accurate
surrogate model of the objective function.

When the objective function is particularly expensive to evaluate, we
might recycle the runs performed during screening for the actual model
fitting step. This is most effective when some variables prove to have
no impact at all. However, since completely inactive variables are rare
in practice, engineers must carefully balance the trade-off between
reusing expensive simulation runs and introducing potential noise into
the model.

\section{Designing a Sampling Plan}\label{designing-a-sampling-plan}

\subsection{Stratification}\label{stratification}

A feature shared by all of the approximation models discussed in
(\textbf{Forr08a?}) is that they are more accurate in the vicinity of
the points where we have evaluated the objective function. In later
chapters we will delve into the laws that quantify our decaying trust in
the model as we move away from a known, sampled point, but for the
purposes of the present discussion we shall merely draw the intuitive
conclusion that a uniform level of model accuracy throughout the design
space requires a uniform spread of points. A sampling plan possessing
this feature is said to be space-filling.

The most straightforward way of sampling a design space in a uniform
fashion is by means of a rectangular grid of points. This is the full
factorial sampling technique.

Here is the simplified version of a \texttt{Python} function that will
sample the unit hypercube at all levels in all dimensions, with the
\(k\)-vector \(q\) containing the number of points required along each
dimension, see
\url{https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotpython/utils/sampling.py}.

The variable \texttt{Edges} specifies whether we want the points to be
equally spaced from edge to edge (\texttt{Edges=1}) or we want them to
be in the centres of \(n = q_1 \times q_2 \times \ldots \times q_k\)
bins filling the unit hypercube (for any other value of \texttt{Edges}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fullfactorial(q\_param, Edges}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""Generates a full factorial sampling plan in the unit cube.}

\CommentTok{    Args:}
\CommentTok{        q (list or np.ndarray):}
\CommentTok{            A list or array containing the number of points along each dimension (k{-}vector).}
\CommentTok{        Edges (int, optional):}
\CommentTok{            Determines spacing of points. If \textasciigrave{}Edges=1\textasciigrave{}, points are equally spaced from edge to edge (default).}
\CommentTok{            Otherwise, points will be in the centers of n = q[0]*q[1]*...*q[k{-}1] bins filling the unit cube.}

\CommentTok{    Returns:}
\CommentTok{        (np.ndarray): Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.}

\CommentTok{    Raises:}
\CommentTok{        ValueError: If any dimension in \textasciigrave{}q\textasciigrave{} is less than 2.}
\CommentTok{    """}
\NormalTok{    q\_levels }\OperatorTok{=}\NormalTok{ np.array(q\_param) }\CommentTok{\# Use a distinct variable for original levels}
    \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(q\_levels) }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"You must have at least two points per dimension."}\NormalTok{)}
    
\NormalTok{    n }\OperatorTok{=}\NormalTok{ np.prod(q\_levels)}
\NormalTok{    k }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(q\_levels)}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.zeros((n, k))}
    
    \CommentTok{\# q\_for\_prod\_calc is used for calculating repetitions, includes the phantom element.}
    \CommentTok{\# This matches the logic of the user{-}provided snippet where \textquotesingle{}q\textquotesingle{} was modified.}
\NormalTok{    q\_for\_prod\_calc }\OperatorTok{=}\NormalTok{ np.append(q\_levels, }\DecValTok{1}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k): }\CommentTok{\# k is the original number of dimensions}
        \CommentTok{\# current\_dim\_levels is the number of levels for the current dimension j}
        \CommentTok{\# In the user\textquotesingle{}s snippet, q[j] correctly refers to the original level count}
        \CommentTok{\# as j ranges from 0 to k{-}1, and q\_for\_prod\_calc[j] = q\_levels[j] for this range.}
\NormalTok{        current\_dim\_levels }\OperatorTok{=}\NormalTok{ q\_for\_prod\_calc[j] }
        
        \ControlFlowTok{if}\NormalTok{ Edges }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
\NormalTok{            one\_d\_slice }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\BuiltInTok{int}\NormalTok{(current\_dim\_levels))}
        \ControlFlowTok{else}\NormalTok{:}
            \CommentTok{\# Corrected calculation for bin centers}
            \ControlFlowTok{if}\NormalTok{ current\_dim\_levels }\OperatorTok{==} \DecValTok{1}\NormalTok{: }\CommentTok{\# Should not be hit if np.min(q\_levels) \textgreater{}= 2}
\NormalTok{                one\_d\_slice }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{])}
            \ControlFlowTok{else}\NormalTok{:}
\NormalTok{                one\_d\_slice }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ current\_dim\_levels), }
                                          \DecValTok{1} \OperatorTok{{-}} \DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ current\_dim\_levels), }
                                          \BuiltInTok{int}\NormalTok{(current\_dim\_levels))}
        
\NormalTok{        column }\OperatorTok{=}\NormalTok{ np.array([])}
        \CommentTok{\# The product q\_for\_prod\_calc[j + 1 : k] correctly calculates }
        \CommentTok{\# the product of remaining original dimensions\textquotesingle{} levels.}
\NormalTok{        num\_consecutive\_repeats }\OperatorTok{=}\NormalTok{ np.prod(q\_for\_prod\_calc[j }\OperatorTok{+} \DecValTok{1}\NormalTok{ : k])}
        
        \CommentTok{\# This loop structure replicates the logic from the user\textquotesingle{}s snippet}
        \ControlFlowTok{while} \BuiltInTok{len}\NormalTok{(column) }\OperatorTok{\textless{}}\NormalTok{ n:}
            \ControlFlowTok{for}\NormalTok{ ll\_idx }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{int}\NormalTok{(current\_dim\_levels)): }\CommentTok{\# Iterate through levels of current dimension}
\NormalTok{                val\_to\_repeat }\OperatorTok{=}\NormalTok{ one\_d\_slice[ll\_idx]}
\NormalTok{                column }\OperatorTok{=}\NormalTok{ np.append(column, np.ones(}\BuiltInTok{int}\NormalTok{(num\_consecutive\_repeats)) }\OperatorTok{*}\NormalTok{ val\_to\_repeat)}
\NormalTok{        X[:, j] }\OperatorTok{=}\NormalTok{ column}
    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q }\OperatorTok{=}\NormalTok{ [}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{X }\OperatorTok{=}\NormalTok{ fullfactorial(q, Edges}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.16666667 0.25      ]
 [0.16666667 0.75      ]
 [0.5        0.25      ]
 [0.5        0.75      ]
 [0.83333333 0.25      ]
 [0.83333333 0.75      ]]
\end{verbatim}

Figure~\ref{fig-fullfactorial-2d-edges0} shows the points in the unit
hypercube for the case of 3x2 points.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-fullfactorial-2d-edges0-output-1.pdf}}

}

\caption{\label{fig-fullfactorial-2d-edges0}2D Full Factorial Sampling
(3x2 Points). Edges = 0}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ fullfactorial(q, Edges}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.  0. ]
 [0.  1. ]
 [0.5 0. ]
 [0.5 1. ]
 [1.  0. ]
 [1.  1. ]]
\end{verbatim}

Figure~\ref{fig-fullfactorial-2d-edges1} shows the points in the unit
hypercube for the case of 3x2 points with edges.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-fullfactorial-2d-edges1-output-1.pdf}}

}

\caption{\label{fig-fullfactorial-2d-edges1}2D Full Factorial Sampling
(3x2 Points). Edges = 1}

\end{figure}%

The full factorial sampling plan method generates a uniform sampling
design by creating a grid of points across all dimensions. For example,
calling fullfactorial({[}3, 4, 5{]}, 1) produces a three-dimensional
sampling plan with 3, 4, and 5 levels along each dimension,
respectively. While this approach satisfies the uniformity criterion, it
has two significant limitations:

\begin{itemize}
\item
  Restricted Design Sizes: The method only works for designs where the
  total number of points \(n\) can be expressed as the product of the
  number of levels in each dimension, i.e.,
  \(n = q_1 \times q_2 \times \cdots \times q_k\).
\item
  Overlapping Projections: When the sampling points are projected onto
  individual axes, sets of points may overlap, reducing the
  effectiveness of the sampling plan. This can lead to non-uniform
  coverage in the projections, which may not fully represent the design
  space.
\end{itemize}

\subsection{Latin Squares and Random Latin
Hypercubes}\label{latin-squares-and-random-latin-hypercubes}

To improve the uniformity of projections for any individual variable,
the range of that variable can be divided into a large number of
equal-sized bins, and random subsamples of equal size can be generated
within these bins. This method is called stratified random sampling.
Extending this idea to all dimensions results in a stratified sampling
plan, commonly implemented using Latin hypercube sampling.

\begin{definition}[Latin Squares and
Hypercubes]\protect\hypertarget{def-latin-hypercube}{}\label{def-latin-hypercube}

In the context of statistical sampling, a square grid containing sample
positions is a Latin square if (and only if) there is only one sample in
each row and each column. A Latin hypercube is the generalisation of
this concept to an arbitrary number of dimensions, whereby each sample
is the only one in each axis-aligned hyperplane containing it

\end{definition}

For two-dimensional discrete variables, a Latin square ensures uniform
projections. An \((n \times n)\) Latin square is constructed by filling
each row and column with a permutation of \(\{1, 2, \dots, n\}\),
ensuring each number appears only once per row and column.

\begin{example}[Latin
Square]\protect\hypertarget{exm-latin-square}{}\label{exm-latin-square}

For \(n = 4\), a Latin square might look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{2   1   3   4}
\NormalTok{3   2   4   1}
\NormalTok{1   4   2   3}
\NormalTok{4   3   1   2}
\end{Highlighting}
\end{Shaded}

\end{example}

Latin Hypercubes are the multidimensional extension of Latin squares.
The design space is divided into equal-sized hypercubes (bins), and one
point is placed in each bin. The placement ensures that moving along any
axis from an occupied bin does not encounter another occupied bin. This
guarantees uniform projections across all dimensions. To construct a
Latin hypercube, the following steps are taken:

\begin{itemize}
\tightlist
\item
  Represent the sampling plan as an \(n \times k\) matrix \(X\), where
  \(n\) is the number of points and \(k\) is the number of dimensions.
\item
  Fill each column of \(X\) with random permutations of
  \(\{1, 2, \dots, n\}\).
\item
  Normalize the plan into the unit hypercube \([0, 1]^k\).
\end{itemize}

This approach ensures multidimensional stratification and uniformity in
projections. Here is the code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ rlh(n: }\BuiltInTok{int}\NormalTok{, k: }\BuiltInTok{int}\NormalTok{, edges: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{\# Initialize array}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.zeros((n, k), dtype}\OperatorTok{=}\BuiltInTok{float}\NormalTok{)}

    \CommentTok{\# Fill with random permutations}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
\NormalTok{        X[:, i] }\OperatorTok{=}\NormalTok{ np.random.permutation(n)}

    \CommentTok{\# Adjust normalization based on the edges flag}
    \ControlFlowTok{if}\NormalTok{ edges }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \CommentTok{\# [X=0..n{-}1] {-}\textgreater{} [0..1]}
\NormalTok{        X }\OperatorTok{=}\NormalTok{ X }\OperatorTok{/}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# Points at true midpoints}
        \CommentTok{\# [X=0..n{-}1] {-}\textgreater{} [0.5/n..(n{-}0.5)/n]}
\NormalTok{        X }\OperatorTok{=}\NormalTok{ (X }\OperatorTok{+} \FloatTok{0.5}\NormalTok{) }\OperatorTok{/}\NormalTok{ n}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{example}[Random Latin
Hypercube]\protect\hypertarget{exm-rlh}{}\label{exm-rlh}

The following code can be used to generate a 2D Latin hypercube with 5
points and edges=0:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ rlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, edges}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.7 0.1]
 [0.9 0.9]
 [0.5 0.5]
 [0.1 0.7]
 [0.3 0.3]]
\end{verbatim}

Figure~\ref{fig-rlh-edges0} shows the points in the unit hypercube for
the case of 5 points with \texttt{edges=0}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-rlh-edges0-output-1.pdf}}

}

\caption{\label{fig-rlh-edges0}2D Latin Hypercube Sampling (5 Points,
Edges=0)}

\end{figure}%

\end{example}

\begin{example}[Random Latin Hypercube with
Edges]\protect\hypertarget{exm-rlh-edges}{}\label{exm-rlh-edges}

The following code can be used to generate a 2D Latin hypercube with 5
points and edges=1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ rlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, edges}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1.   0.75]
 [0.75 0.5 ]
 [0.   0.25]
 [0.25 1.  ]
 [0.5  0.  ]]
\end{verbatim}

Figure~\ref{fig-rlh-edges1} shows the points in the unit hypercube for
the case of 5 points with \texttt{edges=1}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-rlh-edges1-output-1.pdf}}

}

\caption{\label{fig-rlh-edges1}2D Latin Hypercube Sampling (5 Points,
Edges=1)}

\end{figure}%

\end{example}

\subsection{Space-filling Designs: Maximin
Plans}\label{space-filling-designs-maximin-plans}

A widely adopted measure for assessing the uniformity, or
`space-fillingness', of a sampling plan is the maximin metric, initially
proposed by (\textbf{john90a?}). This criterion can be formally defined
as follows.

Consider a sampling plan \(X\). Let \(d_1, d_2, \ldots, d_m\) represent
the unique distances between all possible pairs of points within \(X\),
arranged in ascending order. Furthermore, let \(J_1, J_2, \ldots, J_m\)
be defined such that \(J_j\) denotes the count of point pairs in \(X\)
separated by the distance \(d_j\).

\begin{definition}[Maximin
plan]\protect\hypertarget{def-maximin}{}\label{def-maximin}

A sampling plan \(X\) is considered a maximin plan if, among all
candidate plans, it maximizes the smallest inter-point distance \(d_1\).
Among plans that satisfy this condition, it further minimizes \(J_1\),
the number of pairs separated by this minimum distance.

\end{definition}

While this definition is broadly applicable to any collection of
sampling plans, our focus is narrowed to Latin hypercube designs to
preserve their desirable stratification properties. However, even within
this restricted class, Definition~\ref{def-maximin} may identify
multiple equivalent maximin designs. To address this, a more
comprehensive `tie-breaker' definition, as proposed by
(\textbf{morr95a?}), is employed:

\begin{definition}[Maximin plan with
tie-breaker]\protect\hypertarget{def-maximin2}{}\label{def-maximin2}

A sampling plan \(X\) is designated as the maximin plan if it
sequentially optimizes the following conditions: it maximizes \(d_1\);
among those, it minimizes \(J_1\); among those, it maximizes \(d_2\);
among those, it minimizes \(J_2\); and so forth, concluding with
minimizing \(J_m\).

\end{definition}

(\textbf{john90a?}) established that the maximin criterion
(Definition~\ref{def-maximin}) is equivalent to the D-optimality
criterion used in linear regression. However, the extended maximin
criterion incorporating a tie-breaker (Definition~\ref{def-maximin2}) is
often preferred due to its intuitive nature and practical utility. Given
that the sampling plans under consideration make no assumptions about
model structure, the latter criterion (Definition~\ref{def-maximin2})
will be employed.

To proceed, a precise definition of `distance' within these contexts is
necessary. The p-norm is the most widely adopted metric for this
purpose:

\begin{definition}[p-norm]\protect\hypertarget{def-p-norm}{}\label{def-p-norm}

The p-norm of a vector \(\vec{x} = (x_1, x_2, \ldots, x_k)\) is defined
as:

\begin{equation}\phantomsection\label{eq-p-norm}{
d_p(\vec{x}^{(i_1)}, \vec{x}^{(i_2)}) = \left( \sum_{j=1}^k |x_j^{(i_1)} - x_j^{(i_2)}|^p \right)^{1/p}.
}\end{equation}

\end{definition}

When \(p = 1\), Equation~\ref{eq-p-norm} defines the rectangular
distance, occasionally referred to as the Manhattan norm (an allusion to
a grid-like city layout). Setting \(p = 2\) yields the Euclidean norm.
The existing literature offers limited evidence to suggest the
superiority of one norm over the other for evaluating sampling plans
when no model structure assumptions are made. It is important to note,
however, that the rectangular distance is considerably less
computationally demanding. This advantage can be quite significant,
particularly when evaluating large sampling plans.

For the computational implementation of Definition~\ref{def-maximin2},
the initial step involves constructing the vectors
\(d_1, d_2, \ldots, d_m\) and \(J_1, J_2, \ldots, J_m\). The \texttt{jd}
function facilitates this task.

\subsubsection{\texorpdfstring{The Function
\texttt{jd}}{The Function jd}}\label{the-function-jd}

The function \texttt{jd} computes the distinct p-norm distances between
all pairs of points in a given set and counts their occurrences. It
returns two arrays: one for the distinct distances and another for their
multiplicities.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ jd(X: np.ndarray, p: }\BuiltInTok{float} \OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ Tuple[np.ndarray, np.ndarray]:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            A 2D array of shape (n, d) representing n points}
\CommentTok{            in d{-}dimensional space.}
\CommentTok{        p (float, optional):}
\CommentTok{            The distance norm to use.}
\CommentTok{            p=1 uses the Manhattan (L1) norm, while p=2 uses the}
\CommentTok{            Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).}

\CommentTok{    Returns:}
\CommentTok{        (np.ndarray, np.ndarray):}
\CommentTok{            A tuple (J, distinct\_d), where:}
\CommentTok{            {-} distinct\_d is a 1D float array of unique,}
\CommentTok{            sorted distances between points.}
\CommentTok{            {-} J is a 1D integer array that provides}
\CommentTok{            the multiplicity (occurrence count)}
\CommentTok{            of each distance in distinct\_d.}
\CommentTok{    """}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# Allocate enough space for all pairwise distances}
    \CommentTok{\# (n*(n{-}1))/2 pairs for an n{-}point set}
\NormalTok{    pair\_count }\OperatorTok{=}\NormalTok{ n }\OperatorTok{*}\NormalTok{ (n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{//} \DecValTok{2}
\NormalTok{    d }\OperatorTok{=}\NormalTok{ np.zeros(pair\_count, dtype}\OperatorTok{=}\BuiltInTok{float}\NormalTok{)}

    \CommentTok{\# Fill the distance array}
\NormalTok{    idx }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i }\OperatorTok{+} \DecValTok{1}\NormalTok{, n):}
            \CommentTok{\# Compute the p{-}norm distance}
\NormalTok{            d[idx] }\OperatorTok{=}\NormalTok{ np.linalg.norm(X[i] }\OperatorTok{{-}}\NormalTok{ X[j], }\BuiltInTok{ord}\OperatorTok{=}\NormalTok{p)}
\NormalTok{            idx }\OperatorTok{+=} \DecValTok{1}

    \CommentTok{\# Find unique distances and their multiplicities}
\NormalTok{    distinct\_d }\OperatorTok{=}\NormalTok{ np.unique(d)}
\NormalTok{    J }\OperatorTok{=}\NormalTok{ np.zeros\_like(distinct\_d, dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ i, val }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(distinct\_d):}
\NormalTok{        J[i] }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(d }\OperatorTok{==}\NormalTok{ val)}
    \ControlFlowTok{return}\NormalTok{ J, distinct\_d}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{jd}]\protect\hypertarget{exm-jd}{}\label{exm-jd}

Consider a small 3-point set in 2D space, with points located at (0,0),
(1,1), and (2,2) as shown in Figure~\ref{fig-jd-3points}. The distinct
distances and their occurrences can be computed using the \texttt{jd}
function, as shown in the following code:

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-jd-3points-output-1.pdf}}

}

\caption{\label{fig-jd-3points}3-Point Set in 2D Space}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{J, distinct\_d }\OperatorTok{=}\NormalTok{ jd(X, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Distinct distances (d\_i):"}\NormalTok{, distinct\_d)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Occurrences (J\_i):"}\NormalTok{, J)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Distinct distances (d_i): [1.41421356 2.82842712]
Occurrences (J_i): [2 1]
\end{verbatim}

\end{example}

\subsection{Memory Management}\label{memory-management}

A computationally intensive part of the calculation performed with the
\texttt{jd}-function is the creation of the vector \(\vec{d}\)
containing all pairwise distances. This is particularly true for large
sampling plans; for instance, a 1000-point plan requires nearly half a
million distance calculations.

\begin{definition}[Pre-allocation of
Memory]\protect\hypertarget{def-prealloc}{}\label{def-prealloc}

Pre-allocation of memory is a programming technique where a fixed amount
of memory is reserved for a data structure (like an array or vector)
before it is actually filled with data. This is done to avoid the
computational overhead associated with dynamic memory allocation, which
involves repeatedly requesting and resizing memory as new elements are
added.

\end{definition}

Consequently, pre-allocating memory for the distance vector \(\vec{d}\)
is essential. This necessitates a slightly less direct method for
computing the indices of \(\vec{d}\), rather than appending each new
element, which would involve costly dynamic memory allocation.

The implementation of Definition~\ref{def-maximin2} is now required.
Finding the most space-filling design involves pairwise comparisons.
This problem can be approached using a `divide and conquer' strategy,
simplifying it to the task of selecting the better of two sampling
plans. The function \texttt{mm(X1,X2,p)} is designed for this purpose.
It returns an index indicating which of the two designs is more
space-filling, or \texttt{0} if they are equally space-filling, based on
the \(p\)-norm for distance computation.

\subsubsection{\texorpdfstring{The Function
\texttt{mm}}{The Function mm}}\label{the-function-mm}

The function \texttt{mm} compares two sampling plans based on the
Morris-Mitchell criterion. It uses the \texttt{jd} function to compute
the distances and multiplicities, constructs vectors for comparison, and
determines which plan is more space-filling.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mm(X1: np.ndarray, X2: np.ndarray, p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X1 (np.ndarray): A 2D array representing the first sampling plan.}
\CommentTok{        X2 (np.ndarray): A 2D array representing the second sampling plan.}
\CommentTok{        p (float, optional): The distance metric. p=1 uses Manhattan (L1) distance,}
\CommentTok{            while p=2 uses Euclidean (L2). Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        int:}
\CommentTok{            {-} 0 if both plans are identical or equally space{-}filling}
\CommentTok{            {-} 1 if X1 is more space{-}filling}
\CommentTok{            {-} 2 if X2 is more space{-}filling}
\CommentTok{    """}
\NormalTok{    X1\_sorted }\OperatorTok{=}\NormalTok{ X1[np.lexsort(np.rot90(X1))]}
\NormalTok{    X2\_sorted }\OperatorTok{=}\NormalTok{ X2[np.lexsort(np.rot90(X2))]}
    \ControlFlowTok{if}\NormalTok{ np.array\_equal(X1\_sorted, X2\_sorted):}
        \ControlFlowTok{return} \DecValTok{0}  \CommentTok{\# Identical sampling plans}

    \CommentTok{\# Compute distance multiplicities for each plan}
\NormalTok{    J1, d1 }\OperatorTok{=}\NormalTok{ jd(X1, p)}
\NormalTok{    J2, d2 }\OperatorTok{=}\NormalTok{ jd(X2, p)}
\NormalTok{    m1, m2 }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(d1), }\BuiltInTok{len}\NormalTok{(d2)}

    \CommentTok{\# Construct V1 and V2: alternate distance and negative multiplicity}
\NormalTok{    V1 }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{*}\NormalTok{ m1)}
\NormalTok{    V1[}\DecValTok{0}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ d1}
\NormalTok{    V1[}\DecValTok{1}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{J1}

\NormalTok{    V2 }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2} \OperatorTok{*}\NormalTok{ m2)}
\NormalTok{    V2[}\DecValTok{0}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ d2}
\NormalTok{    V2[}\DecValTok{1}\NormalTok{::}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{J2}

    \CommentTok{\# Trim the longer vector to match the size of the shorter}
\NormalTok{    m }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(m1, m2)}
\NormalTok{    V1 }\OperatorTok{=}\NormalTok{ V1[:m]}
\NormalTok{    V2 }\OperatorTok{=}\NormalTok{ V2[:m]}

    \CommentTok{\# Compare element{-}by{-}element:}
    \CommentTok{\# c[i] = 1 if V1[i] \textgreater{} V2[i], 2 if V1[i] \textless{} V2[i], 0 otherwise.}
\NormalTok{    c }\OperatorTok{=}\NormalTok{ (V1 }\OperatorTok{\textgreater{}}\NormalTok{ V2).astype(}\BuiltInTok{int}\NormalTok{) }\OperatorTok{+} \DecValTok{2} \OperatorTok{*}\NormalTok{ (V1 }\OperatorTok{\textless{}}\NormalTok{ V2).astype(}\BuiltInTok{int}\NormalTok{)}

    \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(c) }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \CommentTok{\# Equally space{-}filling}
        \ControlFlowTok{return} \DecValTok{0}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# The first non{-}zero entry indicates which plan is better}
\NormalTok{        idx }\OperatorTok{=}\NormalTok{ np.argmax(c }\OperatorTok{!=} \DecValTok{0}\NormalTok{)}
        \ControlFlowTok{return}\NormalTok{ c[idx]}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{mm}]\protect\hypertarget{exm-mm}{}\label{exm-mm}

We can use the \texttt{mm} function to compare two sampling plans. The
following code creates two 3-point sampling plans in 2D (shown in
Figure~\ref{fig-mm-3points}) and compares them using the Morris-Mitchell
criterion:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{],[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{],[}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{], [}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{]])}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{],[}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.6}\NormalTok{],[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{], [}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.9}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-mm-3points-output-1.pdf}}

}

\caption{\label{fig-mm-3points}Comparison of Two Sampling Plans}

\end{figure}%

We can compare which plan has better space-filling (Morris-Mitchell).
The output is either \texttt{0}, \texttt{1}, or \texttt{2} depending on
which plan is more space-filling.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{better }\OperatorTok{=}\NormalTok{ mm(X1, X2, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Plan }\SpecialCharTok{\{}\NormalTok{better}\SpecialCharTok{\}}\SpecialStringTok{ is more space{-}filling."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Plan 1 is more space-filling.
\end{verbatim}

\end{example}

\subsubsection{\texorpdfstring{The Function
\texttt{mmphi}}{The Function mmphi}}\label{the-function-mmphi}

Searching across a space of potential sampling plans can be accomplished
by pairwise comparisons. An optimization algorithm could, in theory, be
written with mm as the comparative objective. However, experimental
evidence (\textbf{morr95a?}) suggests that the resulting optimization
landscape can be quite deceptive, making it difficult to search
reliably. This difficulty arises because the comparison process
terminates upon finding the first non-zero element in the comparison
array c.~Consequently, the remaining values in the distance
(\(d_1, d_2, ..., d_m\)) and multiplicity (\(J_1, J_2, ..., J_m\))
arrays are disregarded. These disregarded values, however, might contain
potentially useful `slope' information about the global landscape for
the optimization process.

To address this, (\textbf{morr95a?}) defined the following scalar-valued
criterion function, which is used to rank competing sampling plans. This
function, while based on the logic of Definition~\ref{def-maximin2},
incorporates the complete vectors \(d_1, d_2, ..., d_m\) and
\(J_1, J_2, ..., J_m\).

\begin{definition}[Morris-Mitchell
Criterion]\protect\hypertarget{def-morris-mitchell}{}\label{def-morris-mitchell}

The Morris-Mitchell criterion is defined as:

\begin{equation}\phantomsection\label{eq-phiq}{
\Phi_q (X) = \left(\sum_{j=1}^m J_j d_j^{-q}\right)^{1/q},
}\end{equation}

where \(X\) is the sampling plan, \(d_j\) is the distance between
points, \(J_j\) is the multiplicity of that distance, and \(q\) is a
user-defined exponent. The parameter \(q\) can be adjusted to control
the influence of smaller distances on the overall metric.

\end{definition}

The smaller the value of \(\Phi_q\), the better the space-filling
properties of \(X\) will be.

The function \texttt{mmphi} computes the Morris-Mitchell sampling plan
quality criterion for a given sampling plan. It takes a 2D array of
points and calculates the space-fillingness metric based on the
distances between points. This can be implemented in Python as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mmphi(X: np.ndarray,}
\NormalTok{          q: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}\NormalTok{,}
\NormalTok{          p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            A 2D array representing the sampling plan,}
\CommentTok{            where each row is a point in}
\CommentTok{            d{-}dimensional space (shape: (n, d)).}
\CommentTok{        q (float, optional):}
\CommentTok{            Exponent used in the computation of the metric.}
\CommentTok{            Defaults to 2.0.}
\CommentTok{        p (float, optional):}
\CommentTok{            The distance norm to use.}
\CommentTok{            For example, p=1 is Manhattan (L1),}
\CommentTok{            p=2 is Euclidean (L2). Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        float:}
\CommentTok{            The space{-}fillingness metric Phiq. Larger values typically indicate a more}
\CommentTok{            space{-}filling plan according to the Morris{-}Mitchell criterion.}
\CommentTok{    """}
    \CommentTok{\# Compute the distance multiplicities: J, and unique distances: d}
\NormalTok{    J, d }\OperatorTok{=}\NormalTok{ jd(X, p)}
    \CommentTok{\# Summation of J[i] * d[i]\^{}({-}q), then raised to 1/q}
    \CommentTok{\# This follows the Morris{-}Mitchell definition.}
\NormalTok{    Phiq }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(J }\OperatorTok{*}\NormalTok{ (d }\OperatorTok{**}\NormalTok{ (}\OperatorTok{{-}}\NormalTok{q))) }\OperatorTok{**}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{/}\NormalTok{ q)}
    \ControlFlowTok{return}\NormalTok{ Phiq}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{mmphi}]\protect\hypertarget{exm-mmphi}{}\label{exm-mmphi}

We can use the \texttt{mmphi} function to evaluate the space-filling
quality of the two sampling plans from Example~\ref{exm-mm}. The
following code uses these two 3-point sampling plans in 2D and computes
their quality using the Morris-Mitchell criterion:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Two simple sampling plans from above}
\NormalTok{quality1 }\OperatorTok{=}\NormalTok{ mmphi(X1, q}\OperatorTok{=}\DecValTok{2}\NormalTok{, p}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{quality2 }\OperatorTok{=}\NormalTok{ mmphi(X2, q}\OperatorTok{=}\DecValTok{2}\NormalTok{, p}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Quality of sampling plan X1:  }\SpecialCharTok{\{}\NormalTok{quality1}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Quality of sampling plan X2:  }\SpecialCharTok{\{}\NormalTok{quality2}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Quality of sampling plan X1:  2.91547594742265
Quality of sampling plan X2:  3.917162046269215
\end{verbatim}

\end{example}

This equation provides a more compact representation of the maximin
criterion, but the selection of the \(q\) value is an important
consideration. Larger values of \(q\) ensure that terms in the sum
corresponding to smaller inter-point distances (the \(d_j\) values,
which are sorted in ascending order) have a dominant influence. As a
result, \(\Phi_q\) will rank sampling plans in a way that closely
emulates the original maximin definition
(Definition~\ref{def-maximin2}). This implies that the optimization
landscape might retain the challenging characteristics that the
\(\Phi_q\) metric, especially with smaller \(q\) values, is intended to
alleviate. Conversely, smaller \(q\) values tend to produce a \(\Phi_q\)
landscape that, while not perfectly aligning with the original
definition, is generally more conducive to optimization.

To illustrate the relationship between Equation~\ref{eq-phiq} and the
maximin criterion of Definition~\ref{def-maximin2}, sets of 50 random
Latin hypercubes of varying sizes and dimensionalities were considered
by (\textbf{Forr08a?}). The correlation plots from this analysis suggest
that as the sampling plan size increases, a smaller \(q\) value is
needed for the \(\Phi_q\)-based ranking to closely match the ranking
derived from Definition~\ref{def-maximin2}.

Rankings based on both the direct maximin comparison (\texttt{mm}) and
the \(\Phi_q\) metric (\texttt{mmphi}), determined using a simple bubble
sort algorithm, are implemented in the Python function \texttt{mmsort}.

\subsubsection{\texorpdfstring{The Function
\texttt{mmsort}}{The Function mmsort}}\label{the-function-mmsort}

The function \texttt{mmsort} is designed to rank multiple sampling plans
based on their space-filling properties using the Morris-Mitchell
criterion. It takes a 3D array of sampling plans and returns the indices
of the plans sorted in ascending order of their space-filling quality.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mmsort(X3D: np.ndarray, p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X3D (np.ndarray):}
\CommentTok{            A 3D NumPy array of shape (n, d, m), where m is the number of}
\CommentTok{            sampling plans, and each plan is an (n, d) matrix of points.}
\CommentTok{        p (float, optional):}
\CommentTok{            The distance metric to use. p=1 for Manhattan (L1), p=2 for}
\CommentTok{            Euclidean (L2). Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 1D integer array of length m that holds the plan indices in}
\CommentTok{            ascending order of space{-}filling quality. The first index in the}
\CommentTok{            returned array corresponds to the most space{-}filling plan.}
\CommentTok{    """}
    \CommentTok{\# Number of plans (m)}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ X3D.shape[}\DecValTok{2}\NormalTok{]}

    \CommentTok{\# Create index array (1{-}based to match original MATLAB convention)}
\NormalTok{    Index }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, m }\OperatorTok{+} \DecValTok{1}\NormalTok{)}

\NormalTok{    swap\_flag }\OperatorTok{=} \VariableTok{True}
    \ControlFlowTok{while}\NormalTok{ swap\_flag:}
\NormalTok{        swap\_flag }\OperatorTok{=} \VariableTok{False}
\NormalTok{        i }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{while}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ m }\OperatorTok{{-}} \DecValTok{1}\NormalTok{:}
            \CommentTok{\# Compare plan at Index[i] vs. Index[i+1] using mm()}
            \CommentTok{\# Note: subtract 1 from each index to convert to 0{-}based array indexing}
            \ControlFlowTok{if}\NormalTok{ mm(X3D[:, :, Index[i] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], X3D[:, :, Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], p) }\OperatorTok{==} \DecValTok{2}\NormalTok{:}
                \CommentTok{\# Swap indices if the second plan is more space{-}filling}
\NormalTok{                Index[i], Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{], Index[i]}
\NormalTok{                swap\_flag }\OperatorTok{=} \VariableTok{True}
\NormalTok{            i }\OperatorTok{+=} \DecValTok{1}

    \ControlFlowTok{return}\NormalTok{ Index}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{mmsort}]\protect\hypertarget{exm-mmsort}{}\label{exm-mmsort}

The \texttt{mmsort} function can be used to rank multiple sampling plans
based on their space-filling properties. The following code demonstrates
how to use \texttt{mmsort} to compare two 3-point sampling plans in 3D
space:

Suppose we have two 3-point sampling plans X1 and X1 from above. They
are sorted using the Morris-Mitchell criterion with \(p=2.0\). For
example, the output \texttt{{[}1,\ 2{]}} indicates that X1 is more
space-filling than X2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X3D }\OperatorTok{=}\NormalTok{ np.stack([X1, X2], axis}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ranking }\OperatorTok{=}\NormalTok{ mmsort(X3D, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(ranking)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1 2]
\end{verbatim}

\end{example}

To determine the optimal Latin hypercube for a specific application, a
recommended approach by (\textbf{morr95a?}) involves minimizing
\(\Phi_q\) for a set of \(q\) values (1, 2, 5, 10, 20, 50, and 100).
Subsequently, the best plan from these results is selected based on the
actual maximin definition. The mmsort function can be utilized for this
purpose: a 3D matrix, X3D, can be constructed where each 2D slice
represents the best sampling plan found for each \(\Phi_q\). Applying
\texttt{mmsort(X3D,1)} then ranks these plans according to
Definition~\ref{def-maximin2}, using the rectangular distance metric.
The subsequent discussion will address the methods for finding these
optimized \(\Phi_q\) designs.

\subsubsection{\texorpdfstring{The Function
\texttt{phisort}}{The Function phisort}}\label{the-function-phisort}

\texttt{phisort} only differs from \texttt{mmsort} in having \(q\) as an
additional argument, as well as the comparison line being:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ mmphi(X3D[:, :, Index[i] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p) }\OperatorTok{\textgreater{}}
\NormalTok{    mmphi(X3D[:, :, Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p):}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ phisort(X3D: np.ndarray,}
\NormalTok{            q: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}\NormalTok{,}
\NormalTok{            p: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{1.0}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X3D (np.ndarray):}
\CommentTok{            A 3D array of shape (n, d, m),}
\CommentTok{            where m is the number of sampling plans.}
\CommentTok{        q (float, optional):}
\CommentTok{            Exponent for the mmphi metric. Defaults to 2.0.}
\CommentTok{        p (float, optional):}
\CommentTok{            Distance norm for mmphi.}
\CommentTok{            p=1 is Manhattan; p=2 is Euclidean.}
\CommentTok{            Defaults to 1.0.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 1D integer array of length m, giving the plan indices in ascending}
\CommentTok{            order of mmphi. The first index in the returned array corresponds}
\CommentTok{            to the numerically lowest mmphi value.}
\CommentTok{    """}
    \CommentTok{\# Number of 2D sampling plans}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ X3D.shape[}\DecValTok{2}\NormalTok{]}
    \CommentTok{\# Create a 1{-}based index array}
\NormalTok{    Index }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, m }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
    \CommentTok{\# Bubble{-}sort: plan with lower mmphi() climbs toward the front}
\NormalTok{    swap\_flag }\OperatorTok{=} \VariableTok{True}
    \ControlFlowTok{while}\NormalTok{ swap\_flag:}
\NormalTok{        swap\_flag }\OperatorTok{=} \VariableTok{False}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(m }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
            \CommentTok{\# Retrieve mmphi values for consecutive plans}
\NormalTok{            val\_i }\OperatorTok{=}\NormalTok{ mmphi(X3D[:, :, Index[i] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}
\NormalTok{            val\_j }\OperatorTok{=}\NormalTok{ mmphi(X3D[:, :, Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}\NormalTok{], q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}

            \CommentTok{\# Swap if the left plan\textquotesingle{}s mmphi is larger (i.e. \textquotesingle{}worse\textquotesingle{})}
            \ControlFlowTok{if}\NormalTok{ val\_i }\OperatorTok{\textgreater{}}\NormalTok{ val\_j:}
\NormalTok{                Index[i], Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ Index[i }\OperatorTok{+} \DecValTok{1}\NormalTok{], Index[i]}
\NormalTok{                swap\_flag }\OperatorTok{=} \VariableTok{True}
    \ControlFlowTok{return}\NormalTok{ Index}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{phisort}]\protect\hypertarget{exm-phisort}{}\label{exm-phisort}

The \texttt{phisort} function can be used to rank multiple sampling
plans based on the Morris-Mitchell criterion. The following code
demonstrates how to use \texttt{phisort} to compare two 3-point sampling
plans in 3D space:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{5}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{15}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{25}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{30}\NormalTok{)}
\CommentTok{\# Map X1 and X2 so that X3D has the two sampling plans}
\CommentTok{\# in X3D[:, :, 0] and X3D[:, :, 1]}
\NormalTok{X3D }\OperatorTok{=}\NormalTok{ np.array([X1, X2])}
\BuiltInTok{print}\NormalTok{(phisort(X3D))}
\NormalTok{X3D }\OperatorTok{=}\NormalTok{ np.array([X3, X2])}
\BuiltInTok{print}\NormalTok{(phisort(X3D))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[2 1]
[1 2]
\end{verbatim}

\end{example}

\subsection{\texorpdfstring{Optimizing the Morris-Mitchell Criterion
\(\Phi_q\)}{Optimizing the Morris-Mitchell Criterion \textbackslash Phi\_q}}\label{optimizing-the-morris-mitchell-criterion-phi_q}

Once a criterion for assessing the quality of a Latin hypercube sampling
plan has been established, a systematic method for optimizing this
metric across the space of Latin hypercubes is required. This task is
non-trivial; as the reader may recall from the earlier discussion on
Latin squares, this search space is vast. In fact, its vastness means
that for many practical applications, locating the globally optimal
solution is often infeasible. Therefore, the objective becomes finding
the best possible sampling plan achievable within a specific
computational time budget.

This budget is influenced by the computational cost associated with
obtaining each objective function value. Determining the optimal
allocation of total computational effort---between generating the
sampling plan and actually evaluating the objective function at the
selected points---remains an open research question. However, it is
typical for no more than approximately 5\% of the total available time
to be allocated to the task of generating the sampling plan itself.

(\textbf{Forr08a?}) draw an analogy to the process of devising a
revision timetable before an exam. While a well-structured timetable
enhances the effectiveness of revision, an excessive amount of the
revision time itself should not be consumed by the planning phase.

A significant challenge in devising a sampling plan optimizer is
ensuring that the search process remains confined to the space of valid
Latin hypercubes. As previously discussed, the defining characteristic
of a Latin hypercube \(X\) is that each of its columns represents a
permutation of the possible levels for the corresponding variable.
Consequently, the smallest modification that can be applied to a Latin
hypercube---without compromising its crucial multidimensional
stratification property---involves swapping two elements within any
single column of \(X\). A Python implementation for `mutating' a Latin
hypercube through such an operation, generalized to accommodate random
changes applied to multiple sites, is provided below:

\subsubsection{\texorpdfstring{The Function
\texttt{perturb()}}{The Function perturb()}}\label{the-function-perturb}

The function \texttt{perturb} randomly swaps elements in a Latin
hypercube sampling plan. It takes a 2D array representing the sampling
plan and performs a specified number of random element swaps, ensuring
that the result remains a valid Latin hypercube.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ perturb(X: np.ndarray,}
\NormalTok{            PertNum: Optional[}\BuiltInTok{int}\NormalTok{] }\OperatorTok{=} \DecValTok{1}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X (np.ndarray):}
\CommentTok{            A 2D array (sampling plan) of shape (n, k),}
\CommentTok{            where each row is a point}
\CommentTok{            and each column is a dimension.}
\CommentTok{        PertNum (int, optional):}
\CommentTok{            The number of element swaps (perturbations)}
\CommentTok{            to perform. Defaults to 1.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            The perturbed sampling plan,}
\CommentTok{            identical in shape to the input, with}
\CommentTok{            one or more random column swaps executed.}
\CommentTok{    """}
    \CommentTok{\# Get dimensions of the plan}
\NormalTok{    n, k }\OperatorTok{=}\NormalTok{ X.shape}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{\textless{}} \DecValTok{2} \KeywordTok{or}\NormalTok{ k }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes require at least 2 points and 2 dimensions"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(PertNum):}
        \CommentTok{\# Pick a random column}
\NormalTok{        col }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(np.random.rand() }\OperatorTok{*}\NormalTok{ k))}
        \CommentTok{\# Pick two distinct row indices}
\NormalTok{        el1, el2 }\OperatorTok{=} \DecValTok{0}\NormalTok{, }\DecValTok{0}
        \ControlFlowTok{while}\NormalTok{ el1 }\OperatorTok{==}\NormalTok{ el2:}
\NormalTok{            el1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(np.random.rand() }\OperatorTok{*}\NormalTok{ n))}
\NormalTok{            el2 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(np.random.rand() }\OperatorTok{*}\NormalTok{ n))}
        \CommentTok{\# Swap the two selected elements in the chosen column}
\NormalTok{        X[el1, col], X[el2, col] }\OperatorTok{=}\NormalTok{ X[el2, col], X[el1, col]}
    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{perturb()}]\protect\hypertarget{exm-perturb}{}\label{exm-perturb}

The \texttt{perturb} function can be used to randomly swap elements in a
Latin hypercube sampling plan. The following code demonstrates how to
use \texttt{perturb} to create a perturbed version of a 4x2 sampling
plan:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_original }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{],[}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{],[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{],[}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original Sampling Plan:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_original)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Perturbed Sampling Plan:"}\NormalTok{)}
\NormalTok{X\_perturbed }\OperatorTok{=}\NormalTok{ perturb(X\_original, PertNum}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_perturbed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Original Sampling Plan:
[[1 3]
 [2 4]
 [3 1]
 [4 2]]
Perturbed Sampling Plan:
[[2 3]
 [1 4]
 [3 1]
 [4 2]]
\end{verbatim}

\end{example}

(\textbf{Forr08a?}) uses the term `mutation', because this problem lends
itself to nature-inspired computation. (\textbf{morr95a?}) use a
simulated annealing algorithm, the detailed pseudocode of which can be
found in their paper. As an alternative, a method based on evolutionary
operation (EVOP) is offered by (\textbf{Forr08a?}).

\subsection{Evolutionary Operation}\label{evolutionary-operation}

As introduced by (\textbf{Box57a?}), evolutionary operation was designed
to optimize chemical processes. The current parameters of the reaction
would be recorded in a box at the centre of a board, with a series of
`offspring' boxes along the edges containing values of the parameters
slightly altered with respect to the central, `parent' values. Once the
reaction was completed for all of these sets of variable values and the
corresponding yields recorded, the contents of the central box would be
replaced with that of the setup with the highest yield and this would
then become the parent of a new set of peripheral boxes.

This is generally viewed as a local search procedure, though this
depends on the mutation step sizes, that is on the differences between
the parent box and its offspring. The longer these steps, the more
global is the scope of the search.

For the purposes of the Latin hypercube search, a variable scope
strategy is applied. The process starts with a long step length (that is
a relatively large number of swaps within the columns) and, as the
search progresses, the current best basin of attraction is gradually
approached by reducing the step length to a single change.

In each generation the parent is mutated (randomly, using the
\texttt{perturb} function) a pertnum number of times. The sampling plan
that yields the smallest \(\Phi_q\) value (as per the Morris-Mitchell
criterion, calculated using\texttt{mmphi}) among all offspring and the
parent is then selected; in evolutionary computation parlance this
selection philosophy is referred to as elitism.

The EVOP based search for space-filling Latin hypercubes is thus a truly
evolutionary process: the optimized sampling plan results from the
nonrandom survival of random variations.

\subsection{Putting it all Together}\label{putting-it-all-together}

All the pieces of the optimum Latin hypercube sampling process puzzle
are now in place: the random hypercube generator as a starting point for
the optimization process, the `spacefillingness' metric that needs to be
optimized, the optimization engine that performs this task and the
comparison function that selects the best of the optima found for the
various \(q\)'s. These pieces just need to be put into a sequence. Here
is the Python embodiment of the completed puzzle. It results in a
function \texttt{bestlh} that uses the function \texttt{mmlhs} to find
the best Latin hypercube sampling plan for a given set of parameters.

\subsubsection{\texorpdfstring{The Function
\texttt{mmlhs}}{The Function mmlhs}}\label{the-function-mmlhs}

Performs an evolutionary search (using perturbations) to find a
Morris-Mitchell optimal Latin hypercube, starting from an initial plan
X\_start.

This function does the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initializes a ``best'' Latin hypercube (\texttt{X\_best}) from the
  provided \texttt{X\_start}.
\item
  Iteratively perturbs \texttt{X\_best} to create offspring.
\item
  Evaluates the space-fillingness of each offspring via the
  Morris-Mitchell metric (using \texttt{mmphi}).
\item
  Updates the best plan whenever a better offspring is found.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mmlhs(X\_start: np.ndarray,}
\NormalTok{          population: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{          iterations: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{          q: Optional[}\BuiltInTok{float}\NormalTok{] }\OperatorTok{=} \FloatTok{2.0}\NormalTok{,}
\NormalTok{          plot}\OperatorTok{=}\VariableTok{False}\NormalTok{) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        X\_start (np.ndarray):}
\CommentTok{            A 2D array of shape (n, k) providing the initial Latin hypercube}
\CommentTok{            (n points in k dimensions).}
\CommentTok{        population (int):}
\CommentTok{            Number of offspring to create in each generation.}
\CommentTok{        iterations (int):}
\CommentTok{            Total number of generations to run the evolutionary search.}
\CommentTok{        q (float, optional):}
\CommentTok{            The exponent used by the Morris{-}Mitchell space{-}filling criterion.}
\CommentTok{            Defaults to 2.0.}
\CommentTok{        plot (bool, optional):}
\CommentTok{            If True, a simple scatter plot of the first two dimensions will be}
\CommentTok{            displayed at each iteration. Only if k \textgreater{}= 2. Defaults to False.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 2D array representing the most space{-}filling Latin hypercube found}
\CommentTok{            after all iterations, of the same shape as X\_start.}
\CommentTok{    """}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X\_start.shape[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes require at least 2 points"}\NormalTok{)}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X\_start.shape[}\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes are not defined for dim k \textless{} 2"}\NormalTok{)}
    \CommentTok{\# Initialize best plan and its metric}
\NormalTok{    X\_best }\OperatorTok{=}\NormalTok{ X\_start.copy()}
\NormalTok{    Phi\_best }\OperatorTok{=}\NormalTok{ mmphi(X\_best, q}\OperatorTok{=}\NormalTok{q)}
    \CommentTok{\# After 85\% of iterations, reduce the mutation rate to 1}
\NormalTok{    leveloff }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.floor(}\FloatTok{0.85} \OperatorTok{*}\NormalTok{ iterations))}
    \ControlFlowTok{for}\NormalTok{ it }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, iterations }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
        \CommentTok{\# Decrease number of mutations over time}
        \ControlFlowTok{if}\NormalTok{ it }\OperatorTok{\textless{}}\NormalTok{ leveloff:}
\NormalTok{            mutations }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(}\BuiltInTok{round}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\NormalTok{ (}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ (leveloff }\OperatorTok{{-}}\NormalTok{ it) }\OperatorTok{/}\NormalTok{ (leveloff }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)))}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            mutations }\OperatorTok{=} \DecValTok{1}
\NormalTok{        X\_improved }\OperatorTok{=}\NormalTok{ X\_best.copy()}
\NormalTok{        Phi\_improved }\OperatorTok{=}\NormalTok{ Phi\_best}
        \CommentTok{\# Create offspring, evaluate, and keep the best}
        \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(population):}
\NormalTok{            X\_try }\OperatorTok{=}\NormalTok{ perturb(X\_best.copy(), mutations)}
\NormalTok{            Phi\_try }\OperatorTok{=}\NormalTok{ mmphi(X\_try, q}\OperatorTok{=}\NormalTok{q)}

            \ControlFlowTok{if}\NormalTok{ Phi\_try }\OperatorTok{\textless{}}\NormalTok{ Phi\_improved:}
\NormalTok{                X\_improved }\OperatorTok{=}\NormalTok{ X\_try}
\NormalTok{                Phi\_improved }\OperatorTok{=}\NormalTok{ Phi\_try}
        \CommentTok{\# Update the global best if we found a better plan}
        \ControlFlowTok{if}\NormalTok{ Phi\_improved }\OperatorTok{\textless{}}\NormalTok{ Phi\_best:}
\NormalTok{            X\_best }\OperatorTok{=}\NormalTok{ X\_improved}
\NormalTok{            Phi\_best }\OperatorTok{=}\NormalTok{ Phi\_improved}
        \CommentTok{\# Simple visualization of the first two dimensions}
        \ControlFlowTok{if}\NormalTok{ plot }\KeywordTok{and}\NormalTok{ (X\_best.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{\textgreater{}=} \DecValTok{2}\NormalTok{):}
\NormalTok{            plt.clf()}
\NormalTok{            plt.scatter(X\_best[:, }\DecValTok{0}\NormalTok{], X\_best[:, }\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{)}
\NormalTok{            plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{            plt.title(}\SpecialStringTok{f"Iteration }\SpecialCharTok{\{}\NormalTok{it}\SpecialCharTok{\}}\SpecialStringTok{ {-} Current Best Plan"}\NormalTok{)}
\NormalTok{            plt.pause(}\FloatTok{0.01}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ X\_best}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{mmlhs}]\protect\hypertarget{exm-mmlhs}{}\label{exm-mmlhs}

The \texttt{mmlhs} function can be used to optimize a Latin hypercube
sampling plan. The following code demonstrates how to use \texttt{mmlhs}
to optimize a 4x2 Latin hypercube starting from an initial plan:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Suppose we have an initial 4x2 plan}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.3}\NormalTok{],[}\FloatTok{.1}\NormalTok{, }\FloatTok{.4}\NormalTok{],[}\FloatTok{.2}\NormalTok{, }\FloatTok{.9}\NormalTok{],[}\FloatTok{.9}\NormalTok{, }\FloatTok{.2}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Initial plan:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_start)}
\CommentTok{\# Search for a more space{-}filling plan}
\NormalTok{X\_opt }\OperatorTok{=}\NormalTok{ mmlhs(X\_start, population}\OperatorTok{=}\DecValTok{10}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{100}\NormalTok{, q}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Optimized plan:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(X\_opt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Initial plan:
[[0.1 0.3]
 [0.1 0.4]
 [0.2 0.9]
 [0.9 0.2]]
Optimized plan:
[[0.9 0.3]
 [0.1 0.9]
 [0.2 0.4]
 [0.1 0.2]]
\end{verbatim}

Figure~\ref{fig-forre08a-3} shows the initial and optimized plans in 2D.
The blue points represent the initial plan, while the red points
represent the optimized plan.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-forre08a-3-output-1.pdf}}

}

\caption{\label{fig-forre08a-3}Comparison of the initial and optimized
plans in 2D.}

\end{figure}%

\end{example}

\subsubsection{\texorpdfstring{The Function
\texttt{bestlh}}{The Function bestlh}}\label{the-function-bestlh}

Generates an optimized Latin hypercube by evolving the Morris-Mitchell
criterion across multiple exponents (q values) and selecting the best
plan.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ bestlh(n: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{           k: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{           population: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{           iterations: }\BuiltInTok{int}\NormalTok{,}
\NormalTok{           p}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{           plot}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{           verbosity}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{           edges}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{           q\_list}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{]) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""}
\CommentTok{    Args:}
\CommentTok{        n (int):}
\CommentTok{            Number of points required in the Latin hypercube.}
\CommentTok{        k (int):}
\CommentTok{            Number of design variables (dimensions).}
\CommentTok{        population (int):}
\CommentTok{            Number of offspring in each generation of the evolutionary search.}
\CommentTok{        iterations (int):}
\CommentTok{            Number of generations for the evolutionary search.}
\CommentTok{        p (int, optional):}
\CommentTok{            The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2).}
\CommentTok{            Defaults to 1 (faster than 2).}
\CommentTok{        plot (bool, optional):}
\CommentTok{            If True, a scatter plot of the optimized plan in the first two dimensions}
\CommentTok{            will be displayed. Only if k\textgreater{}=2.  Defaults to False.}
\CommentTok{        verbosity (int, optional):}
\CommentTok{            Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.}
\CommentTok{        edges (int, optional):}
\CommentTok{            If 1, places centers of the extreme bins at the domain edges ([0,1]).}
\CommentTok{            Otherwise, bins are fully contained within the domain, i.e. midpoints.}
\CommentTok{            Defaults to 0.}
\CommentTok{        q\_list (list, optional):}
\CommentTok{            A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100].}
\CommentTok{            These values are used to evaluate the space{-}fillingness of the Latin}
\CommentTok{            hypercube. The best plan is selected based on the lowest mmphi value.}

\CommentTok{    Returns:}
\CommentTok{        np.ndarray:}
\CommentTok{            A 2D array of shape (n, k) representing an optimized Latin hypercube.}
\CommentTok{    """}
    \ControlFlowTok{if}\NormalTok{ n }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes require at least 2 points"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Latin hypercubes are not defined for dim k \textless{} 2"}\NormalTok{)}

    \CommentTok{\# A list of exponents (q) to optimize}

    \CommentTok{\# Start with a random Latin hypercube}
\NormalTok{    X\_start }\OperatorTok{=}\NormalTok{ rlh(n, k, edges}\OperatorTok{=}\NormalTok{edges)}

    \CommentTok{\# Allocate a 3D array to store the results for each q}
    \CommentTok{\# (shape: (n, k, number\_of\_q\_values))}
\NormalTok{    X3D }\OperatorTok{=}\NormalTok{ np.zeros((n, k, }\BuiltInTok{len}\NormalTok{(q\_list)))}

    \CommentTok{\# Evolve the plan for each q in q\_list}
    \ControlFlowTok{for}\NormalTok{ i, q\_val }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(q\_list):}
        \ControlFlowTok{if}\NormalTok{ verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Now optimizing for q=}\SpecialCharTok{\{}\NormalTok{q\_val}\SpecialCharTok{\}}\SpecialStringTok{..."}\NormalTok{)}
\NormalTok{        X3D[:, :, i] }\OperatorTok{=}\NormalTok{ mmlhs(X\_start, population, iterations, q\_val)}

    \CommentTok{\# Sort the set of evolved plans according to the Morris{-}Mitchell criterion}
\NormalTok{    index\_order }\OperatorTok{=}\NormalTok{ mmsort(X3D, p}\OperatorTok{=}\NormalTok{p)}

    \CommentTok{\# index\_order is a 1{-}based array of plan indices; the first element is the best}
\NormalTok{    best\_idx }\OperatorTok{=}\NormalTok{ index\_order[}\DecValTok{0}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
    \ControlFlowTok{if}\NormalTok{ verbosity }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best lh found using q=}\SpecialCharTok{\{}\NormalTok{q\_list[best\_idx]}\SpecialCharTok{\}}\SpecialStringTok{..."}\NormalTok{)}

    \CommentTok{\# The best plan in 3D array order}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ X3D[:, :, best\_idx]}

    \CommentTok{\# Plot the first two dimensions}
    \ControlFlowTok{if}\NormalTok{ plot }\KeywordTok{and}\NormalTok{ (k }\OperatorTok{\textgreater{}=} \DecValTok{2}\NormalTok{):}
\NormalTok{        plt.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\StringTok{"r"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{)}
\NormalTok{        plt.title(}\SpecialStringTok{f"Morris{-}Mitchell optimum plan found using q=}\SpecialCharTok{\{}\NormalTok{q\_list[best\_idx]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{        plt.xlabel(}\StringTok{"x\_1"}\NormalTok{)}
\NormalTok{        plt.ylabel(}\StringTok{"x\_2"}\NormalTok{)}
\NormalTok{        plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{        plt.show()}

    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\begin{example}[The Function
\texttt{bestlh}]\protect\hypertarget{exm-bestlh}{}\label{exm-bestlh}

The \texttt{bestlh} function can be used to generate an optimized Latin
hypercube sampling plan. The following code demonstrates how to use
\texttt{bestlh} to create a 5x2 Latin hypercube with a population of 5
and 10 iterations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Xbestlh}\OperatorTok{=}\NormalTok{ bestlh(n}\OperatorTok{=}\DecValTok{5}\NormalTok{, k}\OperatorTok{=}\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{5}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Figure~\ref{fig-forre08a-4} shows the best Latin hypercube sampling in
2D. The red points represent the optimized plan.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-forre08a-4-output-1.pdf}}

}

\caption{\label{fig-forre08a-4}Best Latin Hypercube Sampling}

\end{figure}%

\end{example}

Sorting all candidate plans in ascending order is not strictly necessary
- after all, only the best one is truly of interest. Nonetheless, the
added computational complexity is minimal (the vector will only ever
contain as many elements as there are candidate \(q\) values, and only
an index array is sorted, not the actual repository of plans). This
sorting gives the reader the opportunity to compare, if desired, how
different choices of \(q\) influence the resulting plans.

\section{Experimental Analysis of the Morris-Mitchell
Criterion}\label{experimental-analysis-of-the-morris-mitchell-criterion}

Morris-Mitchell Criterion Experimental Analysis

\begin{itemize}
\tightlist
\item
  Number of points: 16, Dimensions: 2
\item
  mmphi parameters: q (exponent) = 2.0, p (distance norm) = 2.0
  (1=Manhattan, 2=Euclidean)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_POINTS }\OperatorTok{=} \DecValTok{16}
\NormalTok{N\_DIM }\OperatorTok{=} \DecValTok{2}
\NormalTok{RANDOM\_SEED }\OperatorTok{=} \DecValTok{42}
\NormalTok{q }\OperatorTok{=} \FloatTok{2.0}
\NormalTok{p }\OperatorTok{=} \FloatTok{2.0}
\end{Highlighting}
\end{Shaded}

\subsection{Evaluation of Sampling
Designs}\label{evaluation-of-sampling-designs}

We generate various sampling designs and evaluate their space-filling
properties using the Morris-Mitchell criterion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{designs }\OperatorTok{=}\NormalTok{ \{\}}
\ControlFlowTok{if} \BuiltInTok{int}\NormalTok{(np.sqrt(N\_POINTS))}\OperatorTok{**}\DecValTok{2} \OperatorTok{==}\NormalTok{ N\_POINTS:}
\NormalTok{    grid\_design }\OperatorTok{=}\NormalTok{ Grid(k}\OperatorTok{=}\NormalTok{N\_DIM)}
\NormalTok{    designs[}\StringTok{"Grid (4x4)"}\NormalTok{] }\OperatorTok{=}\NormalTok{ grid\_design.generate\_grid\_design(points\_per\_dim}\OperatorTok{=}\BuiltInTok{int}\NormalTok{(np.sqrt(N\_POINTS)))}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Skipping grid design as N\_POINTS=}\SpecialCharTok{\{}\NormalTok{N\_POINTS}\SpecialCharTok{\}}\SpecialStringTok{ is not a perfect square for a simple 2D grid."}\NormalTok{)}

\NormalTok{lhs\_design }\OperatorTok{=}\NormalTok{ SpaceFilling(k}\OperatorTok{=}\NormalTok{N\_DIM, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{designs[}\StringTok{"LHS"}\NormalTok{] }\OperatorTok{=}\NormalTok{ lhs\_design.generate\_qms\_lhs\_design(n\_points}\OperatorTok{=}\NormalTok{N\_POINTS)}

\NormalTok{sobol\_design }\OperatorTok{=}\NormalTok{ Sobol(k}\OperatorTok{=}\NormalTok{N\_DIM, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{designs[}\StringTok{"Sobol"}\NormalTok{] }\OperatorTok{=}\NormalTok{ sobol\_design.generate\_sobol\_design(n\_points}\OperatorTok{=}\NormalTok{N\_POINTS)}

\NormalTok{random\_design }\OperatorTok{=}\NormalTok{ Random(k}\OperatorTok{=}\NormalTok{N\_DIM)}
\NormalTok{designs[}\StringTok{"Random"}\NormalTok{] }\OperatorTok{=}\NormalTok{ random\_design.uniform(n\_points}\OperatorTok{=}\NormalTok{N\_POINTS)}

\NormalTok{poor\_design }\OperatorTok{=}\NormalTok{ Poor(k}\OperatorTok{=}\NormalTok{N\_DIM)}
\NormalTok{designs[}\StringTok{"Collinear"}\NormalTok{] }\OperatorTok{=}\NormalTok{ poor\_design.generate\_collinear\_design(n\_points}\OperatorTok{=}\NormalTok{N\_POINTS)}

\NormalTok{clustered\_design }\OperatorTok{=}\NormalTok{ Clustered(k}\OperatorTok{=}\NormalTok{N\_DIM)}
\NormalTok{designs[}\StringTok{"Clustered (3 clusters)"}\NormalTok{] }\OperatorTok{=}\NormalTok{ clustered\_design.generate\_clustered\_design(n\_points}\OperatorTok{=}\NormalTok{N\_POINTS, n\_clusters}\OperatorTok{=}\DecValTok{3}\NormalTok{, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\NormalTok{results }\OperatorTok{=}\NormalTok{ \{\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Calculating Morris{-}Mitchell metric (smaller is better):"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ name, X\_design }\KeywordTok{in}\NormalTok{ designs.items():}
\NormalTok{    metric\_val }\OperatorTok{=}\NormalTok{ mmphi(X\_design, q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}
\NormalTok{    results[name] }\OperatorTok{=}\NormalTok{ metric\_val}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  }\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{metric\_val}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Calculating Morris-Mitchell metric (smaller is better):
  Grid (4x4): 20.2617
  LHS: 28.1868
  Sobol: 28.8942
  Random: 46.7319
  Collinear: 87.8829
  Clustered (3 clusters): 90.3702
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ N\_DIM }\OperatorTok{==} \DecValTok{2}\NormalTok{:}
\NormalTok{    num\_designs }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(designs)}
\NormalTok{    cols }\OperatorTok{=} \DecValTok{2}
\NormalTok{    rows }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.ceil(num\_designs }\OperatorTok{/}\NormalTok{ cols))}
\NormalTok{    fig, axes }\OperatorTok{=}\NormalTok{ plt.subplots(rows, cols, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5} \OperatorTok{*}\NormalTok{ cols, }\DecValTok{5} \OperatorTok{*}\NormalTok{ rows))}
\NormalTok{    axes }\OperatorTok{=}\NormalTok{ axes.ravel() }\CommentTok{\# Flatten axes array for easy iteration}

    \ControlFlowTok{for}\NormalTok{ i, (name, X\_design) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(designs.items()):}
\NormalTok{        ax }\OperatorTok{=}\NormalTok{ axes[i]}
\NormalTok{        ax.scatter(X\_design[:, }\DecValTok{0}\NormalTok{], X\_design[:, }\DecValTok{1}\NormalTok{], s}\OperatorTok{=}\DecValTok{50}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\NormalTok{        ax.set\_title(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{mmphi = }\SpecialCharTok{\{}\NormalTok{results[name]}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{        ax.set\_xlabel(}\StringTok{"X1"}\NormalTok{)}
\NormalTok{        ax.set\_ylabel(}\StringTok{"X2"}\NormalTok{)}
\NormalTok{        ax.set\_xlim(}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{1.05}\NormalTok{)}
\NormalTok{        ax.set\_ylim(}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{1.05}\NormalTok{)}
\NormalTok{        ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{, adjustable}\OperatorTok{=}\StringTok{\textquotesingle{}box\textquotesingle{}}\NormalTok{)}
\NormalTok{        ax.grid(}\VariableTok{True}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}

    \CommentTok{\# Hide any unused subplots}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i }\OperatorTok{+} \DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(axes)):}
\NormalTok{        fig.delaxes(axes[j])}

\NormalTok{    plt.tight\_layout()}
\NormalTok{    plt.suptitle(}\SpecialStringTok{f"Comparison of 2D Sampling Designs (}\SpecialCharTok{\{}\NormalTok{N\_POINTS}\SpecialCharTok{\}}\SpecialStringTok{ points each)"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{, y}\OperatorTok{=}\FloatTok{1.02}\NormalTok{)}
\NormalTok{    plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/cell-43-output-1.pdf}}

\subsection{Demonstrate the Impact of mmphi
Parameters}\label{demonstrate-the-impact-of-mmphi-parameters}

Demonstrating Impact of mmphi Parameters on `LHS' Design

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_lhs }\OperatorTok{=}\NormalTok{ designs[}\StringTok{"LHS"}\NormalTok{]}

\CommentTok{\# 1. Default parameters (already calculated)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  LHS (q=}\SpecialCharTok{\{}\NormalTok{q}\SpecialCharTok{\}}\SpecialStringTok{, p=}\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{ Euclidean): }\SpecialCharTok{\{}\NormalTok{results[}\StringTok{\textquotesingle{}LHS\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# 2. Change q (main exponent, literature\textquotesingle{}s p or k)}
\NormalTok{q\_high }\OperatorTok{=} \FloatTok{15.0}
\NormalTok{metric\_lhs\_q\_high }\OperatorTok{=}\NormalTok{ mmphi(X\_lhs, q}\OperatorTok{=}\NormalTok{q\_high, p}\OperatorTok{=}\NormalTok{p)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  LHS (q=}\SpecialCharTok{\{}\NormalTok{q\_high}\SpecialCharTok{\}}\SpecialStringTok{, p=}\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{ Euclidean): }\SpecialCharTok{\{}\NormalTok{metric\_lhs\_q\_high}\SpecialCharTok{:.4f\}}\SpecialStringTok{ (Higher q penalizes small distances more)"}\NormalTok{)}

\CommentTok{\# 3. Change p (distance norm, literature\textquotesingle{}s q or m)}
\NormalTok{p\_manhattan }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{metric\_lhs\_p\_manhattan }\OperatorTok{=}\NormalTok{ mmphi(X\_lhs, q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p\_manhattan)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  LHS (q=}\SpecialCharTok{\{}\NormalTok{q}\SpecialCharTok{\}}\SpecialStringTok{, p=}\SpecialCharTok{\{}\NormalTok{p\_manhattan}\SpecialCharTok{\}}\SpecialStringTok{ Manhattan): }\SpecialCharTok{\{}\NormalTok{metric\_lhs\_p\_manhattan}\SpecialCharTok{:.4f\}}\SpecialStringTok{ (Using L1 distance)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  LHS (q=2.0, p=2.0 Euclidean): 28.1868
  LHS (q=15.0, p=2.0 Euclidean): 8.1573 (Higher q penalizes small distances more)
  LHS (q=2.0, p=1.0 Manhattan): 22.0336 (Using L1 distance)
\end{verbatim}

\subsection{Morris-Mitchell Criterion: Impact of Adding
Points}\label{morris-mitchell-criterion-impact-of-adding-points}

Impact of adding a point to a 2x2 grid design

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initial 2x2 Grid Design}
\NormalTok{X\_initial }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{], [}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{], [}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{], [}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{]])}
\NormalTok{mmphi\_initial }\OperatorTok{=}\NormalTok{ mmphi(X\_initial, q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Parameters: q (exponent) = }\SpecialCharTok{\{}\NormalTok{q}\SpecialCharTok{\}}\SpecialStringTok{, p (distance) = }\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{ (Euclidean)}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Initial 2x2 Grid Design (4 points):"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Points:}\CharTok{\textbackslash{}n}\SpecialCharTok{\{}\NormalTok{X\_initial}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Morris{-}Mitchell Criterion (Phi\_q): }\SpecialCharTok{\{}\NormalTok{mmphi\_initial}\SpecialCharTok{:.4f\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Parameters: q (exponent) = 2.0, p (distance) = 2.0 (Euclidean)

Initial 2x2 Grid Design (4 points):
  Points:
[[0. 0.]
 [1. 0.]
 [0. 1.]
 [1. 1.]]
  Morris-Mitchell Criterion (Phi_q): 2.2361
\end{verbatim}

Scenarios for adding a 5th point:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scenarios }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"Scenario 1: Add to Center"}\NormalTok{: \{}
        \StringTok{"new\_point"}\NormalTok{: np.array([[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{]]),}
        \StringTok{"description"}\NormalTok{: }\StringTok{"Adding a point in the center of the grid."}
\NormalTok{    \},}
    \StringTok{"Scenario 2: Add Close to Existing (Cluster)"}\NormalTok{: \{}
        \StringTok{"new\_point"}\NormalTok{: np.array([[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{]]),}
        \StringTok{"description"}\NormalTok{: }\StringTok{"Adding a point very close to an existing point (0,0)."}
\NormalTok{    \},}
    \StringTok{"Scenario 3: Add on Edge"}\NormalTok{: \{}
        \StringTok{"new\_point"}\NormalTok{: np.array([[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.0}\NormalTok{]]),}
        \StringTok{"description"}\NormalTok{: }\StringTok{"Adding a point on an edge between (0,0) and (1,0)."}
\NormalTok{    \}}
\NormalTok{\}}

\NormalTok{results\_summary }\OperatorTok{=}\NormalTok{ []}
\NormalTok{augmented\_designs\_for\_plotting }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Initial Design"}\NormalTok{: X\_initial\}}

\ControlFlowTok{for}\NormalTok{ name, scenario\_details }\KeywordTok{in}\NormalTok{ scenarios.items():}
\NormalTok{    new\_point }\OperatorTok{=}\NormalTok{ scenario\_details[}\StringTok{"new\_point"}\NormalTok{]}
\NormalTok{    X\_augmented }\OperatorTok{=}\NormalTok{ np.vstack((X\_initial, new\_point))}
\NormalTok{    augmented\_designs\_for\_plotting[name] }\OperatorTok{=}\NormalTok{ X\_augmented}
    
\NormalTok{    mmphi\_augmented }\OperatorTok{=}\NormalTok{ mmphi(X\_augmented, q}\OperatorTok{=}\NormalTok{q, p}\OperatorTok{=}\NormalTok{p)}
\NormalTok{    change }\OperatorTok{=}\NormalTok{ mmphi\_augmented }\OperatorTok{{-}}\NormalTok{ mmphi\_initial}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Description: }\SpecialCharTok{\{}\NormalTok{scenario\_details[}\StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  New Point Added: }\SpecialCharTok{\{}\NormalTok{new\_point}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \CommentTok{\# print(f"  Augmented Design (5 points):\textbackslash{}n\{X\_augmented\}") \# Optional: print full matrix}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Morris{-}Mitchell Criterion (Phi\_q): }\SpecialCharTok{\{}\NormalTok{mmphi\_augmented}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Change from Initial Phi\_q: }\SpecialCharTok{\{}\NormalTok{change}\SpecialCharTok{:+.4f\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
    
\NormalTok{    results\_summary.append(\{}
        \StringTok{"Scenario"}\NormalTok{: name,}
        \StringTok{"Initial Phi\_q"}\NormalTok{: mmphi\_initial,}
        \StringTok{"Augmented Phi\_q"}\NormalTok{: mmphi\_augmented,}
        \StringTok{"Change"}\NormalTok{: change}
\NormalTok{    \})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Scenario 1: Add to Center:
  Description: Adding a point in the center of the grid.
  New Point Added: [[0.5 0.5]]
  Morris-Mitchell Criterion (Phi_q): 3.6056
  Change from Initial Phi_q: +1.3695

Scenario 2: Add Close to Existing (Cluster):
  Description: Adding a point very close to an existing point (0,0).
  New Point Added: [[0.1 0.1]]
  Morris-Mitchell Criterion (Phi_q): 7.6195
  Change from Initial Phi_q: +5.3834

Scenario 3: Add on Edge:
  Description: Adding a point on an edge between (0,0) and (1,0).
  New Point Added: [[0.5 0. ]]
  Morris-Mitchell Criterion (Phi_q): 3.8210
  Change from Initial Phi_q: +1.5849
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_designs }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(augmented\_designs\_for\_plotting)}
\NormalTok{cols }\OperatorTok{=} \DecValTok{2}
\NormalTok{rows }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.ceil(num\_designs }\OperatorTok{/}\NormalTok{ cols))}

\NormalTok{fig, axes }\OperatorTok{=}\NormalTok{ plt.subplots(rows, cols, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6} \OperatorTok{*}\NormalTok{ cols, }\DecValTok{5} \OperatorTok{*}\NormalTok{ rows))}
\NormalTok{axes }\OperatorTok{=}\NormalTok{ axes.ravel() }

\NormalTok{plot\_idx }\OperatorTok{=} \DecValTok{0}
\CommentTok{\# Plot initial design first}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ axes[plot\_idx]}
\NormalTok{ax.scatter(X\_initial[:, }\DecValTok{0}\NormalTok{], X\_initial[:, }\DecValTok{1}\NormalTok{], s}\OperatorTok{=}\DecValTok{100}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{, label}\OperatorTok{=}\StringTok{"Original Points"}\NormalTok{)}
\NormalTok{ax.set\_title(}\SpecialStringTok{f"Initial Design}\CharTok{\textbackslash{}n}\SpecialStringTok{Phi\_q = }\SpecialCharTok{\{}\NormalTok{mmphi\_initial}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"X1"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"X2"}\NormalTok{)}
\NormalTok{ax.set\_xlim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
\NormalTok{ax.set\_ylim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
\NormalTok{ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{, adjustable}\OperatorTok{=}\StringTok{\textquotesingle{}box\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.grid(}\VariableTok{True}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}
\NormalTok{ax.legend(fontsize}\OperatorTok{=}\StringTok{\textquotesingle{}small\textquotesingle{}}\NormalTok{)}
\NormalTok{plot\_idx }\OperatorTok{+=}\DecValTok{1}

\CommentTok{\# Plot augmented designs}
\ControlFlowTok{for}\NormalTok{ name, X\_design }\KeywordTok{in}\NormalTok{ augmented\_designs\_for\_plotting.items():}
    \ControlFlowTok{if}\NormalTok{ name }\OperatorTok{==} \StringTok{"Initial Design"}\NormalTok{:}
        \ControlFlowTok{continue} \CommentTok{\# Already plotted}

\NormalTok{    ax }\OperatorTok{=}\NormalTok{ axes[plot\_idx]}
    \CommentTok{\# Highlight original vs new point}
\NormalTok{    original\_points }\OperatorTok{=}\NormalTok{ X\_design[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, :]}
\NormalTok{    new\_point }\OperatorTok{=}\NormalTok{ X\_design[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, :].reshape(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)}
    
\NormalTok{    ax.scatter(original\_points[:, }\DecValTok{0}\NormalTok{], original\_points[:, }\DecValTok{1}\NormalTok{], s}\OperatorTok{=}\DecValTok{100}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{, label}\OperatorTok{=}\StringTok{"Original Points"}\NormalTok{)}
\NormalTok{    ax.scatter(new\_point[:, }\DecValTok{0}\NormalTok{], new\_point[:, }\DecValTok{1}\NormalTok{], s}\OperatorTok{=}\DecValTok{150}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"Added Point"}\NormalTok{)}
    
\NormalTok{    current\_phi\_q }\OperatorTok{=} \BuiltInTok{next}\NormalTok{(item[}\StringTok{\textquotesingle{}Augmented Phi\_q\textquotesingle{}}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ item }\KeywordTok{in}\NormalTok{ results\_summary }\ControlFlowTok{if}\NormalTok{ item[}\StringTok{"Scenario"}\NormalTok{] }\OperatorTok{==}\NormalTok{ name)}
\NormalTok{    ax.set\_title(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{Phi\_q = }\SpecialCharTok{\{}\NormalTok{current\_phi\_q}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    ax.set\_xlabel(}\StringTok{"X1"}\NormalTok{)}
\NormalTok{    ax.set\_ylabel(}\StringTok{"X2"}\NormalTok{)}
\NormalTok{    ax.set\_xlim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
\NormalTok{    ax.set\_ylim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
\NormalTok{    ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{, adjustable}\OperatorTok{=}\StringTok{\textquotesingle{}box\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.grid(}\VariableTok{True}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}
\NormalTok{    ax.legend(fontsize}\OperatorTok{=}\StringTok{\textquotesingle{}small\textquotesingle{}}\NormalTok{)}
\NormalTok{    plot\_idx }\OperatorTok{+=}\DecValTok{1}
    
\CommentTok{\# Hide any unused subplots}
\ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(plot\_idx, }\BuiltInTok{len}\NormalTok{(axes)):}
\NormalTok{    fig.delaxes(axes[j])}

\NormalTok{plt.tight\_layout(rect}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.96}\NormalTok{]) }\CommentTok{\# Adjust layout to make space for suptitle}
\NormalTok{plt.suptitle(}\SpecialStringTok{f"Impact of Adding a Point to a 2x2 Grid Design (q=}\SpecialCharTok{\{}\NormalTok{q}\SpecialCharTok{\}}\SpecialStringTok{, p=}\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/cell-47-output-1.pdf}}

Summary Table (Conceptual):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4819}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1807}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2048}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1325}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Initial Phi\_q
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Augmented Phi\_q
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Change
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Baseline (2x2 Grid) & 2.236 & --- & --- \\
Scenario 1: Add to Center & 2.236 & 3.606 & +1.369 \\
Scenario 2: Add Close to Existing (Cluster) & 2.236 & 7.619 & +5.383 \\
Scenario 3: Add on Edge & 2.236 & 3.821 & +1.585 \\
\end{longtable}

\section{A Sample-Size Invariant Version of the Morris-Mitchell
Criterion}\label{a-sample-size-invariant-version-of-the-morris-mitchell-criterion}

\subsection{\texorpdfstring{Comparison of \texttt{mmphi()} and
\texttt{mmphi\_intensive()}}{Comparison of mmphi() and mmphi\_intensive()}}\label{comparison-of-mmphi-and-mmphi_intensive}

The Morris-Mitchell criterion is a widely used metric for evaluating the
space-filling properties of Latin hypercube sampling designs. However,
it is sensitive to the number of points in the design, which can lead to
misleading comparisons between designs with different sample sizes. To
address this issue, a sample-size invariant version of the
Morris-Mitchell criterion has been proposed. It is avaiable in the
\texttt{spotpython} package as \texttt{mmphi\_intensive()}, see
\href{https://sequential-parameter-optimization.github.io/spotPython/reference/spotpython/utils/sampling/\#spotpython.utils.sampling.mmphi_intensive}{{[}SOURCE{]}}.

The functions \texttt{mmphi()} and \texttt{mmphi\_intensive()} both
calculate a Morris-Mitchell criterion, but they differ in their
normalization, which makes \texttt{mmphi\_intensive()} invariant to the
sample size.

Let \(X\) be a sampling plan with \(n\) points
\(\{x_1, x_2, \dots, x_n\}\) in a \(k\)-dimensional space. Let
\(d_{ij} = \|x_i - x_j\|_p\) be the \(p\)-norm distance between points
\(x_i\) and \(x_j\). Let \(J_l\) be the multiplicity of the \(l\)-th
unique distance \(d_l\) among all pairs of points in \(X\). Let \(m\) be
the total number of unique distances.

\textbf{1. \texttt{mmphi()} (Morris-Mitchell Criterion \(\Phi_q\))}

The \texttt{mmphi()} function, as defined in the context and implemented
in \texttt{sampling.py}, calculates the Morris-Mitchell criterion
\(\Phi_q\) as:

\[
\Phi_q(X) = \left( \sum_{l=1}^{m} J_l d_l^{-q} \right)^{1/q},
\] where:

\begin{itemize}
\tightlist
\item
  \(J_l\) is the number of pairs of points separated by the unique
  distance \(d_l\).
\item
  \(d_l\) are the unique pairwise distances.
\item
  \(q\) is a user-defined exponent (typically \(q > 0\)).
\end{itemize}

This formulation is directly based on the sum of inverse powers of
distances. The value of \(\Phi_q\) is generally dependent on the number
of points \(n\) in the design \(X\), as the sum \(\sum J_l d_l^{-q}\)
will typically increase with more points (and thus more pairs).

\textbf{2. \texttt{mmphi\_intensive()} (Intensive Morris-Mitchell
Criterion)}

The \texttt{mmphi\_intensive()} function, as implemented in
\texttt{sampling.py} calculates a sample-size invariant version of the
Morris-Mitchell criterion, which will be referred to as \(\Phi_q^{I}\).
The formula is:

\[
\Phi_q^{I}(X) = \left( \frac{1}{M} \sum_{l=1}^{m} J_l d_l^{-q} \right)^{1/q}
\]

where:

\begin{itemize}
\tightlist
\item
  \(M = \binom{n}{2} = \frac{n(n-1)}{2}\) is the total number of unique
  pairs of points in the design \(X\).
\item
  The other terms \(J_l\), \(d_l\), \(q\) are the same as in
  \texttt{mmphi()}.
\end{itemize}

The key mathematical difference is the normalization factor
\(\frac{1}{M}\) inside the parentheses before the outer exponent \(1/q\)
is applied.

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{mmphi()}}: Calculates
  \(\left( \text{SumTerm} \right)^{1/q}\), where SumTerm =
  \(\sum J_l d_l^{-q}\).
\item
  \textbf{\texttt{mmphi\_intensive()}}: Calculates
  \(\left( \frac{\text{SumTerm}}{M} \right)^{1/q}\).
\end{itemize}

By dividing the sum \(\sum J_l d_l^{-q}\) by \(M\) (the total number of
pairs), \texttt{mmphi\_intensive()} effectively calculates an
\emph{average} contribution per pair to the \(-q\)-th power of distance,
before taking the \(q\)-th root. This normalization makes the criterion
less dependent on the absolute number of points \(n\) and allows for
more meaningful comparisons of space-fillingness between designs of
different sizes. A smaller value indicates a better (more space-filling)
design for both criteria.

\subsection{Plotting the Two Morris-Mitchell Criteria for Different
Sample
Sizes}\label{plotting-the-two-morris-mitchell-criteria-for-different-sample-sizes}

Figure~\ref{fig-forre08a-6} shows the comparison of the two
Morris-Mitchell criteria for different sample sizes using the
\texttt{plot\_mmphi\_vs\_n\_lhs} function. The red line represents the
standard Morris-Mitchell criterion, while the blue line represents the
sample-size invariant version. Note the difference in the y-axis scales,
which highlights how the sample-size invariant version remains
consistent across varying sample sizes.

\phantomsection\label{code-forre08a-6}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_mmphi\_vs\_n\_lhs(k\_dim: }\BuiltInTok{int}\NormalTok{, }
\NormalTok{                        seed: }\BuiltInTok{int}\NormalTok{, }
\NormalTok{                        n\_min: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{10}\NormalTok{, }
\NormalTok{                        n\_max: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{100}\NormalTok{, }
\NormalTok{                        n\_step: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{5}\NormalTok{,}
\NormalTok{                        q\_phi: }\BuiltInTok{float} \OperatorTok{=} \FloatTok{2.0}\NormalTok{, }
\NormalTok{                        p\_phi: }\BuiltInTok{float} \OperatorTok{=} \FloatTok{2.0}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Generates LHS designs for varying n, calculates mmphi and mmphi\_intensive,}
\CommentTok{    and plots them against the number of samples (n).}

\CommentTok{    Args:}
\CommentTok{        k\_dim (int): Number of dimensions for the LHS design.}
\CommentTok{        seed (int): Random seed for reproducibility.}
\CommentTok{        n\_min (int): Minimum number of samples.}
\CommentTok{        n\_max (int): Maximum number of samples.}
\CommentTok{        n\_step (int): Step size for increasing n.}
\CommentTok{        q\_phi (float): Exponent q for the Morris{-}Mitchell criteria.}
\CommentTok{        p\_phi (float): Distance norm p for the Morris{-}Mitchell criteria.}
\CommentTok{    """}
\NormalTok{    n\_values }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(n\_min, n\_max }\OperatorTok{+} \DecValTok{1}\NormalTok{, n\_step))}
    \ControlFlowTok{if} \KeywordTok{not}\NormalTok{ n\_values:}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Warning: n\_values list is empty. Check n\_min, n\_max, and n\_step."}\NormalTok{)}
        \ControlFlowTok{return}
\NormalTok{    mmphi\_results }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    mmphi\_intensive\_results }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    lhs\_generator }\OperatorTok{=}\NormalTok{ SpaceFilling(k}\OperatorTok{=}\NormalTok{k\_dim, seed}\OperatorTok{=}\NormalTok{seed)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Calculating for n from }\SpecialCharTok{\{}\NormalTok{n\_min}\SpecialCharTok{\}}\SpecialStringTok{ to }\SpecialCharTok{\{}\NormalTok{n\_max}\SpecialCharTok{\}}\SpecialStringTok{ with step }\SpecialCharTok{\{}\NormalTok{n\_step}\SpecialCharTok{\}}\SpecialStringTok{..."}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ n\_points }\KeywordTok{in}\NormalTok{ n\_values:}
        \ControlFlowTok{if}\NormalTok{ n\_points }\OperatorTok{\textless{}} \DecValTok{2}\NormalTok{ : }\CommentTok{\# mmphi requires at least 2 points to calculate distances}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Skipping n=}\SpecialCharTok{\{}\NormalTok{n\_points}\SpecialCharTok{\}}\SpecialStringTok{ as it\textquotesingle{}s less than 2."}\NormalTok{)}
\NormalTok{            mmphi\_results.append(np.nan)}
\NormalTok{            mmphi\_intensive\_results.append(np.nan)}
            \ControlFlowTok{continue}
        \ControlFlowTok{try}\NormalTok{:}
\NormalTok{            X\_design }\OperatorTok{=}\NormalTok{ lhs\_generator.generate\_qms\_lhs\_design(n\_points}\OperatorTok{=}\NormalTok{n\_points)}
\NormalTok{            phi }\OperatorTok{=}\NormalTok{ mmphi(X\_design, q}\OperatorTok{=}\NormalTok{q\_phi, p}\OperatorTok{=}\NormalTok{p\_phi)}
\NormalTok{            phi\_intensive, \_, \_ }\OperatorTok{=}\NormalTok{ mmphi\_intensive(X\_design, q}\OperatorTok{=}\NormalTok{q\_phi, p}\OperatorTok{=}\NormalTok{p\_phi)}
\NormalTok{            mmphi\_results.append(phi)}
\NormalTok{            mmphi\_intensive\_results.append(phi\_intensive)}
        \ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Error calculating for n=}\SpecialCharTok{\{}\NormalTok{n\_points}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{e}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{            mmphi\_results.append(np.nan)}
\NormalTok{            mmphi\_intensive\_results.append(np.nan)}

\NormalTok{    fig, ax1 }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{    color }\OperatorTok{=} \StringTok{\textquotesingle{}tab:red\textquotesingle{}}
\NormalTok{    ax1.set\_xlabel(}\StringTok{\textquotesingle{}Number of Samples (n)\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax1.set\_ylabel(}\StringTok{\textquotesingle{}mmphi (Phiq)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\NormalTok{color)}
\NormalTok{    ax1.plot(n\_values, mmphi\_results, color}\OperatorTok{=}\NormalTok{color, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}mmphi (Phiq)\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax1.tick\_params(axis}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{, labelcolor}\OperatorTok{=}\NormalTok{color)}
\NormalTok{    ax1.grid(}\VariableTok{True}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}

\NormalTok{    ax2 }\OperatorTok{=}\NormalTok{ ax1.twinx()  }\CommentTok{\# instantiate a second axes that shares the same x{-}axis}
\NormalTok{    color }\OperatorTok{=} \StringTok{\textquotesingle{}tab:blue\textquotesingle{}}
\NormalTok{    ax2.set\_ylabel(}\StringTok{\textquotesingle{}mmphi\_intensive (PhiqI)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\NormalTok{color)  }\CommentTok{\# we already handled the x{-}label with ax1}
\NormalTok{    ax2.plot(n\_values, mmphi\_intensive\_results, color}\OperatorTok{=}\NormalTok{color, marker}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}mmphi\_intensive (PhiqI)\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax2.tick\_params(axis}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{, labelcolor}\OperatorTok{=}\NormalTok{color)}

\NormalTok{    fig.tight\_layout()  }\CommentTok{\# otherwise the right y{-}label is slightly clipped}
\NormalTok{    plt.title(}\SpecialStringTok{f\textquotesingle{}Morris{-}Mitchell Criteria vs. Number of Samples (n)}\CharTok{\textbackslash{}n}\SpecialStringTok{LHS (k=}\SpecialCharTok{\{}\NormalTok{k\_dim}\SpecialCharTok{\}}\SpecialStringTok{, q=}\SpecialCharTok{\{}\NormalTok{q\_phi}\SpecialCharTok{\}}\SpecialStringTok{, p=}\SpecialCharTok{\{}\NormalTok{p\_phi}\SpecialCharTok{\}}\SpecialStringTok{)\textquotesingle{}}\NormalTok{)}
    \CommentTok{\# Add legends}
\NormalTok{    lines, labels }\OperatorTok{=}\NormalTok{ ax1.get\_legend\_handles\_labels()}
\NormalTok{    lines2, labels2 }\OperatorTok{=}\NormalTok{ ax2.get\_legend\_handles\_labels()}
\NormalTok{    ax2.legend(lines }\OperatorTok{+}\NormalTok{ lines2, labels }\OperatorTok{+}\NormalTok{ labels2, loc}\OperatorTok{=}\StringTok{\textquotesingle{}best\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_DIM }\OperatorTok{=} \DecValTok{2}
\NormalTok{RANDOM\_SEED }\OperatorTok{=} \DecValTok{42}
\NormalTok{plot\_mmphi\_vs\_n\_lhs(k\_dim}\OperatorTok{=}\NormalTok{N\_DIM, seed}\OperatorTok{=}\NormalTok{RANDOM\_SEED, n\_min}\OperatorTok{=}\DecValTok{10}\NormalTok{, n\_max}\OperatorTok{=}\DecValTok{100}\NormalTok{, n\_step}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Calculating for n from 10 to 100 with step 5...
\end{verbatim}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{001_sampling_files/figure-pdf/fig-forre08a-6-output-2.pdf}}

}

\caption{\label{fig-forre08a-6}Comparison of the two Morris-Mitchell
Criteria for Different Sample Sizes}

\end{figure}%

\section{Jupyter Notebook}\label{jupyter-notebook-1}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/001_sampling.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Constructing a Surrogate}\label{constructing-a-surrogate}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

This section is based on chapter 2 in (\textbf{Forr08a?}).

\end{tcolorbox}

\begin{definition}[Black Box
Problem]\protect\hypertarget{def-black-box}{}\label{def-black-box}

We are trying to learn a mapping that converts the vector \(\vec{x}\)
into a scalar output \(y\), i.e., we are trying to learn a function \[
y = f(x).
\] If function is hidden (``lives in a black box''), so that the physics
of the problem is not known, the problem is called a black box problem.

\end{definition}

This black box could take the form of either a physical or computer
experiment, for example, a finite element code, which calculates the
maximum stress (\(\sigma\)) for given product dimensions (\(\vec{x}\)).

\begin{definition}[Generic
Solution]\protect\hypertarget{def-generic-solution}{}\label{def-generic-solution}

The generic solution method is to collect the output values \(y^{(1)}\),
\(y^{(2)}\), \ldots, \(y^{(n)}\) that result from a set of inputs
\(\vec{x}^{(1)}\), \(\vec{x}^{(2)}\), \ldots, \(\vec{x}^{(n)}\) and find
a best guess \(\hat{f}(\vec{x})\) for the black box mapping \(f\), based
on these known observations.

\end{definition}

\section{Stage One: Preparing the Data and Choosing a Modelling
Approach}\label{stage-one-preparing-the-data-and-choosing-a-modelling-approach}

The first step is the identification, through a small number of
observations, of the inputs that have a significant impact on \(f\);
that is the determination of the shortest design variable vector
\(\vec{x} = \{x_1, x_2, \ldots, x_k\}^T\) that, by sweeping the ranges
of all of its variables, can still elicit most of the behavior the black
box is capable of. The ranges of the various design variables also have
to be established at this stage.

The second step is to recruit \(n\) of these \(k\)-vectors into a list
\[
X = \{ \vec{x}^{(1)},\vec{x}^{(2)}, \ldots, \vec{x}^{(n)} \}^T,
\] where each \(\vec{x}^{(i)}\) is a \(k\)-vector. The corresponding
responses are collected in a vector such that this represents the design
space as thoroughly as possible.

In the surrogate modeling process, the number of samples \(n\) is often
limited, as it is constrained by the computational cost (money and/or
time) associated with obtaining each observation.

It is advisable to scale \(\vec{x}\) at this stage into the unit cube
\([0, 1]^k\), a step that can simplify the subsequent mathematics and
prevent multidimensional scaling issues.

We now focus on the attempt to learn \(f\) through data pairs \[
\{ (\vec{x}^{(1)}, y^{(1)}), (\vec{x}^{(2)}, y^{(2)}), \ldots, (\vec{x}^{(n)}, y^{(n)}) \}.
\]

This supervised learning process essentially involves searching across
the space of possible functions \(\hat{f}\) that would replicate
observations of \(f\). This space of functions is infinite. Any number
of hypersurfaces could be drawn to pass through or near the known
observations, accounting for experimental error. However, most of these
would generalize poorly; they would be practically useless at predicting
responses at new sites, which is the ultimate goal.

\begin{example}[The Needle(s) in the Haystack
Function]\protect\hypertarget{exm-needle-haystack}{}\label{exm-needle-haystack}

An extreme example is the `needle(s) in the haystack' function:

\[
f(x) = \begin{cases} 
y^{(1)}, & \text{if } x = \vec{x}^{(1)} \\
y^{(2)}, & \text{if } x = \vec{x}^{(2)} \\
\vdots & \\
y^{(n)}, & \text{if } x = \vec{x}^{(n)} \\
0, & \text{otherwise.}
\end{cases}
\]

While this predictor reproduces all training data, it seems
counter-intuitive and unsettling to predict 0 everywhere else for most
engineering functions. Although there is a small chance that the
function genuinely resembles the equation above and we sampled exactly
where the needles are, it is highly unlikely.

\end{example}

There are countless other configurations, perhaps less contrived, that
still generalize poorly. This suggests a need for systematic means to
filter out nonsensical predictors. In our approach, we embed the
structure of \(f\) into the model selection algorithm and search over
its parameters to fine-tune the approximation to observations. For
instance, consider one of the simplest models,
\begin{equation}\phantomsection\label{eq-linear-model-simple}{
f(x, \vec{w}) = \vec{w}^T\vec{x} + v.
}\end{equation} Learning \(f\) with this model implies that its
structure---a hyperplane---is predetermined, and the fitting process
involves finding the \(k + 1\) parameters (the slope vector \(\vec{w}\)
and the intercept \(v\)) that best fit the data. This will be
accomplished in Stage Two.

Complicating this further is the noise present in observed responses (we
assume design vectors \(\vec{x}\) are not corrupted). Here, we focus on
learning from such data, which sometimes risks overfitting.

\begin{definition}[Overfitting]\protect\hypertarget{def-overfitting}{}\label{def-overfitting}

Overfitting occurs when the model becomes too flexible and captures not
only the underlying trend but also the noise in the data.

\end{definition}

In the surrogate modeling process, the second stage as described in
Section~\ref{sec-stage-two}, addresses this issue of complexity control
by estimating the parameters of the fixed structure model. However,
foresight is necessary even at the model type selection stage.

Model selection often involves physics-based considerations, where the
modeling technique is chosen based on expected underlying responses.

\begin{example}[Model
Selection]\protect\hypertarget{exm-model-selection}{}\label{exm-model-selection}

Modeling stress in an elastically deformed solid due to small strains
may justify using a simple linear approximation. Without insights into
the physics, and if one fails to account for the simplicity of the data,
a more complex and excessively flexible model may be incorrectly chosen.
Although parameter estimation might still adjust the approximation to
become linear, an opportunity to develop a simpler and robust model may
be lost.

\begin{itemize}
\tightlist
\item
  Simple linear (or polynomial) models, despite their lack of
  flexibility, have advantages like applicability in further symbolic
  computations.
\item
  Conversely, if we incorrectly assume a quadratic process when multiple
  peaks and troughs exist, the parameter estimation stage will not
  compensate for an unsuitable model choice. A quadratic model is too
  rigid to fit a multimodal function, regardless of parameter
  adjustments.
\end{itemize}

\end{example}

\section{Stage Two: Parameter Estimation and
Training}\label{sec-stage-two}

Assuming that Stage One helped identify the \(k\) critical design
variables, acquire the learning data set, and select a generic model
structure \(f(\vec{x}, \vec{w})\), the task now is to estimate
parameters \(\vec{w}\) to ensure the model fits the data optimally.
Among several estimation criteria, we will discuss two methods here.

\begin{definition}[Maximum Likelihood
Estimation]\protect\hypertarget{def-mle}{}\label{def-mle}

Given a set of parameters \(\vec{w}\), the model \(f(\vec{x}, \vec{w})\)
allows computation of the probability of the data set \[
\{(\vec{x}^{(1)}, y^{(1)} \pm \epsilon), (\vec{x}^{(2)}, y^{(2)} \pm \epsilon), \ldots, (\vec{x}^{(n)}, y^{(n)} \pm \epsilon)\}
\] resulting from \(f\) (where \(\epsilon\) is a small error margin
around each data point).

\end{definition}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Maximum Likelihood Estimation}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\textbf{?@sec-max-likelihood} presents a more detailed discussion of the
maximum likelihood estimation (MLE) method.

\end{tcolorbox}

Taking \textbf{?@eq-likelihood-mvn} and assuming errors \(\epsilon\) are
independently and normally distributed with standard deviation
\(\sigma\), the probability of the data set is given by:

\[
P = \frac{1}{(2\pi \sigma^2)^{n/2}} \exp \left[ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} \left( y^{(i)} - f(\vec{x}^{(i)}, \vec{w}) \right)^2 \epsilon \right].
\]

Intuitively, this is equivalent to the likelihood of the parameters
given the data. Accepting this intuitive relationship as a mathematical
one aids in model parameter estimation. This is achieved by maximizing
the likelihood or, more conveniently, minimizing the negative of its
natural logarithm:

\begin{equation}\phantomsection\label{eq-forr23}{ 
\min_{\vec{w}} \sum_{i=1}^{n} \frac{[y^{(i)} - f(\vec{x}^{(i)}, \vec{w})]^2}{2\sigma^2} + \frac{n}{2} \ln \epsilon .
}\end{equation}

If we assume \(\sigma\) and \(\epsilon\) are constants,
Equation~\ref{eq-forr23} simplifies to the well-known least squares
criterion:

\[ 
\min_{\vec{w}} \sum_{i=1}^{n} [y^{(i)} - f(\vec{x}^{(i)}, \vec{w})]^2 . 
\]

Cross-validation is another method used to estimate model performance.

\begin{definition}[Cross-Validation]\protect\hypertarget{def-cross-validation}{}\label{def-cross-validation}

Cross-validation splits the data randomly into \(q\) roughly equal
subsets, and then cyclically removing each subset and fitting the model
to the remaining \(q - 1\) subsets. A loss function \(L\) is then
computed to measure the error between the predictor and the withheld
subset for each iteration, with contributions summed over all \(q\)
iterations. More formally, if a mapping
\(\theta: \{1, \ldots, n\} \to \{1, \ldots, q\}\) describes the
allocation of the \(n\) training points to one of the \(q\) subsets and
\(f^{(-\theta(i))}(\vec{x})\) is the predicted value by removing the
subset \(\theta(i)\) (i.e., the subset where observation \(i\) belongs),
the cross-validation measure, used as an estimate of prediction error,
is:

\begin{equation}\phantomsection\label{eq-cv-basis}{ 
CV = \frac{1}{n} \sum_{i=1}^{n} L(y^{(i)}, f^{(-\theta(i))}(\vec{x}^{(i)})) . 
}\end{equation}

\end{definition}

Introducing the squared error as the loss function and considering our
generic model \(f\) still dependent on undetermined parameters, we write
Equation~\ref{eq-cv-basis} as:

\begin{equation}\phantomsection\label{eq-cv-sse}{ 
CV = \frac{1}{n} \sum_{i=1}^{n} [y^{(i)} - f^{(-\theta(i))}(\vec{x}^{(i)})]^2 .
}\end{equation}

The extent to which Equation~\ref{eq-cv-sse} is an unbiased estimator of
true risk depends on \(q\). It is shown that if \(q = n\), the
leave-one-out cross-validation (LOOCV) measure is almost unbiased.
However, LOOCV can have high variance because subsets are very similar.
(\textbf{Hast17a?})) suggest using compromise values like \(q = 5\) or
\(q = 10\). Using fewer subsets also reduces the computational cost of
the cross-validation process, see also (\textbf{arlot2010?}) and
(\textbf{Koha95a?}).

\section{Stage Three: Model Testing}\label{stage-three-model-testing}

If there is a sufficient amount of observational data, a random subset
should be set aside initially for model testing. (\textbf{Hast17a?})
recommend setting aside approximately \(0.25n\) of
\(\vec{x} \rightarrow y\) pairs for testing purposes. These observations
must remain untouched during Stages One and Two, as their sole purpose
is to evaluate the testing error---the difference between true and
approximated function values at the test sites---once the model has been
built. Interestingly, if the main goal is to construct an initial
surrogate for seeding a global refinement criterion-based strategy (as
discussed in Section 3.2 in (\textbf{Forr08a?})), the model testing
phase might be skipped.

It is noted that, ideally, parameter estimation (Stage Two) should also
rely on a separate subset. However, observational data is rarely
abundant enough to afford this luxury (if the function is cheap to
evaluate and evaluation sites are selectable, a surrogate model might
not be necessary).

When data are available for model testing and the primary objective is a
globally accurate model, using either a root mean square error (RMSE)
metric or the correlation coefficient (\(r^2\)) is recommended. To test
the model, a test data set of size \(n_t\) is used alongside predictions
at the corresponding locations to calculate these metrics.

The RMSE is defined as follows:

\begin{definition}[Root Mean Square Error
(RMSE)]\protect\hypertarget{def-rmse}{}\label{def-rmse}

\[
\text{RMSE} = \sqrt{\frac{1}{n_t} \sum_{i=1}^{n_t} (y^{(i)} - \hat{y}^{(i)})^2}, 
\]

\end{definition}

Ideally, the RMSE should be minimized, acknowledging its limitation by
errors in the objective function \(f\) calculation. If the error level
is known, like a standard deviation, the aim might be to achieve an RMSE
within this value. Often, the target is an RMSE within a specific
percentage of the observed data's objective value range.

The squared correlation coefficient \(r\), see \textbf{?@eq-pears-corr},
between the observed \(y\) and predicted \(\hat{y}\) values can be
computed as:

\begin{equation}\phantomsection\label{eq-r2}{ 
r^2 = \left( \frac{\text{cov}(y, \hat{y})}{\sqrt{\text{var}(y)\text{var}(\hat{y})}} \right)^2, 
}\end{equation}

Equation~\ref{eq-r2} and can be expanded as:

\[ 
r^2 = 
\left(
\frac{n_t \sum_{i=1}^{n_t} y^{(i)} \hat{y}^{(i)} - \sum_{i=1}^{n_t} y^{(i)} \sum_{i=1}^{n_t} \hat{y}^{(i)}}{ \sqrt{\left( n_t \sum_{i=1}^{n_t} (y^{(i)})^2 - \left(\sum_{i=1}^{n_t} y^{(i)}\right)^2 \right) \left( n_t \sum_{i=1}^{n_t} (\hat{y}^{(i)})^2 - \left(\sum_{i=1}^{n_t} \hat{y}^{(i)}\right)^2 \right)}}
\right)^2.
\]

The correlation coefficient \(r^2\) does not require scaling the data
sets and only compares landscape shapes, not values. An \(r^2 > 0.8\)
typically indicates a surrogate with good predictive capability.

The methods outlined provide quantitative assessments of model accuracy,
yet visual evaluations can also be insightful. In general, the RMSE will
not reach zero but will stabilize around a low value. At this point, the
surrogate model is saturated with data, and further additions do not
enhance the model globally (though local improvements can occur at newly
added points if using an interpolating model).

\begin{example}[The Tea and Sugar
Analogy]\protect\hypertarget{exm-tea-sugar}{}\label{exm-tea-sugar}

(\textbf{Forr08a?}) illustrates this saturation point using a comparison
with a cup of tea and sugar. The tea represents the surrogate model, and
sugar represents data. Initially, the tea is unsweetened, and adding
sugar increases its sweetness. Eventually, a saturation point is reached
where no more sugar dissolves, and the tea cannot get any sweeter.
Similarly, a more flexible model, like one with additional parameters or
employing interpolation rather than regression, can increase the
saturation point---akin to making a hotter cup of tea for dissolving
more sugar.

\end{example}

\section{Jupyter Notebook}\label{jupyter-notebook-2}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_constructing_surrogate.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Response Surface Methods}\label{sec-rsm}

This part deals with numerical implementations of optimization methods.
The goal is to understand the implementation of optimization methods and
to solve real-world problems numerically and efficiently. We will focus
on the implementation of surrogate models, because they are the most
efficient way to solve real-world problems.

Starting point is the well-established response surface methodology
(RSM). It will be extended to the design and analysis of computer
experiments (DACE). The DACE methodology is a modern extension of the
response surface methodology. It is based on the use of surrogate
models, which are used to replace the real-world problem with a simpler
problem. The simpler problem is then solved numerically. The solution of
the simpler problem is then used to solve the real-world problem.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-important-color!10!white, colframe=quarto-callout-important-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Numerical methods: Goals}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  Understand implementation of optimization methods
\item
  Solve real-world problems numerically and efficiently
\end{itemize}

\end{tcolorbox}

\section{What is RSM?}\label{sec-rsm-intro}

Response Surface Methods (RSM) refer to a collection of statistical and
mathematical tools that are valuable for developing, improving, and
optimizing processes. The overarching theme of RSM involves studying how
input variables that control a product or process can potentially
influence a response that measures performance or quality
characteristics.

The advantages of RSM include a rich literature, well-established
methods often used in manufacturing, the importance of careful
experimental design combined with a well-understood model, and the
potential to add significant value to scientific inquiry, process
refinement, optimization, and more. However, there are also drawbacks to
RSM, such as the use of simple and crude surrogates, the hands-on nature
of the methods, and the limitation of local methods.

RSM is related to various fields, including Design of Experiments (DoE),
quality management, reliability, and productivity. Its applications are
widespread in industry and manufacturing, focusing on designing,
developing, and formulating new products and improving existing ones, as
well as from laboratory research. RSM is commonly applied in domains
such as materials science, manufacturing, applied chemistry, climate
science, and many others.

An example of RSM involves studying the relationship between a response
variable, such as yield (\(y\)) in a chemical process, and two process
variables: reaction time (\(\xi_1\)) and reaction temperature
(\(\xi_2\)). The provided code illustrates this scenario, following a
variation of the so-called ``banana function.''

In the context of visualization, RSM offers the choice between 3D plots
and contour plots. In a 3D plot, the independent variables \(\xi_1\) and
\(\xi_2\) are represented, with \(y\) as the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\KeywordTok{def}\NormalTok{ fun\_rosen(x1, x2):}
\NormalTok{    b }\OperatorTok{=} \DecValTok{10}
    \ControlFlowTok{return}\NormalTok{ (x1}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ b}\OperatorTok{*}\NormalTok{(x2}\OperatorTok{{-}}\NormalTok{x1}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{zs }\OperatorTok{=}\NormalTok{ np.array(fun\_rosen(np.ravel(X), np.ravel(Y)))}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ zs.reshape(X.shape)}

\NormalTok{ax.plot\_surface(X, Y, Z)}

\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}X1\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}X2\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-2-output-1.pdf}}

\begin{itemize}
\tightlist
\item
  contour plot example:

  \begin{itemize}
  \tightlist
  \item
    \(x_1\) and \(x_2\) are the independent variables
  \item
    \(y\) is the dependent variable
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{3.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_rosen(X1, X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y , }\DecValTok{50}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Rosenbrock\textquotesingle{}s Banana Function"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, "Rosenbrock's Banana Function")
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-3-output-2.pdf}}

\begin{itemize}
\tightlist
\item
  Visual inspection: yield is optimized near \((\xi_1. \xi_2)\)
\end{itemize}

\subsection{Visualization: Problems in
Practice}\label{visualization-problems-in-practice}

\begin{itemize}
\tightlist
\item
  True response surface is unknown in practice
\item
  When yield evaluation is not as simple as a toy banana function, but a
  process requiring care to monitor, reconfigure and run, it's far too
  expensive to observe over a dense grid
\item
  And, measuring yield may be a noisy/inexact process
\item
  That's where stats (RSM) comes in
\end{itemize}

\subsection{RSM: Strategies}\label{rsm-strategies}

\begin{itemize}
\item
  RSMs consist of experimental strategies for
\item
  \textbf{exploring} the space of the process (i.e., independent/input)
  variables (above \(\xi_1\) and \(\xi2)\)
\item
  empirical statistical \textbf{modeling} targeted toward development of
  an appropriate approximating relationship between the response (yield)
  and process variables local to a study region of interest
\item
  \textbf{optimization} methods for sequential refinement in search of
  the levels or values of process variables that produce desirable
  responses (e.g., that maximize yield or explain variation)
\item
  RSM used for fitting an Empirical Model
\item
  True response surface driven by an unknown physical mechanism
\item
  Observations corrupted by noise
\item
  Helpful: fit an empirical model to output collected under different
  process configurations
\item
  Consider response \(Y\) that depends on controllable input variables
  \(\xi_1, \xi_2, \ldots, \xi_m\)
\item
  RSM: Equations of the Empirical Model

  \begin{itemize}
  \tightlist
  \item
    \(Y=f(\xi_1, \xi_2, \ldots, \xi_m) + \epsilon\)
  \item
    \(\mathbb{E}\{Y\} = \eta = f(\xi1_1, \xi_2, \ldots, \xi_m)\)
  \item
    \(\epsilon\) is treated as zero mean idiosyncratic noise possibly
    representing

    \begin{itemize}
    \tightlist
    \item
      inherent variation, or
    \item
      the effect of other systems or
    \item
      variables not under our purview at this time
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{RSM: Noise in the Empirical
Model}\label{rsm-noise-in-the-empirical-model}

\begin{itemize}
\tightlist
\item
  Typical simplifying assumption: \(\epsilon \sim N(0,\sigma^2)\)
\item
  We seek estimates for \(f\) and \(\sigma^2\) from noisy observations
  \(Y\) at inputs \(\xi\)
\end{itemize}

\subsection{RSM: Natural and Coded
Variables}\label{rsm-natural-and-coded-variables}

\begin{itemize}
\tightlist
\item
  Inputs \(\xi_1, \xi_2, \ldots, \xi_m\) called \textbf{natural
  variables}:

  \begin{itemize}
  \tightlist
  \item
    expressed in natural units of measurement, e.g., degrees Celsius,
    pounds per square inch (psi), etc.
  \end{itemize}
\item
  Transformed to \textbf{coded variables} \(x_1, x_2, \ldots, x_m\):

  \begin{itemize}
  \tightlist
  \item
    to mitigate hassles and confusion that can arise when working with a
    multitude of scales of measurement
  \end{itemize}
\item
  Typical \textbf{Transformations} offering dimensionless inputs
  \(x_1, x_2, \ldots, x_m\)

  \begin{itemize}
  \tightlist
  \item
    in the unit cube, or
  \item
    scaled to have a mean of zero and standard deviation of one, are
    common choices.
  \end{itemize}
\item
  Empirical model becomes \(\eta = f(x_1, x_2, \ldots, x_m)\)
\end{itemize}

\subsection{RSM Low-order Polynomials}\label{rsm-low-order-polynomials}

\begin{itemize}
\tightlist
\item
  Low-order polynomial make the following simplifying Assumptions

  \begin{itemize}
  \tightlist
  \item
    Learning about \(f\) is lots easier if we make some simplifying
    approximations
  \item
    Appealing to \textbf{Taylor's theorem}, a low-order polynomial in a
    small, localized region of the input (\(x\)) space is one way
    forward
  \item
    Classical RSM:

    \begin{itemize}
    \tightlist
    \item
      disciplined application of \textbf{local analysis} and
    \item
      \textbf{sequential refinement} of locality through conservative
      extrapolation
    \end{itemize}
  \item
    Inherently a \textbf{hands-on process}
  \end{itemize}
\end{itemize}

\section{First-Order Models (Main Effects
Model)}\label{first-order-models-main-effects-model}

\begin{itemize}
\tightlist
\item
  \textbf{First-order model} (sometimes called main effects model)
  useful in parts of the input space where it's believed that there's
  little curvature in \(f\):
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \]
\item
  For example: \[\eta = 50 + 8 x_1 + 3x_2\]
\item
  In practice, such a surface would be obtained by fitting a model to
  the outcome of a designed experiment
\item
  First-Order Model in python Evaluated on a Grid
\item
  Evaluate model on a grid in a double-unit square centered at the
  origin
\item
  Coded units are chosen arbitrarily, although one can imagine deploying
  this approximating function nearby \(x^{(0)} = (0,0)\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_1(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_1(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model: $50 + 8x\_1 + 3x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model: $50 + 8x_1 + 3x_2$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-5-output-2.pdf}}

\subsection{First-Order Model
Properties}\label{first-order-model-properties}

\begin{itemize}
\tightlist
\item
  First-order model in 2d traces out a \textbf{plane} in
  \(y \times (x_1, x_2)\) space
\item
  Only be appropriate for the most trivial of response surfaces, even
  when applied in a highly localized part of the input space
\item
  Adding \textbf{curvature} is key to most applications:

  \begin{itemize}
  \tightlist
  \item
    First-order model with \textbf{interactions} induces limited degree
    of curvature via different rates of change of \(y\) as \(x_1\) is
    varied for fixed \(x_2\), and vice versa:
    \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_{12} \]
  \end{itemize}
\item
  For example \(\eta = 50+8x_1+3x_2-4x_1x_2\)
\end{itemize}

\subsection{First-order Model with Interactions in
python}\label{first-order-model-with-interactions-in-python}

\begin{itemize}
\tightlist
\item
  Code below facilitates evaluations for pairs \((x_1, x_2)\)
\item
  Responses may be observed over a mesh in the same double-unit square
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_11(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_11(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}First Order Model with Interactions\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'First Order Model with Interactions')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-7-output-2.pdf}}

\subsection{Observations: First-Order Model with
Interactions}\label{observations-first-order-model-with-interactions}

\begin{itemize}
\tightlist
\item
  Mean response \(\eta\) is increasing marginally in both \(x_1\) and
  \(x_2\), or conditional on a fixed value of the other until \(x_1\) is
  0.75
\item
  Rate of increase slows as both coordinates grow simultaneously since
  the coefficient in front of the interaction term \(x_1 x_2\) is
  negative
\item
  Compared to the first-order model (without interactions): surface is
  far more useful locally
\item
  Least squares regressions often flag up significant interactions when
  fit to data collected on a design far from local optima
\end{itemize}

\section{Second-Order Models}\label{second-order-models}

\begin{itemize}
\item
  Second-order model may be appropriate near local optima where \(f\)
  would have substantial curvature:
  \[\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2  + \beta_{11}x_1^2 + \beta_{22}x^2 + \beta_{12} x_1 x_2\]
\item
  For example \[\eta = 50 + 8 x_1 + 3x_2 - 7x_1^2 - 3 x_2^2 - 4x_1x_2\]
\item
  Implementation of the Second-Order Model as \texttt{fun\_2()}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_2(x1,x2):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{+} \DecValTok{8} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{+} \DecValTok{3} \OperatorTok{*}\NormalTok{ x2 }\OperatorTok{{-}} \DecValTok{7} \OperatorTok{*}\NormalTok{ x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{4} \OperatorTok{*}\NormalTok{ x1 }\OperatorTok{*}\NormalTok{ x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_2(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Second Order Model with Interactions. Maximum near about $(0.6,0.2)$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Second Order Model with Interactions. Maximum near about $(0.6,0.2)$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-9-output-2.pdf}}

\subsection{Second-Order Models:
Properties}\label{second-order-models-properties}

\begin{itemize}
\tightlist
\item
  Not all second-order models would have a single stationary point (in
  RSM jargon called ``a simple maximum'')
\item
  In ``yield maximizing'' setting we're presuming response surface is
  \textbf{concave} down from a global viewpoint

  \begin{itemize}
  \tightlist
  \item
    even though local dynamics may be more nuanced
  \end{itemize}
\item
  Exact criteria depend upon the eigenvalues of a certain matrix built
  from those coefficients
\item
  Box and Draper (2007) provide a diagram categorizing all of the kinds
  of second-order surfaces in RSM analysis, where finding local maxima
  is the goal
\end{itemize}

\subsection{Example: Stationary Ridge}\label{example-stationary-ridge}

\begin{itemize}
\tightlist
\item
  Example set of coefficients describing what's called a
  \textbf{stationary ridge} is provided by the code below
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Example of a stationary ridge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Example of a stationary ridge')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-11-output-2.pdf}}

\subsection{Observations: Second-Order Model
(Ridge)}\label{observations-second-order-model-ridge}

\begin{itemize}
\tightlist
\item
  \textbf{Ridge}: a whole line of stationary points corresponding to
  maxima
\item
  Situation means that the practitioner has some flexibility when it
  comes to optimizing:

  \begin{itemize}
  \tightlist
  \item
    can choose the precise setting of \((x_1, x_2)\) either arbitrarily
    or (more commonly) by consulting some tertiary criteria
  \end{itemize}
\end{itemize}

\subsection{Example: Rising Ridge}\label{example-rising-ridge}

\begin{itemize}
\tightlist
\item
  An example of a rising ridge is implemented by the code below.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_ridge\_rise(x1, x2):}
     \ControlFlowTok{return} \DecValTok{80} \OperatorTok{{-}} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{12}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{3}\OperatorTok{*}\NormalTok{x1}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_ridge\_rise(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Rising ridge: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 3x\_1\^{}2 {-} 12x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Rising ridge: $\\eta = 80 + 4x_1 + 8x_2 - 3x_1^2 - 12x_2^2 - 12x_1x_2$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-13-output-2.pdf}}

\subsection{Summary: Rising Ridge}\label{summary-rising-ridge}

\begin{itemize}
\tightlist
\item
  The stationary point is remote to the study region
\item
  Ccontinuum of (local) stationary points along any line going through
  the 2d space, excepting one that lies directly on the ridge
\item
  Although estimated response will increase while moving along the axis
  of symmetry toward its stationary point, this situation indicates

  \begin{itemize}
  \tightlist
  \item
    either a poor fit by the approximating second-order function, or
  \item
    that the study region is not yet precisely in the vicinity of a
    local optima---often both.
  \end{itemize}
\end{itemize}

\subsection{Falling Ridge}\label{falling-ridge}

\begin{itemize}
\tightlist
\item
  Inversion of a rising ridge is a falling ridge
\item
  Similarly indicating one is far from local optima, except that the
  response decreases as you move toward the stationary point
\item
  Finding a falling ridge system can be a back-to-the-drawing-board
  affair.
\end{itemize}

\subsection{Saddle Point}\label{saddle-point}

\begin{itemize}
\tightlist
\item
  Finally, we can get what's called a saddle or minimax system.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fun\_saddle(x1, x2):}
    \ControlFlowTok{return} \DecValTok{80} \OperatorTok{+} \DecValTok{4}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+} \DecValTok{8}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{{-}} \DecValTok{2}\OperatorTok{*}\NormalTok{x2}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{12}\OperatorTok{*}\NormalTok{x1}\OperatorTok{*}\NormalTok{x2 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.cm }\ImportTok{as}\NormalTok{ cm}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{delta }\OperatorTok{=} \FloatTok{0.025}
\NormalTok{x1 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.arange(}\OperatorTok{{-}}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, delta)}
\NormalTok{X1, X2 }\OperatorTok{=}\NormalTok{ np.meshgrid(x1, x2)}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ fun\_saddle(X1,X2)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{CS }\OperatorTok{=}\NormalTok{ ax.contour(X1, X2, Y, }\DecValTok{20}\NormalTok{)}
\NormalTok{ax.clabel(CS, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{\textquotesingle{}Saddle Point: $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{eta = 80 + 4x\_1 + 8x\_2 {-} 2x\_2\^{}2 {-} 12x\_1x\_2$\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Saddle Point: $\\eta = 80 + 4x_1 + 8x_2 - 2x_2^2 - 12x_1x_2$')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/cell-15-output-2.pdf}}

\subsection{Interpretation: Saddle
Points}\label{interpretation-saddle-points}

\begin{itemize}
\tightlist
\item
  Likely further data collection, and/or outside expertise, is needed
  before determining a course of action in this situation
\end{itemize}

\subsection{Summary: Ridge Analysis}\label{summary-ridge-analysis}

\begin{itemize}
\tightlist
\item
  Finding a simple maximum, or stationary ridge, represents ideals in
  the spectrum of second-order approximating functions
\item
  But getting there can be a bit of a slog
\item
  Using models fitted from data means uncertainty due to noise, and
  therefore uncertainty in the type of fitted second-order model
\item
  A ridge analysis attempts to offer a principled approach to navigating
  uncertainties when one is seeking local maxima
\item
  The two-dimensional setting exemplified above is convenient for
  visualization, but rare in practice
\item
  Complications compound when studying the effect of more than two
  process variables
\end{itemize}

\section{General RSM Models}\label{general-rsm-models}

\begin{itemize}
\tightlist
\item
  General \textbf{first-order model} on \(m\) process variables
  \(x_1, x_2, \cdots, x_m\) is
  \[\eta = \beta_0 + \beta_1x_1 + \cdots + \beta_m x_m\]
\item
  General \textbf{second-order model} on \(m\) process variables \[
  \eta= \beta_0 + \sum_{j=1}^m + \sum_{j=1}^m x_j^2 + \sum_{j=2}^m \sum_{k=1}^j \beta_{kj}x_k x_j.
  \]
\end{itemize}

\subsection{Ordinary Least Squares}\label{ordinary-least-squares}

\begin{itemize}
\tightlist
\item
  Inference from data is carried out by \textbf{ordinary least squares}
  (OLS)
\item
  For an excellent review including R examples, see Sheather (2009)
\item
  OLS and maximum likelihood estimators (MLEs) are in the typical
  Gaussian linear modeling setup basically equivalent
\end{itemize}

\section{General Linear Regression}\label{general-linear-regression}

We are considering a model, which can be written in the form

\[
Y = X \beta + \epsilon,
\] where \(Y\) is an \((n \times 1)\) vector of observations
(responses), \(X\) is an \((n \times p)\) matrix of known form,
\(\beta\) is a \((1 \times p)\) vector of unknown parameters, and
\(\epsilon\) is an \((n \times 1)\) vector of errors. Furthermore,
\(E(\epsilon) = 0\), \(Var(\epsilon) = \sigma^2 I\) and the
\(\epsilon_i\) are uncorrelated.

Using the normal equations \[
(X'X)b = X'Y,
\]

the solution is given by

\[
b = (X'X)^{-1}X'Y.
\]

\begin{example}[Linear
Regression]\protect\hypertarget{exm-ols}{}\label{exm-ols}

~

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{n }\OperatorTok{=} \DecValTok{8}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(X, }\DecValTok{2}\NormalTok{))}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(X)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(y, }\DecValTok{2}\NormalTok{))}
\CommentTok{\# fit an OLS model to the data, predict the response based on the 1 x values}
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, m, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{model.fit(X, y)}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ model.predict(x)}
\CommentTok{\# visualize the data and the fitted model}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{plt.scatter(X, y, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(x, y\_pred, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\CommentTok{\# add the ground truth (sine function) in orange}
\NormalTok{plt.plot(x, np.sin(x), color}\OperatorTok{=}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]
[[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{005_num_rsm_files/figure-pdf/linreg-example-output-2.pdf}}

\end{example}

\section{Designs}\label{designs}

\begin{itemize}
\tightlist
\item
  Important: Organize the data collection phase of a response surface
  study carefully
\item
  \textbf{Design}: choice of \(x\)'s where we plan to observe \(y\)'s,
  for the purpose of approximating \(f\)
\item
  Analyses and designs need to be carefully matched
\item
  When using a first-order model, some designs are preferred over others
\item
  When using a second-order model to capture curvature, a different sort
  of design is appropriate
\item
  Design choices often contain features enabling modeling assumptions to
  be challenged

  \begin{itemize}
  \tightlist
  \item
    e.g., to check if initial impressions are supported by the data
    ultimately collected
  \end{itemize}
\end{itemize}

\subsection{Different Designs}\label{different-designs}

\begin{itemize}
\tightlist
\item
  \textbf{Screening desings}: determine which variables matter so that
  subsequent experiments may be smaller and/or more focused
\item
  Then there are designs tailored to the form of model (first- or
  second-order, say) in the screened variables
\item
  And then there are more designs still
\end{itemize}

\section{RSM Experimentation}\label{rsm-experimentation}

\subsection{First Step}\label{first-step}

\begin{itemize}
\tightlist
\item
  RSM-based experimentation begins with a \textbf{first-order model},
  possibly with interactions
\item
  Presumption: current process operating \textbf{far from optimal}
  conditions
\item
  Collect data and apply \textbf{method of steepest ascent} (gradient)
  on fitted surfaces to move to the optimum
\end{itemize}

\subsection{Second Step}\label{second-step}

\begin{itemize}
\tightlist
\item
  Eventually, if all goes well after several such carefully iterated
  refinements, \textbf{second-order models} are used on appropriate
  designs in order to zero-in on ideal operating conditions
\item
  Careful analysis of the fitted surface:

  \begin{itemize}
  \tightlist
  \item
    Ridge analysis with further refinement using gradients of, and
  \item
    standard errors associated with, the fitted surfaces, and so on
  \end{itemize}
\end{itemize}

\subsection{Third Step}\label{third-step}

\begin{itemize}
\tightlist
\item
  Once the practitioner is satisfied with the full arc of

  \begin{itemize}
  \tightlist
  \item
    design(s),
  \item
    fit(s), and
  \item
    decision(s):
  \end{itemize}
\item
  A small experiment called \textbf{confirmation test} may be performed
  to check if the predicted optimal settings are realizable in practice
\end{itemize}

\section{RSM: Review and General
Considerations}\label{rsm-review-and-general-considerations}

\begin{itemize}
\item
  First Glimpse, RSM seems sensible, and pretty straightforward as
  quantitative statistics-based analysis goes
\item
  But: RSM can get complicated, especially when input dimensions are not
  very low
\item
  Design considerations are particularly nuanced, since the goal is to
  obtain reliable estimates of main effects, interaction, and curvature
  while minimizing sampling effort/expense
\item
  RSM Downside: Inefficiency

  \begin{itemize}
  \tightlist
  \item
    Despite intuitive appeal, several RSM downsides become apparent upon
    reflection
  \item
    Problems in practice
  \item
    Stepwise nature of sequential decision making is inefficient:

    \begin{itemize}
    \tightlist
    \item
      Not obvious how to re-use or update analysis from earlier phases,
      or couple with data from other sources/related experiments
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Locality

  \begin{itemize}
  \tightlist
  \item
    In addition to being local in experiment-time (stepwise approach),
    it's local in experiment-space
  \item
    Balance between

    \begin{itemize}
    \tightlist
    \item
      exploration (maybe we're barking up the wrong tree) and
    \item
      exploitation (let's make things a little better) is modest at best
    \end{itemize}
  \end{itemize}
\item
  RSM Downside: Expert Knowledge

  \begin{itemize}
  \tightlist
  \item
    Interjection of expert knowledge is limited to hunches about
    relevant variables (i.e., the screening phase), where to initialize
    search, how to design the experiments
  \item
    Yet at the same time classical RSMs rely heavily on constant
    examination throughout stages of modeling and design and on the
    instincts of seasoned practitioners
  \end{itemize}
\item
  RSM Downside: Replicability

  \begin{itemize}
  \tightlist
  \item
    Parallel analyses, conducted according to the same best intentions,
    rarely lead to the same designs, model fits and so on
  \item
    Sometimes that means they lead to different conclusions, which can
    be cause for concern
  \end{itemize}
\end{itemize}

\subsection{Historical Considerations about
RSM}\label{historical-considerations-about-rsm}

\begin{itemize}
\tightlist
\item
  In spite of those criticisms, however, there was historically little
  impetus to revise the status quo
\item
  Classical RSM was comfortable in its skin, consistently led to
  improvements or compelling evidence that none can reasonably be
  expected
\item
  But then in the late 20th century came an explosive expansion in
  computational capability, and with it a means of addressing many of
  those downsides
\end{itemize}

\subsection{Status Quo}\label{status-quo}

\begin{itemize}
\tightlist
\item
  Nowadays, field experiments and statistical models, designs and
  optimizations are coupled with with mathematical models
\item
  Simple equations are not regarded as sufficient to describe real-world
  systems anymore
\item
  Physicists figured that out fifty years ago; industrial engineers
  followed, biologists, social scientists, climate scientists and
  weather forecasters, etc.
\item
  Systems of equations are required, solved over meshes (e.g., finite
  elements), or stochastically interacting agents
\item
  Goals for those simulation experiments are as diverse as their
  underlying dynamics
\item
  Optimization of systems is common, e.g., to identify worst-case
  scenarios
\end{itemize}

\subsection{The Role of Statistics}\label{the-role-of-statistics}

\begin{itemize}
\tightlist
\item
  Solving systems of equations, or interacting agents, requires
  computing
\item
  Statistics involved at various stages:

  \begin{itemize}
  \tightlist
  \item
    choosing the mathematical model
  \item
    solving by stochastic simulation (Monte Carlo)
  \item
    designing the computer experiment
  \item
    smoothing over idiosyncrasies or noise
  \item
    finding optimal conditions, or
  \item
    calibrating mathematical/computer models to data from field
    experiments
  \end{itemize}
\end{itemize}

\subsection{New RSM is needed: DACE}\label{new-rsm-is-needed-dace}

\begin{itemize}
\tightlist
\item
  Classical RSMs are not well-suited to any of those tasks, because

  \begin{itemize}
  \tightlist
  \item
    they lack the fidelity required to model these data
  \item
    their intended application is too local
  \item
    they're also too hands-on.
  \end{itemize}
\item
  Once computers are involved, a natural inclination is to automate---to
  remove humans from the loop and set the computer running on the
  analysis in order to maximize computing throughput, or minimize idle
  time
\item
  \textbf{Design and Analysis of Computer Experiments} as a modern
  extension of RSM
\item
  Experimentation is changing due to advances in machine learning
\item
  \textbf{Gaussian process} (GP) regression is the canonical surrogate
  model
\item
  Origins in geostatistics (gold mining)
\item
  Wide applicability in contexts where prediction is king
\item
  Machine learners exposed GPs as powerful predictors for all sorts of
  tasks:
\item
  from regression to classification,
\item
  active learning/sequential design,
\item
  reinforcement learning and optimization,
\item
  latent variable modeling, and so on
\end{itemize}

\section{Exercises}\label{exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate 3d Plots for the Contour Plots in this notebook.
\item
  Write a \texttt{plot\_3d} function, that takes the objective function
  \texttt{fun} as an argument.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface: \texttt{plot\_3d(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Write a \texttt{plot\_contour} function, that takes the objective
  function \texttt{fun} as an argument:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  It should provide the following interface:
  \texttt{plot\_contour(fun)}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Consider further arguments that might be useful for both function,
  e.g., ranges, size, etc.
\end{enumerate}

\section{Jupyter Notebook}\label{jupyter-notebook-3}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/005_num_rsm.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Polynomial Models}\label{polynomial-models}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  This section is based on chapter 2.2 in (\textbf{Forr08a?}).
\item
  The following Python packages are imported:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Fitting a Polynomial}\label{fitting-a-polynomial}

We will consider one-variable cases, i.e., \(k=1\), first.

Let us consider the scalar-valued function
\(f: \mathbb{R} \to \mathbb{R}\) observed according to the sampling plan
\(X = \{x^{(1)}, x^{(2)} \dots, x^{(n)}\}^T\), yielding the responses
\(\vec{y} = \{y^{(1)}, y^{(2)}, \dots, y^{(n)}\}^T\).

A polynomial approximation of \(f\) of order \(m\) can be written as:

\[
\hat{f}(x, m, \vec{w}) = \sum_{i=0}^m w_i x^i.
\]

In the spirit of the earlier discussion of maximum likelihood parameter
estimation, we seek to estimate \(w = {w_0, w_1, \dots, w_m}^T\) through
a least squares solution of:

\[
\Phi \vec{w} = \vec{y}
\] where \(\Phi\) is the Vandermonde matrix:

\[
\Phi =
\begin{bmatrix}
1 & x_1 & x_1^2 & \dots & x_1^m \\
1 & x_2 & x_2^2 & \dots & x_2^m \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \dots & x_n^m
\end{bmatrix}.
\]

The maximum likelihood estimate of \(w\) is given by:

\[
\vec{w} = (\Phi^T \Phi)^{-1} \Phi^T y,
\]

where \(\Phi^+ = (\Phi^T \Phi)^{-1} \Phi^T\) is the Moore-Penrose
pseudo-inverse of \(\Phi\) (see Section~\ref{sec-matrix-pseudoinverse}).

The polynomial approximation of order \(m\) is essentially a truncated
Taylor series expansion. While higher values of \(m\) yield more
accurate approximations, they risk overfitting the noise in the data.

To prevent this, we estimate \(m\) using cross-validation. This involves
minimizing the cross-validation error over a discrete set of possible
orders \(m\) (e.g., \(m \in {1, 2, \dots, 15}\)).

For each \(m\), the data is split into \(q\) subsets. The model is
trained on \(q-1\) subsets, and the error is computed on the left-out
subset. This process is repeated for all subsets, and the
cross-validation error is summed. The order \(m\) with the smallest
cross-validation error is chosen.

\section{Polynomial Fitting in
Python}\label{polynomial-fitting-in-python}

\subsection{Fitting the Polynomial}\label{fitting-the-polynomial}

\phantomsection\label{polynomial_fit}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ KFold}
\KeywordTok{def}\NormalTok{ polynomial\_fit(X, Y, max\_order}\OperatorTok{=}\DecValTok{15}\NormalTok{, q}\OperatorTok{=}\DecValTok{5}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Fits a one{-}variable polynomial to one{-}dimensional data using cross{-}validation.}

\CommentTok{    Args:}
\CommentTok{        X (array{-}like): Training data vector (independent variable).}
\CommentTok{        Y (array{-}like): Training data vector (dependent variable).}
\CommentTok{        max\_order (int): Maximum polynomial order to consider. Default is 15.}
\CommentTok{        q (int): Number of cross{-}validation folds. Default is 5.}

\CommentTok{    Returns:}
\CommentTok{        best\_order (int): The optimal polynomial order.}
\CommentTok{        coeff (array): Coefficients of the best{-}fit polynomial.}
\CommentTok{        mnstd (tuple): Normalization parameters (mean, std) for X.}
\CommentTok{    """}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.array(X)}
\NormalTok{    Y }\OperatorTok{=}\NormalTok{ np.array(Y)}
\NormalTok{    n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X)}
    \CommentTok{\# Normalize X}
\NormalTok{    mnstd }\OperatorTok{=}\NormalTok{ (np.mean(X), np.std(X))}
\NormalTok{    X\_norm }\OperatorTok{=}\NormalTok{ (X }\OperatorTok{{-}}\NormalTok{ mnstd[}\DecValTok{0}\NormalTok{]) }\OperatorTok{/}\NormalTok{ mnstd[}\DecValTok{1}\NormalTok{]}
    \CommentTok{\# Cross{-}validation setup}
\NormalTok{    kf }\OperatorTok{=}\NormalTok{ KFold(n\_splits}\OperatorTok{=}\NormalTok{q, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{    cross\_val\_errors }\OperatorTok{=}\NormalTok{ np.zeros(max\_order)}
    \ControlFlowTok{for}\NormalTok{ order }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, max\_order }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
\NormalTok{        fold\_errors }\OperatorTok{=}\NormalTok{ []}
        \ControlFlowTok{for}\NormalTok{ train\_idx, val\_idx }\KeywordTok{in}\NormalTok{ kf.split(X\_norm):}
\NormalTok{            X\_train, X\_val }\OperatorTok{=}\NormalTok{ X\_norm[train\_idx], X\_norm[val\_idx]}
\NormalTok{            Y\_train, Y\_val }\OperatorTok{=}\NormalTok{ Y[train\_idx], Y[val\_idx]}
            \CommentTok{\# Fit polynomial}
\NormalTok{            coeff }\OperatorTok{=}\NormalTok{ np.polyfit(X\_train, Y\_train, order)}
            \CommentTok{\# Predict on validation set}
\NormalTok{            Y\_pred }\OperatorTok{=}\NormalTok{ np.polyval(coeff, X\_val)}
            \CommentTok{\# Compute mean squared error}
\NormalTok{            mse }\OperatorTok{=}\NormalTok{ np.mean((Y\_val }\OperatorTok{{-}}\NormalTok{ Y\_pred) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}
\NormalTok{            fold\_errors.append(mse)}
\NormalTok{        cross\_val\_errors[order }\OperatorTok{{-}} \DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.mean(fold\_errors)}
    \CommentTok{\# Find the best order}
\NormalTok{    best\_order }\OperatorTok{=}\NormalTok{ np.argmin(cross\_val\_errors) }\OperatorTok{+} \DecValTok{1}
    \CommentTok{\# Fit the best polynomial on the entire dataset}
\NormalTok{    best\_coeff }\OperatorTok{=}\NormalTok{ np.polyfit(X\_norm, Y, best\_order)}
    \ControlFlowTok{return}\NormalTok{ best\_order, best\_coeff, mnstd}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Explaining the \(k\)-fold
Cross-Validation}{Explaining the k-fold Cross-Validation}}\label{explaining-the-k-fold-cross-validation}

The line

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kf }\OperatorTok{=}\NormalTok{ KFold(n\_splits}\OperatorTok{=}\NormalTok{q, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

initializes a \(k\)-Fold cross-validator object from the
\texttt{sklearn.model\_selection} library. The \texttt{n\_splits}
parameter specifies the number of folds. The data will be divided into
\texttt{q} parts. In each iteration of the cross-validation, one part
will be used as the validation set, and the remaining \texttt{q-1} parts
will be used as the training set.

The \texttt{kf.split} method takes the dataset \texttt{X\_norm} as input
and yields pairs of index arrays for each fold: * \texttt{train\_idx}:
In each iteration, \texttt{train\_idx} is an array containing the
indices of the data points that belong to the training set for that
specific fold. * \texttt{val\_idx}: Similarly, \texttt{val\_idx} is an
array containing the indices of the data points that belong to the
validation (or test) set for that specific fold.

The loop will run \texttt{q} times (the number of splits). In each
iteration, a different fold serves as the validation set, while the
other \texttt{q-1} folds form the training set.

Here's a Python example to demonstrate the values of \texttt{train\_idx}
and \texttt{val\_idx}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample data (e.g., X\_norm)}
\NormalTok{X\_norm }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{1.0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Original data indices: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{arange(}\BuiltInTok{len}\NormalTok{(X\_norm))}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# Number of splits (folds)}
\NormalTok{q }\OperatorTok{=} \DecValTok{3} \CommentTok{\# Let\textquotesingle{}s use 3 folds for this example}
\CommentTok{\# Initialize KFold}
\NormalTok{kf }\OperatorTok{=}\NormalTok{ KFold(n\_splits}\OperatorTok{=}\NormalTok{q, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\CommentTok{\# Iterate through the splits and print the indices}
\NormalTok{fold\_number }\OperatorTok{=} \DecValTok{1}
\ControlFlowTok{for}\NormalTok{ train\_idx, val\_idx }\KeywordTok{in}\NormalTok{ kf.split(X\_norm):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"{-}{-}{-} Fold }\SpecialCharTok{\{}\NormalTok{fold\_number}\SpecialCharTok{\}}\SpecialStringTok{ {-}{-}{-}"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Train indices: }\SpecialCharTok{\{}\NormalTok{train\_idx}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Validation indices: }\SpecialCharTok{\{}\NormalTok{val\_idx}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training data for this fold: }\SpecialCharTok{\{}\NormalTok{X\_norm[train\_idx]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Validation data for this fold: }\SpecialCharTok{\{}\NormalTok{X\_norm[val\_idx]}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    fold\_number }\OperatorTok{+=} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Original data indices: [0 1 2 3 4 5 6 7 8 9]

--- Fold 1 ---
Train indices: [2 3 4 6 7 9]
Validation indices: [0 1 5 8]
Training data for this fold: [0.3 0.4 0.5 0.7 0.8 1. ]
Validation data for this fold: [0.1 0.2 0.6 0.9]

--- Fold 2 ---
Train indices: [0 1 3 4 5 6 8]
Validation indices: [2 7 9]
Training data for this fold: [0.1 0.2 0.4 0.5 0.6 0.7 0.9]
Validation data for this fold: [0.3 0.8 1. ]

--- Fold 3 ---
Train indices: [0 1 2 5 7 8 9]
Validation indices: [3 4 6]
Training data for this fold: [0.1 0.2 0.3 0.6 0.8 0.9 1. ]
Validation data for this fold: [0.4 0.5 0.7]
\end{verbatim}

\subsection{Making Predictions}\label{making-predictions}

To make predictions, we can use the coefficients. The data is
standardized around its mean in the polynomial function, which is why
the vector \texttt{mnstd} is required. The coefficient vector is
computed based on the normalized data, and this must be taken into
account if further analytical calculations are performed on the fitted
model.

The polynomial approximation of \(C_D\) is:

\[
C_D(x) = w_8 x^8 + w_7 x^7 + \dots + w_1 x + w_0,
\]

where \(x\) is normalized as:

\[
\bar{x} = \frac{x - \mu(X)}{\sigma(X)}
\]

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ predict\_polynomial\_fit(X, coeff, mnstd):}
    \CommentTok{"""}
\CommentTok{    Generates predictions for the polynomial fit.}

\CommentTok{    Args:}
\CommentTok{        X (array{-}like): Original independent variable data.}
\CommentTok{        coeff (array): Coefficients of the best{-}fit polynomial.}
\CommentTok{        mnstd (tuple): Normalization parameters (mean, std) for X.}

\CommentTok{    Returns:}
\CommentTok{        tuple: De{-}normalized predicted X values and corresponding Y predictions.}
\CommentTok{    """}
    \CommentTok{\# Normalize X}
\NormalTok{    X\_norm }\OperatorTok{=}\NormalTok{ (X }\OperatorTok{{-}}\NormalTok{ mnstd[}\DecValTok{0}\NormalTok{]) }\OperatorTok{/}\NormalTok{ mnstd[}\DecValTok{1}\NormalTok{]}

    \CommentTok{\# Generate predictions}
\NormalTok{    X\_pred }\OperatorTok{=}\NormalTok{ np.linspace(}\BuiltInTok{min}\NormalTok{(X\_norm), }\BuiltInTok{max}\NormalTok{(X\_norm), }\DecValTok{100}\NormalTok{)}
\NormalTok{    Y\_pred }\OperatorTok{=}\NormalTok{ np.polyval(coeff, X\_pred)}

    \CommentTok{\# De{-}normalize X for plotting}
\NormalTok{    X\_pred\_original }\OperatorTok{=}\NormalTok{ X\_pred }\OperatorTok{*}\NormalTok{ mnstd[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ mnstd[}\DecValTok{0}\NormalTok{]}

    \ControlFlowTok{return}\NormalTok{ X\_pred\_original, Y\_pred}
\end{Highlighting}
\end{Shaded}

\subsection{Plotting the Results}\label{plotting-the-results}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_polynomial\_fit(X, Y, X\_pred\_original, Y\_pred, best\_order, y\_true}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Visualizes the polynomial fit.}

\CommentTok{    Args:}
\CommentTok{        X (array{-}like): Original independent variable data.}
\CommentTok{        Y (array{-}like): Original dependent variable data.}
\CommentTok{        X\_pred\_original (array): De{-}normalized predicted X values.}
\CommentTok{        Y\_pred (array): Predicted Y values.}
\CommentTok{        y\_true (array): True Y values.}
\CommentTok{        best\_order (int): The optimal polynomial order.}
\CommentTok{    """}
\NormalTok{    plt.scatter(X, Y, label}\OperatorTok{=}\StringTok{"Training Data"}\NormalTok{, color}\OperatorTok{=}\StringTok{"grey"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{)}
\NormalTok{    plt.plot(X\_pred\_original, Y\_pred, label}\OperatorTok{=}\SpecialStringTok{f"Order }\SpecialCharTok{\{}\NormalTok{best\_order}\SpecialCharTok{\}}\SpecialStringTok{ Polynomial"}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ y\_true }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        plt.plot(X, y\_true, label}\OperatorTok{=}\StringTok{"True Function"}\NormalTok{, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{)}
\NormalTok{    plt.title(}\SpecialStringTok{f"Polynomial Fit (Order }\SpecialCharTok{\{}\NormalTok{best\_order}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{)}
\NormalTok{    plt.xlabel(}\StringTok{"X"}\NormalTok{)}
\NormalTok{    plt.ylabel(}\StringTok{"Y"}\NormalTok{)}
\NormalTok{    plt.legend()}
\NormalTok{    plt.show()}
\end{Highlighting}
\end{Shaded}

\section{Example One: Aerofoil Drag}\label{example-one-aerofoil-drag}

The circles in Figure~\ref{fig-aerofoil-drag} represent 101 drag
coefficient values obtained through a numerical simulation by iterating
each member of a family of aerofoils towards a target lift value (see
the Appendix, Section A.3 in (\textbf{Forr08a?})). The members of the
family have different shapes, as determined by the sampling plan:

\[
X = {x_1, x_2, \dots, x_{101}}
\]

The responses are:

\[
C_D = \{C_{D}^{(1)}, C_{D}^{(2)}, \dots, C_{D}^{(101)}\}
\]

These responses are corrupted by ``noise,'' which are deviations of the
systematic variety caused by small changes in the computational mesh
from one design to the next.

The original data is measured in natural units, i.e., from \(-0.3\)
untion to \(0.1\) unit. The data is normalized to the range of \(0\) to
\(1\) for the computation with the \texttt{aerofoilcd} function. The
data is then fitted with a polynomial of order \(m\). To obtain the best
polynomial through this data, the following Python code can be used:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotpython.surrogate.functions.forr08a }\ImportTok{import}\NormalTok{ aerofoilcd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{101}\NormalTok{)}
\CommentTok{\# normalize the data so that it will be in the range of 0 to 1}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(X)}
\NormalTok{b }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(X)}
\NormalTok{X\_cod }\OperatorTok{=}\NormalTok{ (X }\OperatorTok{{-}}\NormalTok{ a) }\OperatorTok{/}\NormalTok{ (b }\OperatorTok{{-}}\NormalTok{ a)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ aerofoilcd(X\_cod)}
\NormalTok{best\_order, best\_coeff, mnstd }\OperatorTok{=}\NormalTok{ polynomial\_fit(X, y)}
\NormalTok{X\_pred\_original, Y\_pred }\OperatorTok{=}\NormalTok{ predict\_polynomial\_fit(X, best\_coeff, mnstd)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_polynomial\_fit(X, y, X\_pred\_original, Y\_pred, best\_order)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_poly_files/figure-pdf/fig-aerofoil-drag-output-1.pdf}}

}

\caption{\label{fig-aerofoil-drag}Aerofoil drag data}

\end{figure}%

Figure~\ref{fig-aerofoil-drag} shows an eighth-order polynomial fitted
through the aerofoil drag data. The order was selected via
cross-validation, and the coefficients were determined through
likelihood maximization. Results, i.e, the best polynomial order and
coefficients, are printed in the console. The coefficients are stored in
the vector \texttt{best\_coeff}, which contains the coefficients of the
polynomial in descending order. The first element is the coefficient of
\(x^8\), and the last element is the constant term. The vector
\texttt{mnstd}, containing the mean and standard deviation of \(X\), is:

\phantomsection\label{aerofoil-coefficients}
\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best polynomial order: }\SpecialCharTok{\{}\NormalTok{best\_order}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Coefficients (starting with w0):}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{best\_coeff}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Normalization parameters (mean, std):}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{mnstd}\SpecialCharTok{\}}\CharTok{\textbackslash{}n}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Best polynomial order: 8

Coefficients (starting with w0):
 [-0.00022964 -0.00014636  0.00116742  0.00052988 -0.0016912  -0.00047398
  0.00244373  0.00270342  0.03041508]

Normalization parameters (mean, std):
 (np.float64(-0.09999999999999999), np.float64(0.11661903789690602))
\end{verbatim}

\section{Example Two: A Multimodal Test
Case}\label{example-two-a-multimodal-test-case}

Let us consider the one-variable test function:

\[
f(x) = (6x - 2)^2 \sin(12x - 4).
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotpython.surrogate.functions.forr08a }\ImportTok{import}\NormalTok{ onevar}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{51}\NormalTok{)}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ onevar(X)}
\CommentTok{\# initialize random seed}
\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ y\_true }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(X))}\OperatorTok{*}\FloatTok{1.1}
\NormalTok{best\_order, best\_coeff, mnstd }\OperatorTok{=}\NormalTok{ polynomial\_fit(X, y)}
\NormalTok{X\_pred\_original, Y\_pred }\OperatorTok{=}\NormalTok{ predict\_polynomial\_fit(X, best\_coeff, mnstd)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_polynomial\_fit(X, y, X\_pred\_original, Y\_pred, best\_order, y\_true}\OperatorTok{=}\NormalTok{y\_true)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_poly_files/figure-pdf/fig-onevar-output-1.pdf}}

}

\caption{\label{fig-onevar}Onevar function}

\end{figure}%

This function, depicted by the dotted line in Figure~\ref{fig-onevar},
has local minima of different depths, which can be deceptive to some
surrogate-based optimization procedures. Here, we use it as an example
of a multimodal function for polynomial fitting.

We generate the training data (depicted by circles in
Figure~\ref{fig-onevar}) by adding normally distributed noise to the
function. Figure~\ref{fig-onevar} shows a seventh-order polynomial
fitted through the noisy data. This polynomial was selected as it
minimizes the cross-validation metric.

\section{Extending to Multivariable Polynomial
Models}\label{extending-to-multivariable-polynomial-models}

While the examples above focus on the one-variable case, real-world
engineering problems typically involve multiple input variables. For
\(k\)-variable problems, polynomial approximation becomes significantly
more complex but follows the same fundamental principles. For a
\(k\)-dimensional input space, the polynomial approximation can be
expressed as:

\[
\hat{f}(\vec{x}) = \sum_{i=1}^N w_i \phi_i(\vec{x}),
\] where \(\phi_i(\vec{x})\) represents multivariate basis functions,
and \(N\) is the total number of terms in the polynomial. Unlike the
univariate case, these basis functions include all possible combinations
of variables up to the selected polynomial order \(m\), which might
result in a ``basis function explosion'' as the number of variables
increases.

For a third-order polynomial (\(m = 3\)) with three variables
(\(k = 3\)), the complete set of basis functions would include 20 terms:

\begin{align} 
\text{Constant term: } & {1} \\
\text{First-order terms: } & {x_1, x_2, x_3} \\
\text{Second-order terms: } & {x_1^2, x_2^2, x_3^2, x_1x_2, x_1x_3, x_2x_3} \\
\text{Third-order terms: } & {x_1^3, x_2^3, x_3^3, x_1^2x_2, x_1^2x_3, x_2^2x_1, x_2^2x_3, x_3^2x_1, x_3^2x_2, x_1x_2x_3}
\end{align}

The total number of terms grows combinatorially as
\(N = \binom{k+m}{m}\), which quickly becomes prohibitive as
dimensionality increases. For example, a 10-variable cubic polynomial
requires \(\binom{13}{3} = 286\) coefficients! This exponential growth
creates three interrelated challenges:

\begin{itemize}
\tightlist
\item
  Model Selection: Determining the appropriate polynomial order \(m\)
  that balances complexity with generalization ability
\item
  Coefficient Estimation: Computing the potentially large number of
  weights \(\vec{w}\) while avoiding numerical instability
\item
  Term Selection: Identifying which specific basis functions should be
  included, as many may be irrelevant to the response
\end{itemize}

Several techniques have been developed to address these challenges:

\begin{itemize}
\tightlist
\item
  Regularization methods (LASSO, ridge regression) that penalize model
  complexity
\item
  Stepwise regression algorithms that incrementally add or remove terms
\item
  Dimension reduction techniques that project the input space to lower
  dimensions
\item
  Orthogonal polynomials that improve numerical stability for
  higher-order models
\end{itemize}

These limitations of polynomial models in higher dimensions motivate the
exploration of more flexible surrogate modeling approaches like Radial
Basis Functions and Kriging, which we'll examine in subsequent sections.

\section{Jupyter Notebook}\label{jupyter-notebook-4}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_poly.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Radial Basis Function Models}\label{sec-rbf}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  This section is based on chapter 2.3 in (\textbf{Forr08a?}).
\item
  The following Python packages are imported:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{Radial Basis Function
Models}\label{radial-basis-function-models}

Scientists and engineers frequently tackle complex functions by
decomposing them into a ``vocabulary'' of simpler, well-understood basic
functions. These fundamental building blocks possess properties that
make them easier to analyze mathematically and implement
computationally. We explored this concept earlier with multivariable
polynomials, where complex behaviors were modeled using combinations of
polynomial terms such as \(1\), \(x_1\), \(x_2\), \(x_1^2\), and
\(x_1 x_2\). This approach is not limited to polynomials; it extends to
various function classes, including trigonometric functions, exponential
functions, and even more complex structures.

While Fourier analysis---perhaps the most widely recognized example of
this approach---excels at representing periodic phenomena through sine
and cosine functions, the focus in (\textbf{Forr08a?}) is broader. They
aim to approximate arbitrary smooth, continuous functions using
strategically positioned basis functions. Specifically, radial basis
function (RBF) models employ symmetrical basis functions centered at
selected points distributed throughout the design space. These basis
functions have the unique property that their output depends only on the
distance from their center point.

First, we give a definition of the Euclidean distance, which is the most
common distance measure used in RBF models.

\begin{definition}[Euclidean
Distance]\protect\hypertarget{def-euclidean-distance}{}\label{def-euclidean-distance}

The Euclidean distance between two points in a \(k\)-dimensional space
is defined as:

\begin{equation}\phantomsection\label{eq-euclidean-distance}{
\|\vec{x} - \vec{c}\| = \sqrt{\sum_{i=1}^{k} (x_i - c_i)^2},
}\end{equation}

where:

\begin{itemize}
\tightlist
\item
  \(\vec{x} = (x_1, x_2, \ldots, x_d)\) is the first point,
\item
  \(\vec{c} = (c_1, c_2, \ldots, c_d)\) is the second point, and
\item
  \(k\) is the number of dimensions.
\end{itemize}

\end{definition}

The Euclidean distance measure represents the straight-line distance
between two points in Euclidean space.

Using the Euclidean distance, we can define the radial basis function
(RBF) model.

\begin{definition}[Radial Basis Function
(RBF)]\protect\hypertarget{def-rbf}{}\label{def-rbf}

Mathematically, a radial basis function \(\psi\) can be expressed as:

\begin{equation}\phantomsection\label{eq-rbf-def}{
\psi(\vec{x}) = \psi(\|\vec{x} - \vec{c}\|),
}\end{equation} where \(\vec{x}\) is the input vector, \(\vec{c}\) is
the center of the function, and \(\|\vec{x} - \vec{c}\|\) denotes the
Euclidean distance between \(\vec{x}\) and \(\vec{c}\).

\end{definition}

In the context of RBFs, the Euclidean distance calculation determines
how much influence a particular center point \(\vec{c}\) has on the
prediction at point \(\vec{x}\).

We will first examine interpolating RBF models, which assume noise-free
data and pass exactly through all training points. This approach
provides an elegant mathematical foundation before we consider more
practical scenarios where data contains measurement or process noise.

\subsection{Fitting Noise-Free Data}\label{fitting-noise-free-data}

Let us consider the scalar valued function \(f\) observed without error,
according to the sampling plan
\(X = \{\vec{x}^{(1)}, \vec{x}^{(2)}, \ldots, \vec{x}^{(n)}\}^T\),
yielding the responses
\(\vec{y} = \{y^{(1)}, y^{(2)}, \ldots, y^{(n)}\}^T\).

For a given set of \(n_c\) centers \(\vec{c}^{(i)}\), we would like to
express the RBF model as a linear combination of the basis functions
centered at these points. The goal is to find the weights \(\vec{w}\)
that minimize the error between the predicted and observed values. Thus,
we seek a radial basis function approximation to \(f\) of the fixed
form:

\begin{equation}\phantomsection\label{eq-rbf}{
\hat{f}(\vec{x}) = \sum_{i=1}^{n_c} w_i \psi(||\vec{x} - \vec{c}^{(i)}||),
}\end{equation} where

\begin{itemize}
\tightlist
\item
  \(w_i\) are the weights of the \(n_c\) basis functions,
\item
  \(\vec{c}^{(i)}\) are the \(n_c\) centres of the basis functions, and
\item
  \(\psi\) is a radial basis function.
\end{itemize}

The notation \(||\cdot||\) denotes the Euclidean distance between two
points in the design space as defined in
Equation~\ref{eq-euclidean-distance}.

\subsubsection{Selecting Basis Functions: From Fixed to Parametric
Forms}\label{selecting-basis-functions-from-fixed-to-parametric-forms}

When implementing a radial basis function model, we initially have one
undetermined parameter per basis function: the weight applied to each
function's output. This simple parameterization remains true when we
select from several standard fixed-form basis functions, such as:

\begin{itemize}
\tightlist
\item
  Linear (\(\psi(r) = r\)): The simplest form, providing a response
  proportional to distance
\item
  Cubic (\(\psi(r) = r^3\)): Offers stronger emphasis on points farther
  from the center
\item
  Thin plate spline (\(\psi(r) = r^2 \ln r\)): Models the physical
  bending of a thin sheet, providing excellent smoothness properties
\end{itemize}

While these fixed basis functions are computationally efficient, they
offer limited flexibility in how they generalize across the design
space. For more adaptive modeling power, we can employ parametric basis
functions that introduce additional tunable parameters:

\begin{itemize}
\tightlist
\item
  Gaussian (\(\psi(r) = e^{-r^2/(2\sigma^2)}\)): Produces bell-shaped
  curves with \(\sigma\) controlling the width of influence
\item
  Multiquadric (\(\psi(r) = (r^2 + \sigma^2)^{1/2}\)): Provides broader
  coverage with less localized effects
\item
  Inverse multiquadric (\(\psi(r) = (r^2 + \sigma^2)^{-1/2}\)): Offers
  sharp peaks near centers with asymptotic behavior
\end{itemize}

The parameter \(\sigma\) in these functions serves as a shape parameter
that controls how rapidly the function's influence decays with distance.
This added flexibility enables significantly better generalization,
particularly when modeling complex responses, though at the cost of a
more involved parameter estimation process requiring optimization of
both weights and shape parameters.

\begin{example}[Gaussian
RBF]\protect\hypertarget{exm-rbf-gaussian}{}\label{exm-rbf-gaussian}

Using the general definition of a radial basis function
(Equation~\ref{eq-rbf-def}), we can express the Gaussian RBF as:
\begin{equation}\phantomsection\label{eq-gaussian-rbf}{
\psi(\vec{x}) = \exp\left(-\frac{\|\vec{x} - \vec{c}\|^2}{2\sigma^2}\right)  = \exp\left(-\frac{\sum_{j=1}^{k} (x_j - c_j)^2}{2\sigma^2}\right) 
}\end{equation} where:

\begin{itemize}
\tightlist
\item
  \(\vec{x}\) is the input vector,
\item
  \(\vec{c}\) is the center vector,
\item
  \(\|\vec{x} - \vec{c}\|\) is the Euclidean distance between the input
  and center, and
\item
  \(\sigma\) is the width parameter that controls how quickly the
  function's response diminishes with distance from the center.
\end{itemize}

The Gaussian RBF produces a bell-shaped response that reaches its
maximum value of 1 when \(\vec{x} = \vec{c}\) and asymptotically
approaches zero as the distance increases. The parameter \(\sigma\)
determines how ``localized'' the response is---smaller values create a
narrower peak with faster decay, while larger values produce a broader,
more gradual response across the input space.
Figure~\ref{fig-rbf-gaussian-k1} shows the Gaussian RBF for different
values of \(\sigma\) in an one-dimensional space. The center of the RBF
is set at 0, and the width parameter \(\sigma\) varies to illustrate how
it affects the shape of the function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gaussian\_rbf(x, center, sigma):}
    \CommentTok{"""}
\CommentTok{    Compute the Gaussian Radial Basis Function.}

\CommentTok{    Args:}
\CommentTok{        x (ndarray): Input points}
\CommentTok{        center (float): Center of the RBF}
\CommentTok{        sigma (float): Width parameter}

\CommentTok{    Returns:}
\CommentTok{        ndarray: RBF values}
\CommentTok{    """}
    \ControlFlowTok{return}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{((x }\OperatorTok{{-}}\NormalTok{ center)}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ sigma}\OperatorTok{**}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_rbf_files/figure-pdf/fig-rbf-gaussian-k1-output-1.pdf}}

}

\caption{\label{fig-rbf-gaussian-k1}Gaussian RBF}

\end{figure}%

The sum of Gaussian RBFs can be visualized by summing the individual
Gaussian RBFs centered at different points as shown in
Figure~\ref{fig-rbf-gaussian-k2}. The following code snippet
demonstrates how to create this plot showing the sum of three Gaussian
RBFs with different centers and a common width parameter \(\sigma\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_rbf_files/figure-pdf/fig-rbf-gaussian-k2-output-1.pdf}}

}

\caption{\label{fig-rbf-gaussian-k2}Sum of Gaussian RBFs}

\end{figure}%

\end{example}

\subsubsection{The Interpolation Condition: Elegant Solutions Through
Linear
Systems}\label{the-interpolation-condition-elegant-solutions-through-linear-systems}

A remarkable property of radial basis function models is that regardless
of which basis functions we choose---parametric or fixed---determining
the weights \(\vec{w}\) remains straightforward through interpolation.
The core principle is elegantly simple: we require our model to exactly
reproduce the observed data points:

\begin{equation}\phantomsection\label{eq-rbf-interp}{
\hat{f}(\vec{x}^{(i)}) = y^{(i)}, \quad i = 1, 2, \ldots, n.
}\end{equation}

This constraint produces one of the most powerful aspects of RBF
modeling: while the system in Equation~\ref{eq-rbf-interp} is linear
with respect to the weights \(\vec{w}\), the resulting predictor
\(\hat{f}\) can capture highly nonlinear relationships in the data. The
RBF approach transforms a complex nonlinear modeling problem into a
solvable linear algebra problem.

For a unique solution to exist, we require that the number of basis
functions equals the number of data points (\(n_c = n\)). The standard
practice, which greatly simplifies implementation, is to center each
basis function at a training data point, setting
\(\vec{c}^{(i)} = \vec{x}^{(i)}\) for all \(i = 1, 2, \ldots, n\). This
choice allows us to express the interpolation condition as a compact
matrix equation:

\[
\Psi \vec{w} = \vec{y}.
\]

Here, \(\Psi\) represents the Gram matrix (also called the design matrix
or kernel matrix), whose elements measure the similarity between data
points:

\[
\Psi_{i,j} = \psi(||\vec{x}^{(i)} - \vec{x}^{(j)}||), \quad i, j = 1, 2, \ldots, n.
\]

The solution for the weight vector becomes:

\[
\vec{w} = \Psi^{-1} \vec{y}.
\]

This matrix inversion step is the computational core of the RBF model
fitting process, and the numerical properties of this operation depend
critically on the chosen basis function. Different basis functions
produce Gram matrices with distinct conditioning properties, directly
affecting both computational stability and the model's generalization
capabilities.

\subsection{Numerical Stability Through Positive Definite
Matrices}\label{numerical-stability-through-positive-definite-matrices}

A significant advantage of Gaussian and inverse multiquadric basis
functions lies in their mathematical guarantees. (\textbf{vapn98a?})
demonstrated that these functions always produce symmetric positive
definite Gram matrices when using strictly positive definite kernels
(see Section~\ref{sec-strictly-positive-definite}), which is a critical
property for numerical reliability. Unlike other basis functions that
may lead to ill-conditioned systems, these functions ensure the
existence of unique, stable solutions.

This positive definiteness enables the use of Cholesky factorization,
which offers substantial computational advantages over standard matrix
inversion techniques. The Cholesky approach reduces the computational
cost (reducing from \(O(n^3)\) to roughly \(O(n^3/3)\)) while
significantly improving numerical stability when handling the inevitable
rounding errors in floating-point arithmetic. This robustness to
numerical issues explains why Gaussian and inverse multiquadric basis
functions remain the preferred choice in many practical RBF
implementations.

Furthermore, the positive definiteness guarantee provides theoretical
assurances about the model's interpolation properties---ensuring that
the RBF interpolant exists and is unique for any distinct set of
centers. This mathematical foundation gives practitioners confidence in
the method's reliability, particularly for complex engineering
applications where model stability is paramount.

The computational advantage stems from how a symmetric positive definite
matrix \(\Psi\) can be efficiently decomposed into the product of an
upper triangular matrix \(U\) and its transpose:

\[
\Psi = U^T U.
\]

This decomposition transforms the system \[
\Psi \vec{w} = \vec{y}
\] into \[
U^T U \vec{w} = \vec{y},
\] which can be solved through two simpler triangular systems:

\begin{itemize}
\tightlist
\item
  First solve \(U^T \vec{v} = \vec{y}\) for the intermediate vector
  \(\vec{v}\)
\item
  Then solve \(U \vec{w} = \vec{v}\) for the desired weights \(\vec{w}\)
\end{itemize}

In Python implementations, this process is elegantly handled using
NumPy's or SciPy's Cholesky decomposition functions, followed by
specialized solvers that exploit the triangular structure:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, cho\_solve}
\CommentTok{\# Compute the Cholesky factorization}
\NormalTok{L }\OperatorTok{=}\NormalTok{ cholesky(Psi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# L is the lower triangular factor}
\NormalTok{weights }\OperatorTok{=}\NormalTok{ cho\_solve((L, }\VariableTok{True}\NormalTok{), y)  }\CommentTok{\# Efficient solver for (L L\^{}T)w = y}
\end{Highlighting}
\end{Shaded}

\subsection{Ill-Conditioning}\label{ill-conditioning}

An important numerical consideration in RBF modeling is that points
positioned extremely close to each other in the input space \(X\) can
lead to severe ill-conditioning of the Gram matrix (\textbf{micc86a?}).
This ill-conditioning manifests as nearly linearly dependent rows and
columns in \(\Psi\), potentially causing the Cholesky factorization to
fail.

While this problem rarely arises with initial space-filling experimental
designs (such as Latin Hypercube or quasi-random sequences), it
frequently emerges during sequential optimization processes that
adaptively add infill points in promising regions. As these clusters of
points concentrate in areas of high interest, the condition number of
the Gram matrix deteriorates, jeopardizing numerical stability.

Several mitigation strategies exist: regularization through ridge-like
penalties (modifying the standard RBF interpolation problem by adding a
penalty term to the diagonal of the Gram matrix. This creates a literal
``ridge'' along the diagonal of the matrix), removing nearly coincident
points, clustering, or applying more sophisticated approaches. One
theoretically elegant solution involves augmenting non-conditionally
positive definite basis functions with polynomial terms
(\textbf{kean05a?}). This technique not only improves conditioning but
also ensures polynomial reproduction properties, enhancing the
approximation quality for certain function classes while maintaining
numerical stability.

Beyond determining \(\vec{w}\), there is, of course, the additional task
of estimating any other parameters introduced via the basis functions. A
typical example is the \(\sigma\) of the Gaussian basis function,
usually taken to be the same for all basis functions, though a different
one can be selected for each centre, as is customary in the case of the
Kriging basis function, to be discussed shortly (once again, we trade
additional parameter estimation complexity versus increased flexibility
and, hopefully, better generalization).

\subsection{Parameter Optimization: A Two-Level
Approach}\label{parameter-optimization-a-two-level-approach}

When building RBF models, we face two distinct parameter estimation
challenges:

\begin{itemize}
\tightlist
\item
  Determining the weights (\(\vec{w}\)): These parameters ensure our
  model precisely reproduces the training data. For any fixed basis
  function configuration, we can calculate these weights directly
  through linear algebra as shown earlier.
\item
  Optimizing shape parameters (like \(\sigma\) in Gaussian RBF): These
  parameters control how the model generalizes to new, unseen data.
  Unlike weights, there's no direct formula to find their optimal
  values.
\end{itemize}

To address this dual challenge, we employ a nested optimization strategy
(inner and outer levels):

\subsubsection{\texorpdfstring{Inner Level
(\(\vec{w}\))}{Inner Level (\textbackslash vec\{w\})}}\label{inner-level-vecw}

For each candidate value of shape parameters (e.g., \(\sigma\)), we
determine the corresponding optimal weights \(\vec{w}\) by solving the
linear system. The \texttt{estim\_weights()} method implements the inner
level optimization by calculating the optimal weights \(\vec{w}\) for a
given shape parameter (\(\sigma\)):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ estim\_weights(}\VariableTok{self}\NormalTok{):}
    \CommentTok{\# [...]}
    
    \CommentTok{\# Construct the Phi (Psi) matrix}
    \VariableTok{self}\NormalTok{.Phi }\OperatorTok{=}\NormalTok{ np.zeros((n, n))}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
            \VariableTok{self}\NormalTok{.Phi[i, j] }\OperatorTok{=} \VariableTok{self}\NormalTok{.basis(d[i, j], }\VariableTok{self}\NormalTok{.sigma)}
            \VariableTok{self}\NormalTok{.Phi[j, i] }\OperatorTok{=} \VariableTok{self}\NormalTok{.Phi[i, j]}
    
    \CommentTok{\# Calculate weights using appropriate method}
    \ControlFlowTok{if} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{4} \KeywordTok{or} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{6}\NormalTok{:}
        \CommentTok{\# Use Cholesky factorization for Gaussian or inverse multiquadric}
        \ControlFlowTok{try}\NormalTok{:}
\NormalTok{            L }\OperatorTok{=}\NormalTok{ cholesky(}\VariableTok{self}\NormalTok{.Phi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
            \VariableTok{self}\NormalTok{.weights }\OperatorTok{=}\NormalTok{ cho\_solve((L, }\VariableTok{True}\NormalTok{), }\VariableTok{self}\NormalTok{.y)}
            \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{True}
        \ControlFlowTok{except}\NormalTok{ np.linalg.LinAlgError:}
            \CommentTok{\# Error handling...}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# Use direct solve for other basis functions}
        \ControlFlowTok{try}\NormalTok{:}
            \VariableTok{self}\NormalTok{.weights }\OperatorTok{=}\NormalTok{ np.linalg.solve(}\VariableTok{self}\NormalTok{.Phi, }\VariableTok{self}\NormalTok{.y)}
            \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{True}
        \ControlFlowTok{except}\NormalTok{ np.linalg.LinAlgError:}
            \CommentTok{\# Error handling...}
    
    \ControlFlowTok{return} \VariableTok{self}
\end{Highlighting}
\end{Shaded}

This method:

\begin{itemize}
\tightlist
\item
  Creates the Gram matrix (Phi) based on distances between points
\item
  Solves the linear system \(\Psi\vec{w} = \vec{y}\) for weights
\item
  Uses appropriate numerical methods based on the basis function type
  (Cholesky factorization or direct solve)
\end{itemize}

\subsubsection{\texorpdfstring{Outer Level
(\(\sigma\))}{Outer Level (\textbackslash sigma)}}\label{outer-level-sigma}

We use cross-validation to evaluate how well the model generalizes with
different shape parameter values. The outer level optimization is
implemented within the \texttt{fit()} method, where cross-validation is
used to evaluate different \(\sigma\) values:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fit(}\VariableTok{self}\NormalTok{):}
    \ControlFlowTok{if} \VariableTok{self}\NormalTok{.code }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
        \CommentTok{\# Fixed basis function, only w needs estimating}
        \VariableTok{self}\NormalTok{.estim\_weights()}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# Basis function requires a sigma, estimate first using cross{-}validation}
        \CommentTok{\# [...]}
        
        \CommentTok{\# Generate candidate sigma values}
\NormalTok{        sigmas }\OperatorTok{=}\NormalTok{ np.logspace(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{30}\NormalTok{)}
        
        \CommentTok{\# Setup cross{-}validation (determine number of folds)}
        \CommentTok{\# [...]}
        
\NormalTok{        cross\_val }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(sigmas))}
        
        \CommentTok{\# For each candidate sigma value}
        \ControlFlowTok{for}\NormalTok{ sig\_index, sigma }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(sigmas):}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Computing cross{-}validation metric for Sigma=}\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{:.4f\}}\SpecialStringTok{..."}\NormalTok{)}
            
            \CommentTok{\# Perform k{-}fold cross{-}validation}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(from\_idx)):}
                \CommentTok{\# Create and fit model on training subset}
\NormalTok{                temp\_model }\OperatorTok{=}\NormalTok{ Rbf(}
\NormalTok{                    X}\OperatorTok{=}\NormalTok{X\_orig[xs\_temp],}
\NormalTok{                    y}\OperatorTok{=}\NormalTok{y\_orig[xs\_temp],}
\NormalTok{                    code}\OperatorTok{=}\VariableTok{self}\NormalTok{.code}
\NormalTok{                )}
\NormalTok{                temp\_model.sigma }\OperatorTok{=}\NormalTok{ sigma}
                
                \CommentTok{\# Call inner level optimization}
\NormalTok{                temp\_model.estim\_weights()}
                
                \CommentTok{\# Evaluate on held{-}out data}
                \CommentTok{\# [...]}
            
        \CommentTok{\# Select best sigma based on cross{-}validation performance}
\NormalTok{        min\_cv\_index }\OperatorTok{=}\NormalTok{ np.argmin(cross\_val)}
\NormalTok{        best\_sig }\OperatorTok{=}\NormalTok{ sigmas[min\_cv\_index]}
        
        \CommentTok{\# Use the best sigma for final model}
        \VariableTok{self}\NormalTok{.sigma }\OperatorTok{=}\NormalTok{ best\_sig}
        \VariableTok{self}\NormalTok{.estim\_weights()  }\CommentTok{\# Call inner level again with optimal sigma}
\end{Highlighting}
\end{Shaded}

The outer level:

\begin{itemize}
\tightlist
\item
  Generates a range of candidate \(\sigma\) values
\item
  For each \(\sigma\), performs k-fold cross-validation:

  \begin{itemize}
  \tightlist
  \item
    Creates models on subsets of the data
  \item
    Calls the inner level method (estim\_weights()) to determine weights
  \item
    Evaluates prediction quality on held-out data
  \end{itemize}
\item
  Selects the \(\sigma\) that minimizes cross-validation error
\item
  Performs a final call to the inner level method with the optimal
  \(\sigma\)
\end{itemize}

This two-level approach is particularly critical for parametric basis
functions (Gaussian, multiquadric, etc.), where the wrong choice of
shape parameter could lead to either overfitting (too much flexibility)
or underfitting (too rigid). Cross-validation provides an unbiased
estimate of how well different parameter choices will perform on new
data, helping us balance the trade-off between fitting the training data
perfectly and generalizing well.

\section{Python Implementation of the RBF Model}\label{sec-rbf-python}

Section~\ref{sec-rbf-python} shows a Python implementation of this
parameter estimation process (based on a cross-validation routine),
which will represent the surrogate, once its parameters have been
estimated. The model building process is very simple.

Instead of using a dictionary for bookkeeping, we implement a Python
class Rbf that encapsulates all the necessary data and functionality.
The class stores the sampling plan \(X\) as the X attribute and the
corresponding \(n\)-vector of responses \(y\) as the y attribute. The
code attribute specifies the type of basis function to be used. After
fitting the model, the class will also contain the estimated parameter
values \(\vec{w}\) and, if a parametric basis function is used,
\(\sigma\). These are stored in the weights and sigma attributes
respectively.

Finally, a note on prediction error estimation. We have already
indicated that the guarantee of a positive definite \(\Psi\) is one of
the advantages of Gaussian radial basis functions. They also possess
another desirable feature: it is relatively easy to estimate their
prediction error at any \(\vec{x}\) in the design space. Additionally,
the expectation function of the improvement in minimum (or maximum)
function value with respect to the minimum (or maximum) known so far can
also be calculated quite easily, both of these features being very
useful when the optimization of \(f\) is the goal of the surrogate
modelling process.

\subsection{The Rbf Class}\label{the-rbf-class}

The Rbf class implements the Radial Basis Function model. It
encapsulates all the data and methods needed for fitting the model and
making predictions.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, cho\_solve}
\ImportTok{import}\NormalTok{ numpy.random }\ImportTok{as}\NormalTok{ rnd}

\KeywordTok{class}\NormalTok{ Rbf:}
    \CommentTok{"""Radial Basis Function model implementation.}
\CommentTok{    }
\CommentTok{    Attributes:}
\CommentTok{        X (ndarray): The sampling plan (input points).}
\CommentTok{        y (ndarray): The response vector.}
\CommentTok{        code (int): Type of basis function to use.}
\CommentTok{        weights (ndarray, optional): The weights vector (set after fitting).}
\CommentTok{        sigma (float, optional): Parameter for parametric basis functions.}
\CommentTok{        Phi (ndarray, optional): The Gram matrix (set during fitting).}
\CommentTok{        success (bool, optional): Flag indicating successful fitting.}
\CommentTok{    """}
    
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, X}\OperatorTok{=}\VariableTok{None}\NormalTok{, y}\OperatorTok{=}\VariableTok{None}\NormalTok{, code}\OperatorTok{=}\DecValTok{3}\NormalTok{):}
        \CommentTok{"""Initialize the RBF model.}
\CommentTok{        }
\CommentTok{        Args:}
\CommentTok{            X (ndarray, optional):}
\CommentTok{                The sampling plan.}
\CommentTok{            y (ndarray, optional):}
\CommentTok{                The response vector.}
\CommentTok{            code (int, optional):}
\CommentTok{                Type of basis function.}
\CommentTok{                Default is 3 (thin plate spline).}
\CommentTok{        """}
        \VariableTok{self}\NormalTok{.X }\OperatorTok{=}\NormalTok{ X}
        \VariableTok{self}\NormalTok{.y }\OperatorTok{=}\NormalTok{ y}
        \VariableTok{self}\NormalTok{.code }\OperatorTok{=}\NormalTok{ code}
        \VariableTok{self}\NormalTok{.weights }\OperatorTok{=} \VariableTok{None}
        \VariableTok{self}\NormalTok{.sigma }\OperatorTok{=} \VariableTok{None}
        \VariableTok{self}\NormalTok{.Phi }\OperatorTok{=} \VariableTok{None}
        \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{None}
    
    \KeywordTok{def}\NormalTok{ basis(}\VariableTok{self}\NormalTok{, r, sigma}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \CommentTok{"""Compute the value of the basis function.}
\CommentTok{        }
\CommentTok{        Args:}
\CommentTok{            r (float): Radius (distance)}
\CommentTok{            sigma (float, optional): Parameter for parametric basis functions}
\CommentTok{            }
\CommentTok{        Returns:}
\CommentTok{            float: Value of the basis function}
\CommentTok{        """}
        \CommentTok{\# Use instance sigma if not provided}
        \ControlFlowTok{if}\NormalTok{ sigma }\KeywordTok{is} \VariableTok{None} \KeywordTok{and} \BuiltInTok{hasattr}\NormalTok{(}\VariableTok{self}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{):}
\NormalTok{            sigma }\OperatorTok{=} \VariableTok{self}\NormalTok{.sigma}
            
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
            \CommentTok{\# Linear function}
            \ControlFlowTok{return}\NormalTok{ r}
        \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{2}\NormalTok{:}
            \CommentTok{\# Cubic}
            \ControlFlowTok{return}\NormalTok{ r}\OperatorTok{**}\DecValTok{3}
        \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{3}\NormalTok{:}
            \CommentTok{\# Thin plate spline}
            \ControlFlowTok{if}\NormalTok{ r }\OperatorTok{\textless{}} \FloatTok{1e{-}200}\NormalTok{:}
                \ControlFlowTok{return} \DecValTok{0}
            \ControlFlowTok{else}\NormalTok{:}
                \ControlFlowTok{return}\NormalTok{ r}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.log(r)}
        \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{4}\NormalTok{:}
            \CommentTok{\# Gaussian}
            \ControlFlowTok{return}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{(r}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{sigma}\OperatorTok{**}\DecValTok{2}\NormalTok{))}
        \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{5}\NormalTok{:}
            \CommentTok{\# Multi{-}quadric}
            \ControlFlowTok{return}\NormalTok{ (r}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ sigma}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\FloatTok{0.5}
        \ControlFlowTok{elif} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{6}\NormalTok{:}
            \CommentTok{\# Inverse Multi{-}Quadric}
            \ControlFlowTok{return}\NormalTok{ (r}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ sigma}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{)}
        \ControlFlowTok{else}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Invalid basis function code"}\NormalTok{)}
    
    \KeywordTok{def}\NormalTok{ estim\_weights(}\VariableTok{self}\NormalTok{):}
        \CommentTok{"""Estimates the basis function weights if sigma is known or not required.}
\CommentTok{        }
\CommentTok{        Returns:}
\CommentTok{            self: The updated model instance}
\CommentTok{        """}
        \CommentTok{\# Check if sigma is required but not provided}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.code }\OperatorTok{\textgreater{}} \DecValTok{3} \KeywordTok{and} \VariableTok{self}\NormalTok{.sigma }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"The basis function requires a sigma parameter"}\NormalTok{)}
        
        \CommentTok{\# Number of points}
\NormalTok{        n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.y)}
        
        \CommentTok{\# Build distance matrix}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ np.zeros((n, n))}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
\NormalTok{                d[i, j] }\OperatorTok{=}\NormalTok{ np.linalg.norm(}\VariableTok{self}\NormalTok{.X[i] }\OperatorTok{{-}} \VariableTok{self}\NormalTok{.X[j])}
\NormalTok{                d[j, i] }\OperatorTok{=}\NormalTok{ d[i, j]}
        
        \CommentTok{\# Construct the Phi (Psi) matrix}
        \VariableTok{self}\NormalTok{.Phi }\OperatorTok{=}\NormalTok{ np.zeros((n, n))}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
                \VariableTok{self}\NormalTok{.Phi[i, j] }\OperatorTok{=} \VariableTok{self}\NormalTok{.basis(d[i, j], }\VariableTok{self}\NormalTok{.sigma)}
                \VariableTok{self}\NormalTok{.Phi[j, i] }\OperatorTok{=} \VariableTok{self}\NormalTok{.Phi[i, j]}
        
        \CommentTok{\# Calculate weights using appropriate method}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{4} \KeywordTok{or} \VariableTok{self}\NormalTok{.code }\OperatorTok{==} \DecValTok{6}\NormalTok{:}
            \CommentTok{\# Use Cholesky factorization for Gaussian or inverse multiquadric}
            \ControlFlowTok{try}\NormalTok{:}
\NormalTok{                L }\OperatorTok{=}\NormalTok{ cholesky(}\VariableTok{self}\NormalTok{.Phi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
                \VariableTok{self}\NormalTok{.weights }\OperatorTok{=}\NormalTok{ cho\_solve((L, }\VariableTok{True}\NormalTok{), }\VariableTok{self}\NormalTok{.y)}
                \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{True}
            \ControlFlowTok{except}\NormalTok{ np.linalg.LinAlgError:}
                \BuiltInTok{print}\NormalTok{(}\StringTok{"Cholesky factorization failed."}\NormalTok{)}
                \BuiltInTok{print}\NormalTok{(}\StringTok{"Two points may be too close together."}\NormalTok{)}
                \VariableTok{self}\NormalTok{.weights }\OperatorTok{=} \VariableTok{None}
                \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{False}
        \ControlFlowTok{else}\NormalTok{:}
            \CommentTok{\# Use direct solve for other basis functions}
            \ControlFlowTok{try}\NormalTok{:}
                \VariableTok{self}\NormalTok{.weights }\OperatorTok{=}\NormalTok{ np.linalg.solve(}\VariableTok{self}\NormalTok{.Phi, }\VariableTok{self}\NormalTok{.y)}
                \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{True}
            \ControlFlowTok{except}\NormalTok{ np.linalg.LinAlgError:}
                \VariableTok{self}\NormalTok{.weights }\OperatorTok{=} \VariableTok{None}
                \VariableTok{self}\NormalTok{.success }\OperatorTok{=} \VariableTok{False}
        
        \ControlFlowTok{return} \VariableTok{self}
    
    \KeywordTok{def}\NormalTok{ fit(}\VariableTok{self}\NormalTok{):}
        \CommentTok{"""Estimates the parameters of the Radial Basis Function model.}
\CommentTok{        }
\CommentTok{        Returns:}
\CommentTok{            self: The updated model instance}
\CommentTok{        """}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.code }\OperatorTok{\textless{}} \DecValTok{4}\NormalTok{:}
            \CommentTok{\# Fixed basis function, only w needs estimating}
            \VariableTok{self}\NormalTok{.estim\_weights()}
        \ControlFlowTok{else}\NormalTok{:}
            \CommentTok{\# Basis function also requires a sigma, estimate first}
            \CommentTok{\# Save original model data}
\NormalTok{            X\_orig }\OperatorTok{=} \VariableTok{self}\NormalTok{.X.copy()}
\NormalTok{            y\_orig }\OperatorTok{=} \VariableTok{self}\NormalTok{.y.copy()}
            
            \CommentTok{\# Direct search between 10\^{}{-}2 and 10\^{}2}
\NormalTok{            sigmas }\OperatorTok{=}\NormalTok{ np.logspace(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{30}\NormalTok{)}
            
            \CommentTok{\# Number of cross{-}validation subsets}
            \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X) }\OperatorTok{\textless{}} \DecValTok{6}\NormalTok{:}
\NormalTok{                q }\OperatorTok{=} \DecValTok{2}
            \ControlFlowTok{elif} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X) }\OperatorTok{\textless{}} \DecValTok{15}\NormalTok{:}
\NormalTok{                q }\OperatorTok{=} \DecValTok{3}
            \ControlFlowTok{elif} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X) }\OperatorTok{\textless{}} \DecValTok{50}\NormalTok{:}
\NormalTok{                q }\OperatorTok{=} \DecValTok{5}
            \ControlFlowTok{else}\NormalTok{:}
\NormalTok{                q }\OperatorTok{=} \DecValTok{10}
            
            \CommentTok{\# Number of sample points}
\NormalTok{            n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X)}
            
            \CommentTok{\# X split into q randomly selected subsets}
\NormalTok{            xs }\OperatorTok{=}\NormalTok{ rnd.permutation(n)}
\NormalTok{            full\_xs }\OperatorTok{=}\NormalTok{ xs.copy()}
            
            \CommentTok{\# The beginnings of the subsets...}
\NormalTok{            from\_idx }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{0}\NormalTok{, n, n}\OperatorTok{//}\NormalTok{q)}
            \ControlFlowTok{if}\NormalTok{ from\_idx[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{\textgreater{}=}\NormalTok{ n:}
\NormalTok{                from\_idx }\OperatorTok{=}\NormalTok{ from\_idx[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
            
            \CommentTok{\# ...and their ends}
\NormalTok{            to\_idx }\OperatorTok{=}\NormalTok{ np.zeros\_like(from\_idx)}
            \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(from\_idx) }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
\NormalTok{                to\_idx[i] }\OperatorTok{=}\NormalTok{ from\_idx[i}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{            to\_idx[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ n }\OperatorTok{{-}} \DecValTok{1}
            
\NormalTok{            cross\_val }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(sigmas))}
            
            \CommentTok{\# Cycling through the possible values of Sigma}
            \ControlFlowTok{for}\NormalTok{ sig\_index, sigma }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(sigmas):}
                \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Computing cross{-}validation metric for Sigma=}\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{:.4f\}}\SpecialStringTok{..."}\NormalTok{)}
                
\NormalTok{                cross\_val[sig\_index] }\OperatorTok{=} \DecValTok{0}
                
                \CommentTok{\# Model fitting to subsets of the data}
                \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(from\_idx)):}
\NormalTok{                    removed }\OperatorTok{=}\NormalTok{ xs[from\_idx[j]:to\_idx[j]}\OperatorTok{+}\DecValTok{1}\NormalTok{]}
\NormalTok{                    xs\_temp }\OperatorTok{=}\NormalTok{ np.delete(xs, np.arange(from\_idx[j], to\_idx[j]}\OperatorTok{+}\DecValTok{1}\NormalTok{))}
                    
                    \CommentTok{\# Create a temporary model for CV}
\NormalTok{                    temp\_model }\OperatorTok{=}\NormalTok{ Rbf(}
\NormalTok{                        X}\OperatorTok{=}\NormalTok{X\_orig[xs\_temp],}
\NormalTok{                        y}\OperatorTok{=}\NormalTok{y\_orig[xs\_temp],}
\NormalTok{                        code}\OperatorTok{=}\VariableTok{self}\NormalTok{.code}
\NormalTok{                    )}
\NormalTok{                    temp\_model.sigma }\OperatorTok{=}\NormalTok{ sigma}
                    
                    \CommentTok{\# Sigma and subset chosen, now estimate w}
\NormalTok{                    temp\_model.estim\_weights()}
                    
                    \ControlFlowTok{if}\NormalTok{ temp\_model.weights }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{                        cross\_val[sig\_index] }\OperatorTok{=} \FloatTok{1e20}
\NormalTok{                        xs }\OperatorTok{=}\NormalTok{ full\_xs.copy()}
                        \ControlFlowTok{break}
                    
                    \CommentTok{\# Compute vector of predictions at the removed sites}
\NormalTok{                    pr }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(removed))}
                    \ControlFlowTok{for}\NormalTok{ jj, idx }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(removed):}
\NormalTok{                        pr[jj] }\OperatorTok{=}\NormalTok{ temp\_model.predict(X\_orig[idx])}
                    
                    \CommentTok{\# Calculate cross{-}validation error}
\NormalTok{                    cross\_val[sig\_index] }\OperatorTok{+=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((y\_orig[removed] }\OperatorTok{{-}}\NormalTok{ pr)}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(removed)}
                    
\NormalTok{                    xs }\OperatorTok{=}\NormalTok{ full\_xs.copy()}
                
                \CommentTok{\# Now attempt Cholesky on the full set, in case the subsets could}
                \CommentTok{\# be fitted correctly, but the complete X could not}
\NormalTok{                temp\_model }\OperatorTok{=}\NormalTok{ Rbf(}
\NormalTok{                    X}\OperatorTok{=}\NormalTok{X\_orig,}
\NormalTok{                    y}\OperatorTok{=}\NormalTok{y\_orig,}
\NormalTok{                    code}\OperatorTok{=}\VariableTok{self}\NormalTok{.code}
\NormalTok{                )}
\NormalTok{                temp\_model.sigma }\OperatorTok{=}\NormalTok{ sigma}
\NormalTok{                temp\_model.estim\_weights()}
                
                \ControlFlowTok{if}\NormalTok{ temp\_model.weights }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
\NormalTok{                    cross\_val[sig\_index] }\OperatorTok{=} \FloatTok{1e20}
                    \BuiltInTok{print}\NormalTok{(}\StringTok{"Failed to fit complete sample data."}\NormalTok{)}
            
            \CommentTok{\# Find the best sigma}
\NormalTok{            min\_cv\_index }\OperatorTok{=}\NormalTok{ np.argmin(cross\_val)}
\NormalTok{            best\_sig }\OperatorTok{=}\NormalTok{ sigmas[min\_cv\_index]}
            
            \CommentTok{\# Set the best sigma and recompute weights}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Selected sigma=}\SpecialCharTok{\{}\NormalTok{best\_sig}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
            \VariableTok{self}\NormalTok{.sigma }\OperatorTok{=}\NormalTok{ best\_sig}
            \VariableTok{self}\NormalTok{.estim\_weights()}
        
        \ControlFlowTok{return} \VariableTok{self}
    
    \KeywordTok{def}\NormalTok{ predict(}\VariableTok{self}\NormalTok{, x):}
        \CommentTok{"""Calculates the value of the Radial Basis Function surrogate model at x.}
\CommentTok{        }
\CommentTok{        Args:}
\CommentTok{            x (ndarray): Point at which to make prediction}
\CommentTok{            }
\CommentTok{        Returns:}
\CommentTok{            float: Predicted value}
\CommentTok{        """}
        \CommentTok{\# Calculate distances to all sample points}
\NormalTok{        d }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X))}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X)):}
\NormalTok{            d[k] }\OperatorTok{=}\NormalTok{ np.linalg.norm(x }\OperatorTok{{-}} \VariableTok{self}\NormalTok{.X[k])}
        
        \CommentTok{\# Calculate basis function values}
\NormalTok{        phi }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X))}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X)):}
\NormalTok{            phi[k] }\OperatorTok{=} \VariableTok{self}\NormalTok{.basis(d[k], }\VariableTok{self}\NormalTok{.sigma)}
        
        \CommentTok{\# Calculate prediction}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ np.dot(phi, }\VariableTok{self}\NormalTok{.weights)}
        \ControlFlowTok{return}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{RBF Example: The One-Dimensional \texttt{sin}
Function}{RBF Example: The One-Dimensional sin Function}}\label{rbf-example-the-one-dimensional-sin-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, cho\_solve}

\CommentTok{\# Define the data points for fitting}
\NormalTok{x\_centers }\OperatorTok{=}\NormalTok{ np.array([np.pi}\OperatorTok{/}\DecValTok{2}\NormalTok{, np.pi, }\DecValTok{3}\OperatorTok{*}\NormalTok{np.pi}\OperatorTok{/}\DecValTok{2}\NormalTok{]).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# Centers for RBFs}
\NormalTok{y\_values }\OperatorTok{=}\NormalTok{ np.sin(x\_centers.flatten())  }\CommentTok{\# Sine values at these points}

\CommentTok{\# Create and fit the RBF model}
\NormalTok{rbf\_model }\OperatorTok{=}\NormalTok{ Rbf(X}\OperatorTok{=}\NormalTok{x\_centers, y}\OperatorTok{=}\NormalTok{y\_values, code}\OperatorTok{=}\DecValTok{4}\NormalTok{)  }\CommentTok{\# Code 4 is Gaussian RBF}
\NormalTok{rbf\_model.sigma }\OperatorTok{=} \FloatTok{1.0}  \CommentTok{\# Set sigma parameter directly}
\NormalTok{rbf\_model.estim\_weights()  }\CommentTok{\# Calculate optimal weights}

\CommentTok{\# Print the weights}
\BuiltInTok{print}\NormalTok{(}\StringTok{"RBF model weights:"}\NormalTok{, rbf\_model.weights)}

\CommentTok{\# Create a grid for visualization}
\NormalTok{x\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, }\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ np.sin(x\_grid.flatten())  }\CommentTok{\# True sine function}

\CommentTok{\# Generate predictions using the RBF model}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(x\_grid))}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x\_grid)):}
\NormalTok{    y\_pred[i] }\OperatorTok{=}\NormalTok{ rbf\_model.predict(x\_grid[i])}

\CommentTok{\# Calculate individual basis functions for visualization}
\NormalTok{basis\_funcs }\OperatorTok{=}\NormalTok{ np.zeros((}\BuiltInTok{len}\NormalTok{(x\_grid), }\BuiltInTok{len}\NormalTok{(x\_centers)))}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x\_grid)):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x\_centers)):}
        \CommentTok{\# Calculate distance}
\NormalTok{        distance }\OperatorTok{=}\NormalTok{ np.linalg.norm(x\_grid[i] }\OperatorTok{{-}}\NormalTok{ x\_centers[j])}
        \CommentTok{\# Compute basis function value scaled by its weight}
\NormalTok{        basis\_funcs[i, j] }\OperatorTok{=}\NormalTok{ rbf\_model.basis(distance, rbf\_model.sigma) }\OperatorTok{*}\NormalTok{ rbf\_model.weights[j]}

\CommentTok{\# Plot the results}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\CommentTok{\# Plot the true sine function}
\NormalTok{plt.plot(x\_grid, y\_true, }\StringTok{\textquotesingle{}k{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}True sine function\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Plot individual basis functions}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x\_centers)):}
\NormalTok{    plt.plot(x\_grid, basis\_funcs[:, i], }\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, }
\NormalTok{             label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Basis function at x=}\SpecialCharTok{\{}\NormalTok{x\_centers[i][}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Plot the RBF fit (sum of basis functions)}
\NormalTok{plt.plot(x\_grid, y\_pred, }\StringTok{\textquotesingle{}r{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}RBF fit\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Plot the sample points}
\NormalTok{plt.scatter(x\_centers, y\_values, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{100}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Sample points\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Add horizontal line at y=0}
\NormalTok{plt.axhline(y}\OperatorTok{=}\DecValTok{0}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}

\NormalTok{plt.title(}\StringTok{\textquotesingle{}RBF Approximation of Sine Function\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RBF model weights: [ 1.00724398e+00  2.32104414e-16 -1.00724398e+00]
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{006_num_rbf_files/figure-pdf/cell-7-output-2.pdf}}

\section{\texorpdfstring{RBF Example: The Two-Diemnsional \texttt{dome}
Function}{RBF Example: The Two-Diemnsional dome Function}}\label{rbf-example-the-two-diemnsional-dome-function}

The \texttt{dome} function is an example of a test function that can be
used to evaluate the performance of the Radial Basis Function model. It
is a simple mathematical function defined over a two-dimensional space.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ dome(x) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
  \CommentTok{"""}
\CommentTok{  Dome test function.}
\CommentTok{  }
\CommentTok{  Args:}
\CommentTok{      x (ndarray): Input vector (1D array of length 2)}
\CommentTok{  }
\CommentTok{  Returns:}
\CommentTok{      float: Function value}
\CommentTok{  }
\CommentTok{  Examples:}
\CommentTok{      dome(np.array([0.5, 0.5]))}
\CommentTok{  """}
  \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ (}\DecValTok{2}\OperatorTok{*}\NormalTok{x }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

The following code demonstrates how to use the Radial Basis Function
model to approximate a function. It generates a Latin Hypercube sample,
computes the objective function values, estimates the model parameters,
and plots the results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ generate\_rbf\_data(n\_samples}\OperatorTok{=}\DecValTok{10}\NormalTok{, grid\_points}\OperatorTok{=}\DecValTok{41}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Generates data for RBF visualization.}

\CommentTok{    Args:}
\CommentTok{        n\_samples (int): Number of samples for the RBF model}
\CommentTok{        grid\_points (int): Number of grid points for prediction}

\CommentTok{    Returns:}
\CommentTok{        tuple: (rbf\_model, X, Y, Z, Z\_0) {-} Model and grid data for plotting}
\CommentTok{    """}
    \ImportTok{from}\NormalTok{ spotpython.utils.sampling }\ImportTok{import}\NormalTok{ bestlh }\ImportTok{as}\NormalTok{ best\_lh}
    \CommentTok{\# Generate sampling plan}
\NormalTok{    X\_samples }\OperatorTok{=}\NormalTok{ best\_lh(n\_samples, }\DecValTok{2}\NormalTok{, population}\OperatorTok{=}\DecValTok{10}\NormalTok{, iterations}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
    \CommentTok{\# Compute objective function values}
\NormalTok{    y\_samples }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(X\_samples))}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(X\_samples)):}
\NormalTok{        y\_samples[i] }\OperatorTok{=}\NormalTok{ dome(X\_samples[i])}
    \CommentTok{\# Create and fit RBF model}
\NormalTok{    rbf\_model }\OperatorTok{=}\NormalTok{ Rbf(X}\OperatorTok{=}\NormalTok{X\_samples, y}\OperatorTok{=}\NormalTok{y\_samples, code}\OperatorTok{=}\DecValTok{3}\NormalTok{)  }\CommentTok{\# Thin plate spline}
\NormalTok{    rbf\_model.fit()}
    \CommentTok{\# Generate grid for prediction}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, grid\_points)}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, grid\_points)}
\NormalTok{    X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(x, y)}
\NormalTok{    Z\_0 }\OperatorTok{=}\NormalTok{ np.zeros\_like(X)}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ np.zeros\_like(X)}
    
    \CommentTok{\# Evaluate model at grid points}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x)):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(y)):}
\NormalTok{            Z\_0[j, i] }\OperatorTok{=}\NormalTok{ dome(np.array([x[i], y[j]]))}
\NormalTok{            Z[j, i] }\OperatorTok{=}\NormalTok{ rbf\_model.predict(np.array([x[i], y[j]]))}
    
    \ControlFlowTok{return}\NormalTok{ rbf\_model, X, Y, Z, Z\_0}

\KeywordTok{def}\NormalTok{ plot\_rbf\_results(rbf\_model, X, Y, Z, Z\_0}\OperatorTok{=}\VariableTok{None}\NormalTok{, n\_contours}\OperatorTok{=}\DecValTok{10}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Plots RBF approximation results.}

\CommentTok{    Args:}
\CommentTok{        rbf\_model (Rbf): Fitted RBF model}
\CommentTok{        X (ndarray): Grid X{-}coordinates}
\CommentTok{        Y (ndarray): Grid Y{-}coordinates}
\CommentTok{        Z (ndarray): RBF model predictions}
\CommentTok{        Z\_0 (ndarray, optional): True function values for comparison}
\CommentTok{        n\_contours (int): Number of contour levels to plot}
\CommentTok{    """}
    \ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
    
\NormalTok{    plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{))}
    
    \CommentTok{\# Plot the contour}
\NormalTok{    contour }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, Z, n\_contours)}

    \ControlFlowTok{if}\NormalTok{ Z\_0 }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        contour\_0 }\OperatorTok{=}\NormalTok{ plt.contour(X, Y, Z\_0, n\_contours, colors}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, linestyles}\OperatorTok{=}\StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{)}
    
    \CommentTok{\# Plot the sample points}
\NormalTok{    plt.scatter(rbf\_model.X[:, }\DecValTok{0}\NormalTok{], rbf\_model.X[:, }\DecValTok{1}\NormalTok{], }
\NormalTok{                c}\OperatorTok{=}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
    
\NormalTok{    plt.title(}\StringTok{\textquotesingle{}RBF Approximation (Thin Plate Spline)\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.xlabel(}\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.ylabel(}\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.colorbar(label}\OperatorTok{=}\StringTok{\textquotesingle{}f(x1, x2)\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.show()}
\end{Highlighting}
\end{Shaded}

Figure~\ref{fig-rbf-approximation} shows the contour plots of the
underlying function \(f(x_1, x_2) = 0.5[-(2x_1-1)^2-(2x_2-1)^2]\) and
its thin plate spline radial basis function approximation, along with
the 10 points of a Morris-Mitchell optimal Latin hypercube sampling plan
(obtained via best\_lh()).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rbf\_model, X, Y, Z, Z\_0 }\OperatorTok{=}\NormalTok{ generate\_rbf\_data(n\_samples}\OperatorTok{=}\DecValTok{10}\NormalTok{, grid\_points}\OperatorTok{=}\DecValTok{41}\NormalTok{)}
\NormalTok{plot\_rbf\_results(rbf\_model, X, Y, Z, Z\_0)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_rbf_files/figure-pdf/fig-rbf-approximation-output-1.pdf}}

}

\caption{\label{fig-rbf-approximation}RBF Approximation.}

\end{figure}%

\subsection{The Connection Between RBF Models and Neural
Networks}\label{the-connection-between-rbf-models-and-neural-networks}

Radial basis function models share a profound architectural similarity
with artificial neural networks, specifically with what's known as RBF
networks. This connection provides valuable intuition about how RBF
models function. A radial basis function model can be viewed as a
specialized neural network with the following structure:

\begin{itemize}
\tightlist
\item
  Input Layer: Receives the feature vector \(\vec{x}\)
\item
  Hidden Layer: Contains neurons (basis functions) that compute radial
  distances
\item
  Output Layer: Produces a weighted sum of the hidden unit activations
\end{itemize}

Unlike traditional neural networks that use dot products followed by
nonlinear activation functions, RBF networks measure the distance
between inputs and learned center points. This distance is then
transformed by the radial basis function.

Mathematically, the equivalence between RBF models and RBF networks can
be expressed as follows:

The RBF model equation:

\[
\hat{f}(\vec{x}) = \sum_{i=1}^{n_c} w_i \psi(||\vec{x} - \vec{c}^{(i)}||)
\]

directly maps to the following neural network components:

\begin{itemize}
\tightlist
\item
  \(\vec{x}\): Input vector
\item
  \(\vec{c}^{(i)}\): Center vectors for each hidden neuron
\item
  \(\psi(\cdot)\): Activation function (Gaussian, inverse multiquadric,
  etc.)
\item
  \(w_i\): Output weights
\item
  \(\hat{f}(\vec{x})\): Network output
\end{itemize}

\begin{example}[Comparison of RBF Networks and Traditional Neural
Networks]\protect\hypertarget{exm-rbfnn}{}\label{exm-rbfnn}

Consider approximating a simple 1D function \(f(x) = \sin(2\pi x)\) over
the interval \([0,1]\):

The neral network approach would use multiple layers with neurons
computing \(\sigma(w \cdot x + b)\). It would require a large number of
neurons and layers to capture the sine wave's complexity. The network
would learn both weights and biases, making it less interpretable.

The RBF network approach, on the other hand, places basis functions at
strategic points (e.g., 5 evenly spaced centers). Each neuron computes
\(\psi(||x - c_i||)\) (e.g., using Gaussian RBF). The output layer
combines these values with learned weights. If we place Gaussian RBFs
with \(\sigma=0.15\) at \({0.1, 0.3, 0.5, 0.7, 0.9}\), each neuron
responds strongly when the input is close to its center and weakly
otherwise. The network can then learn weights that, when multiplied by
these response patterns and summed, closely approximate the sine
function.

This locality property gives RBF networks a notable advantage: they
offer more interpretable internal representations and often require
fewer neurons for certain types of function approximation compared to
traditional multilayer perceptrons.

The key insight is that while standard neural networks create complex
decision boundaries through compositions of hyperplanes, RBF networks
directly model functions using a set of overlapping ``bumps'' positioned
strategically in the input space.

\end{example}

\section{Radial Basis Function Models for Noisy
Data}\label{radial-basis-function-models-for-noisy-data}

When the responses \(\vec{y} = {y^{(1)}, y^{(2)}, \ldots, y^{(n)}}^T\)
contain measurement or simulation noise, the standard RBF interpolation
approach can lead to overfitting---the model captures both the
underlying function and the random noise. This compromises
generalization performance on new data points. Two principal strategies
address this challenge:

\subsection{Ridge Regularization
Approach}\label{ridge-regularization-approach}

The most straightforward solution involves introducing regularization
through the parameter \(\lambda\) (\textbf{pogg90a?}). This is
implemented by adding \(\lambda\) to the diagonal elements of the Gram
matrix, creating a ``ridge'' that improves numerical stability.
Mathematically, the weights are determined by:

\[
\vec{w} = (\Psi + \lambda I)^{-1} \vec{y},
\]

where \(I\) is an \(n \times n\) identity matrix. This regularized
solution balances two competing objectives:

\begin{itemize}
\tightlist
\item
  fitting the training data accurately versus
\item
  keeping the magnitude of weights controlled to prevent overfitting.
\end{itemize}

Theoretically, optimal performance is achieved when \(\lambda\) equals
the variance of the noise in the response data \(\vec{y}\)
(\textbf{kean05a?}). Since this information is rarely available in
practice, \(\lambda\) is typically estimated through cross-validation
alongside other model parameters.

\subsection{Reduced Basis Approach}\label{reduced-basis-approach}

An alternative strategy involves reducing \(m\), the number of basis
functions. This might result in a non-square \(\Psi\) matrix. With a
non-square \(\Psi\) matrix, the weights are found through least squares
minimization:

\[
\vec{w} = (\Psi^T\Psi)^{-1}\Psi^T\vec{y}
\]

This approach introduces an important design decision: which subset of
points should serve as basis function centers? Several selection
strategies exist:

\begin{itemize}
\tightlist
\item
  Clustering methods that identify representative points
\item
  Greedy algorithms that sequentially select influential centers
\item
  Support vector regression techniques (discussed elsewhere in the
  literature)
\end{itemize}

Additional parameters such as the width parameter \(\sigma\) in Gaussian
bases can be optimized through cross-validation to minimize
generalization error estimates.

The ridge regularization and reduced basis approaches can be combined,
allowing for a flexible modeling framework, though at the cost of a more
complex parameter estimation process. This hybrid approach often yields
superior results for highly noisy datasets or when the underlying
function has varying complexity across the input space.

The broader challenge of building accurate models from noisy
observations is examined comprehensively in the context of Kriging
models, which provide a statistical framework for explicitly modeling
both the underlying function and the noise process.

\section{Jupyter Notebook}\label{jupyter-notebook-5}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_poly.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Kriging (Gaussian Process
Regression)}\label{kriging-gaussian-process-regression}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  This section is based on chapter 2.4 in (\textbf{Forr08a?}).
\item
  The following Python packages are imported:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ (array, zeros, power, ones, exp, multiply,}
\NormalTok{                    eye, linspace, spacing, sqrt, arange,}
\NormalTok{                    append, ravel)}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ squareform, pdist, cdist}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\section{From Gaussian RBF to Kriging Basis
Functions}\label{from-gaussian-rbf-to-kriging-basis-functions}

Kriging can be explained using the concept of radial basis functions
(RBFs), which were introduced in Chapter~\ref{sec-rbf}. An RBF is a
real-valued function whose value depends only on the distance from a
certain point, called the center, usually in a multidimensional space.
The basis function is a function of the distance between the point
\(\vec{x}\) and the center \(\vec{x}^{(i)}\). Other names for basis
functions are \emph{kernel} or \emph{covariance} functions.

\begin{definition}[The Kriging Basis
Functions]\protect\hypertarget{def-kriging-basis-function}{}\label{def-kriging-basis-function}

Kriging (also known as Gaussian Process Regression) uses
\(k\)-dimensional basis functions of the form
\begin{equation}\phantomsection\label{eq-krigingbase}{
\psi^{(i)}(\vec{x}) = 
\psi(\vec{x}^{(i)}, \vec{x}) = \exp \left( - \sum_{j=1}^k \theta_j | x_{j}^{(i)} - x_{j} | ^{p_j} \right),
}\end{equation} where \(\vec{x}\) and \(\vec{x}^{(i)}\) denote the
\(k\)-dim vector \(\vec{x}= (x_1, \ldots, x_k)^T\) and
\(\vec{x}^{(i)}= (x_1^{(i)}, \ldots, x_k^{(i)})^T\), respectively.

\(\Box\)

\end{definition}

Kriging uses a specialized basis function that offers greater
flexibility than standard RBFs. Examining Equation~\ref{eq-krigingbase},
we can observe how Kriging builds upon and extends the Gaussian basis
concept. The key enhancements of Kriging over Gaussian RBF can be
summarized as follows:

\begin{itemize}
\tightlist
\item
  Dimension-specific width parameters: While a Gaussian RBF uses a
  single width parameter \(1/\sigma^2\), Kriging employs a vector
  \(\vec{\theta} = (\theta_1, \theta_2, \ldots, \theta_k)^T\). This
  allows the model to automatically adjust its sensitivity to each input
  dimension, effectively performing automatic feature relevance
  determination.
\item
  Flexible smoothness control: The Gaussian RBF fixes the exponent at 2,
  producing uniformly smooth functions. In contrast, Kriging's
  dimension-specific exponents \(\vec{p} = (p_1, p_2, \ldots, p_k)^T\)
  (typically with \(p_j \in [1, 2]\)) enable precise control over
  smoothness properties in each dimension.
\item
  Unifying framework: When all exponents are set to \(p_j = 2\) and all
  width parameters are equal (\(\theta_j = 1/\sigma^2\) for all \(j\)),
  the Kriging basis function reduces exactly to the Gaussian RBF. This
  makes Gaussian RBF a special case within the more general Kriging
  framework.
\end{itemize}

These enhancements make Kriging particularly well-suited for engineering
problems where variables may operate at different scales or exhibit
varying degrees of smoothness across dimensions. For now, we will only
consider Kriging interpolation. We will cover Kriging regression later.

\section{Building the Kriging Model}\label{building-the-kriging-model}

Consider sample data \(X\) and \(\vec{y}\) from \(n\) locations that are
available in matrix form: \(X\) is a \((n \times k)\) matrix, where
\(k\) denotes the problem dimension and \(\vec{y}\) is a \((n\times 1)\)
vector. We want to find an expression for a predicted values at a new
point \(\vec{x}\), denoted as \(\hat{y}\).

We start with an abstract, not really intuitive concept: The observed
responses \(\vec{y}\) are considered as if they are from a stochastic
process, which will be denoted as
\begin{equation}\phantomsection\label{eq-yvec-51}{
\begin{pmatrix}
Y(\vec{x}^{(1)})\\
\vdots\\
Y(\vec{x}^{(n)})\\
\end{pmatrix}.
}\end{equation}

The set of random vectors from Equation~\ref{eq-yvec-51} (also referred
to as a \emph{random field}) has a mean of \(\vec{1} \mu\), which is a
\((n\times 1)\) vector. The random vectors are correlated with each
other using the basis function expression from
Equation~\ref{eq-krigingbase}:
\begin{equation}\phantomsection\label{eq-corr-kriging-51}{
\text{cor} \left(Y(\vec{x}^{(i)}),Y(\vec{x}^{(l)}) \right) = \exp\left(- \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right).
}\end{equation} Using Equation~\ref{eq-corr-kriging-51}, we can compute
the \((n \times n)\) correlation matrix \(\Psi\) of the observed sample
data as shown in Equation~\ref{eq-corr-matrix-kriging-51},

\begin{equation}\phantomsection\label{eq-corr-matrix-kriging-51}{
\Psi = \begin{pmatrix}
\text{cor}\left(
Y(\vec{x}^{(1)}),
Y(\vec{x}^{(1)}) 
\right) & \ldots &
\text{cor}\left(
Y(\vec{x}^{(1)}),
Y(\vec{x}^{(n)}) 
\right)\\
\vdots  & \vdots &  \vdots\\
 \text{cor}\left(
Y(\vec{x}^{(n)}),
Y(\vec{x}^{(1)}) 
\right)&
\ldots &
\text{cor}\left(
Y(\vec{x}^{(n)}),
Y(\vec{x}^{(n)}) 
\right)
\end{pmatrix},
}\end{equation}

and a covariance matrix as shown in
Equation~\ref{eq-cov-matrix-kriging-52},

\begin{equation}\phantomsection\label{eq-cov-matrix-kriging-52}{
\text{Cov}(Y, Y ) = \sigma^2\Psi.
}\end{equation}

This assumed correlation between the sample data reflects our
expectation that an engineering function will behave in a certain way
and it will be smoothly and continuous.

\begin{refremark}[Note on Stochastic Processes]
See \textbf{?@sec-random-samples-gp} for a more detailed discussion on
realizations of stochastic processes.

\(\Box\)

\label{rem-stocastic-process}

\end{refremark}

We now have a set of \(n\) random variables (\(\mathbf{Y}\)) that are
correlated with each other as described in the \((n \times n)\)
correlation matrix \(\Psi\), see
Equation~\ref{eq-corr-matrix-kriging-51}. The correlations depend on the
absolute distances in dimension \(j\) between the \(i\)-th and the
\(l\)-th sample point \(|x_j^{(i)} - x_j^{(l)}|\) and the corresponding
parameters \(p_j\) and \(\theta_j\) for dimension \(j\). The correlation
is intuitive, because when

\begin{itemize}
\tightlist
\item
  two points move close together, then \(|x_j^{(i)} - x_j| \to 0\) and
  \(\exp \left(-|x_j^{(i)} - x_j|^{p_j} \right) \to 1\) (these points
  show very close correlation and \(Y(x_j^{(i)}) = Y(x_j)\)).
\item
  two points move far apart, then \(|x_j^{(i)} - x_j| \to \infty\) and
  \(\exp \left(-|x_j^{(i)} - x_j|^{p_j} \right) \to 0\) (these points
  show very low correlation).
\end{itemize}

\begin{example}[Correlations for different
\(p_j\)]\protect\hypertarget{exm-kriging-corr-1}{}\label{exm-kriging-corr-1}

Three different correlations are shown in Figure~\ref{fig-pval12}:
\(p_j= 0.1, 1, 2\). The smoothness parameter \(p_j\) affects the
correlation:

\begin{itemize}
\tightlist
\item
  With \(p_j=0.1\), there is basicaly no immediate correlation between
  the points and there is a near discontinuity between the points
  \(Y(\vec{x}_j^{(i)})\) and \(Y(\vec{x}_j)\).
\item
  With \(p_j=2\), the correlation is more smooth and we have a
  continuous gradient through \(x_j^{(i)} - x_j\).
\end{itemize}

Reducing \(p_j\) increases the rate at which the correlation initially
drops with distance. This is shown in Figure~\ref{fig-pval12}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-pval12-output-1.pdf}}

}

\caption{\label{fig-pval12}Correlations with varying \(p\). \(\theta\)
set to 1.}

\end{figure}%

\(\Box\)

\end{example}

\begin{example}[Correlations for different
\(\theta\)]\protect\hypertarget{exm-kriging-corr-2}{}\label{exm-kriging-corr-2}

Figure~\ref{fig-theta12} visualizes the correlation between two points
\(Y(\vec{x}_j^{(i)})\) and \(Y(\vec{x}_j)\) for different values of
\(\theta\). The parameter \(\theta\) can be seen as a \emph{width}
parameter:

\begin{itemize}
\tightlist
\item
  low \(\theta_j\) means that all points will have a high correlation,
  with \(Y(x_j)\) being similar across the sample.
\item
  high \(\theta_j\) means that there is a significant difference between
  the \(Y(x_j)\)'s.
\item
  \(\theta_j\) is a measure of how \emph{active} the function we are
  approximating is.
\item
  High \(\theta_j\) indicate important parameters, see
  Figure~\ref{fig-theta12}.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-theta12-output-1.pdf}}

}

\caption{\label{fig-theta12}Correlations with varying \(\theta\). \(p\)
set to 2.}

\end{figure}%

\(\Box\)

\end{example}

Considering the activity parameter \(\theta\) is useful in
high-dimensional problems where it is difficult to visualize the design
landscape and the effect of the variable is unknown. By examining the
elements of the vector \(\vec{\theta}\), we can identify the most
important variables and focus on them. This is a crucial step in the
optimization process, as it allows us to reduce the dimensionality of
the problem and focus on the most important variables.

\begin{example}[The Correlation Matrix (Detailed
Computation)]\protect\hypertarget{exm-corr-matrix-detailed}{}\label{exm-corr-matrix-detailed}

Let \(n=4\) and \(k=3\). The sample plan is represented by the following
matrix \(X\): \[
X = \begin{pmatrix} x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}\\
x_{31} & x_{32} & x_{33}\\
x_{41} & x_{42} & x_{43}\\ 
\end{pmatrix}
\]

To compute the elements of the matrix \(\Psi\), the following \(k\) (one
for each of the \(k\) dimensions) \((n,n)\)-matrices have to be
computed:

\begin{itemize}
\item
  For \(k=1\), i.e., the first column of \(X\): \[
  D_1 = \begin{pmatrix} x_{11} - x_{11} & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  x_{21} - x_{11} & x_{21} - x_{21} & x_{21} -x_{31} & x_{21} - x_{41} \\ x_{31} - x_{11} & x_{31} - x_{21} & x_{31} -x_{31} & x_{31} - x_{41} \\ x_{41} - x_{11} & x_{41} - x_{21} & x_{41} -x_{31} & x_{41} - x_{41} \\
  \end{pmatrix}
  \]
\item
  For \(k=2\), i.e., the second column of \(X\): \[
  D_2 = \begin{pmatrix} x_{12} - x_{12} & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  x_{22} - x_{12} & x_{22} - x_{22} & x_{22} -x_{32} & x_{22} - x_{42} \\ x_{32} - x_{12} & x_{32} - x_{22} & x_{32} -x_{32} & x_{32} - x_{42} \\ x_{42} - x_{12} & x_{42} - x_{22} & x_{42} -x_{32} & x_{42} - x_{42} \\
  \end{pmatrix}
  \]
\item
  For \(k=3\), i.e., the third column of \(X\): \[
  D_3 = \begin{pmatrix} x_{13} - x_{13} & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  x_{23} - x_{13} & x_{23} - x_{23} & x_{23} -x_{33} & x_{23} - x_{43} \\ x_{33} - x_{13} & x_{33} - x_{23} & x_{33} -x_{33} & x_{33} - x_{43} \\ x_{43} - x_{13} & x_{43} - x_{23} & x_{43} -x_{33} & x_{43} - x_{43} \\\end{pmatrix}
  \]
\end{itemize}

Since the matrices are symmetric and the main diagonals are zero, it is
sufficient to compute the following matrices: \[
D_1 = \begin{pmatrix} 0 & x_{11} - x_{21} & x_{11} -x_{31} & x_{11} - x_{41} \\  0 &  0 & x_{21} -x_{31} & x_{21} - x_{41} \\ 0 & 0 & 0 & x_{31} - x_{41} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\] \[
D_2 = \begin{pmatrix} 0 & x_{12} - x_{22} & x_{12} -x_{32} & x_{12} - x_{42} \\  0 & 0 & x_{22} -x_{32} & x_{22} - x_{42} \\ 0 & 0 & 0 & x_{32} - x_{42} \\ 0 & 0 & 0 & 0 \\
\end{pmatrix}
\]

\[
D_3 = \begin{pmatrix} 0 & x_{13} - x_{23} & x_{13} -x_{33} & x_{13} - x_{43} \\  0 & 0 & x_{23} -x_{33} & x_{23} - x_{43} \\ 0 & 0 & 0 & x_{33} - x_{43} \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

We will consider \(p_l=2\). The differences will be squared and
multiplied by \(\theta_i\), i.e.:

\[
D_1 = \theta_1 \begin{pmatrix} 0 & (x_{11} - x_{21})^2 & (x_{11} -x_{31})^2 & (x_{11} - x_{41})^2 \\  0 &  0 & (x_{21} -x_{31})^2 & (x_{21} - x_{41})^2 \\ 0 & 0 & 0 & (x_{31} - x_{41})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

\[
D_2 = \theta_2 \begin{pmatrix} 0 & (x_{12} - x_{22})^2 & (x_{12} -x_{32})^2 & (x_{12} - x_{42})^2 \\  0 & 0 & (x_{22} -x_{32})^2 & (x_{22} - x_{42})^2 \\ 0 & 0 & 0 & (x_{32} - x_{42})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

\[
D_3 = \theta_3 \begin{pmatrix} 0 & (x_{13} - x_{23})^2 & (x_{13} -x_{33})^2 & (x_{13} - x_{43})^2 \\  0 & 0 & (x_{23} -x_{33})^2 & (x_{23} - x_{43})^2 \\ 0 & 0 & 0 & (x_{33} - x_{43})^2 \\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

The sum of the three matrices \(D=D_1+ D_2 + D_3\) will be calculated
next:

\[
\begin{pmatrix} 0 & 
\theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &
\theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &
\theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
\\  0 &  0 & 
\theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &
\theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
\\ 0 & 0 & 0 & 
\theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
\\ 0 & 0 & 0 & 0 \\\end{pmatrix}
\]

Finally, \[ \Psi = \exp(-D)\] is computed.

Next, we will demonstrate how this computation can be implemented in
Python. We will consider four points in three dimensions and compute the
correlation matrix \(\Psi\) using the basis function from
Equation~\ref{eq-krigingbase}. These points are placed at the origin, at
the unit vectors, and at the points \((100, 100, 100)\) and
\((101, 100, 100)\). So, they form two clusters: one at the origin and
one at \((100, 100, 100)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([ [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{], [}\DecValTok{101}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{]])}
\NormalTok{X}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[  1,   0,   0],
       [  0,   1,   0],
       [100, 100, 100],
       [101, 100, 100]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Psi(X, theta):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((k, n, n))}
    \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k):}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
            \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, n):}
\NormalTok{                D[l, i, j] }\OperatorTok{=}\NormalTok{ theta[l]}\OperatorTok{*}\NormalTok{(X[i,l] }\OperatorTok{{-}}\NormalTok{ X[j,l])}\OperatorTok{**}\DecValTok{2}
\NormalTok{    D }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(D)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ D }\OperatorTok{+}\NormalTok{ D.T}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])
\end{verbatim}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-corr-matrix-build_psi-output-1.pdf}}

}

\caption{\label{fig-corr-matrix-build_psi}Correlation matrix \(\Psi\).}

\end{figure}%

\(\Box\)

\end{example}

\begin{example}[Example: The Correlation Matrix (Using Existing
Functions)]\protect\hypertarget{exm-corr-matrix-existing}{}\label{exm-corr-matrix-existing}

The same result as computed in Example~\ref{exm-corr-matrix-detailed}
can be obtained with existing python functions, e.g., from the package
\texttt{scipy}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
    \ControlFlowTok{return}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{ squareform(pdist(X,}
\NormalTok{                            metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{                            out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{                            w}\OperatorTok{=}\NormalTok{theta))) }\OperatorTok{+}\NormalTok{  multiply(eye(X.shape[}\DecValTok{0}\NormalTok{]),}
\NormalTok{                                                   eps)}

\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\FloatTok{.0}\NormalTok{)}
\NormalTok{Psi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])
\end{verbatim}

The condition number of the correlation matrix \(\Psi\) is a measure of
how well the matrix can be inverted. A high condition number indicates
that the matrix is close to singular, which can lead to numerical
instability in computations involving the inverse of the matrix, see
Section~\ref{sec-conditon-number}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.linalg.cond(Psi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
np.float64(2.163953413738652)
\end{verbatim}

\(\Box\)

\end{example}

\section{\texorpdfstring{MLE to estimate \(\theta\) and
\(p\)}{MLE to estimate \textbackslash theta and p}}\label{mle-to-estimate-theta-and-p}

\subsection{The Log-Likelihood}\label{the-log-likelihood}

Until now, the observed data \(\vec{y}\) was not used. We know what the
correlations mean, but how do we estimate the values of \(\theta_j\) and
where does our observed data \(y\) come in? To estimate the values of
\(\vec{\theta}\) and \(\vec{p}\), they are chosen to maximize the
likelihood of \(\vec{y}\),
\begin{equation}\phantomsection\label{eq-likelihood-55-a}{
L = L\left(Y(\vec{x}^{(1)}), \ldots, Y(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma^2)^{n/2}} \exp\left[ - \frac{\sum_{i=1}^n(Y(\vec{x}^{(i)})-\mu)^2}{2 \sigma^2}\right],
}\end{equation} where \(\mu\) is the mean of the observed data
\(\vec{y}\) and \(\sigma\) is the standard deviation of the errors
\(\epsilon\), which can be expressed in terms of the sample data
\begin{equation}\phantomsection\label{eq-likelihood-55}{
L = \frac{1}{(2\pi \sigma^2)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left[ - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right].
}\end{equation}

\begin{refremark}
The transition from Equation~\ref{eq-likelihood-55-a} to
Equation~\ref{eq-likelihood-55} reflects a shift from assuming
independent errors in the observed data to explicitly modeling the
\emph{correlation structure} between the observed responses, which is a
key aspect of the stochastic process framework used in methods like
Kriging. It can be explained as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initial Likelihood Expression (assuming independent errors):}
  Equation~\ref{eq-likelihood-55-a} is an expression for the likelihood
  of the data set, which is based on the assumption that the errors
  \(\epsilon\) are \emph{independently randomly distributed according to
  a normal distribution with standard deviation \(\sigma\)}. This form
  is characteristic of the likelihood of \(n\) independent observations
  \(Y(\vec{x}^{(i)})\), each following a normal distribution with mean
  \(\mu\) and variance \(\sigma^2\).
\item
  \textbf{Using Vector Notation.} The sum in the exponent, i.e., \[
  \sum_{i=1}^n(Y(\vec{x}^{(i)})-\mu)^2
  \] is equivalent to \[
  (\vec{y} - \vec{1}\mu)^T (\vec{y} - \vec{1}\mu),
  \] assuming \(Y(\vec{x}^{(i)}) = y^{(i)}\) and using vector notation
  for \(\vec{y}\) and \(\vec{1}\mu\).
\item
  \textbf{Assuming Independent Observations:}
  Equation~\ref{eq-likelihood-55-a} assumes that the observations are
  independent, which means that the covariance between any two
  observations \(Y(\vec{x}^{(i)})\) and \(Y(\vec{x}^{(l)})\) is zero for
  \(i \neq l\). In this case, the covariance matrix of the observations
  would be a diagonal matrix with \(\sigma^2\) along the diagonal (i.e.,
  \(\sigma^2 I\)), where \(I\) is the identity matrix.
\item
  \textbf{Stochastic Process and Correlation:} In the context of
  Kriging, the observed responses \(\vec{y}\) are considered as if they
  are from a \emph{stochastic process} or \emph{random field}. This
  means the random variables \(Y(\vec{x}^{(i)})\) at different locations
  \(\vec{x}^{(i)}\) are not independent, but they correlated with each
  other. This correlation is described by an \((n \times n)\)
  \textbf{correlation matrix \(\Psi\)}, which is used instead of
  \(\sigma^2 I\). The strength of the correlation between two points
  \(Y(\vec{x}^{(i)})\) and \(Y(\vec{x}^{(l)})\) depends on the distance
  between them and model parameters \(\theta_j\) and \(p_j\).
\item
  \textbf{Multivariate Normal Distribution:} When random variables are
  correlated, their joint probability distribution is generally
  described by a \emph{multivariate distribution}. Assuming the
  stochastic process follows a Gaussian process, the joint distribution
  of the observed responses \(\vec{y}\) is a \textbf{multivariate normal
  distribution}. A multivariate normal distribution for a vector
  \(\vec{Y}\) with mean vector \(\vec{\mu}\) and covariance matrix
  \(\Sigma\) has a probability density function given by: \[
  p(\vec{y}) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp\left[ -\frac{1}{2}(\vec{y} - \vec{\mu})^T \Sigma^{-1}(\vec{y} - \vec{\mu}) \right].
  \]
\item
  \textbf{Connecting the Expressions:} In the stochastic process
  framework, the following holds:

  \begin{itemize}
  \tightlist
  \item
    The mean vector of the observed data \(\vec{y}\) is \(\vec{1}\mu\).
  \item
    The covariance matrix \(\Sigma\) is constructed by considering both
    the variance \(\sigma^2\) and the correlations \(\Psi\).
  \item
    The covariance between \(Y(\vec{x}^{(i)})\) and \(Y(\vec{x}^{(l)})\)
    is \(\sigma^2 \text{cor}(Y(\vec{x}^{(i)}), Y(\vec{x}^{(l)}))\).
  \item
    Therefore, the covariance matrix is
    \(\Sigma = \sigma^2 \vec{\Psi}\).
  \item
    Substituting \(\vec{\mu} = \vec{1}\mu\) and
    \(\Sigma = \sigma^2 \vec{\Psi}\) into the multivariate normal PDF
    formula, we get: \[
    \Sigma^{-1} = (\sigma^2 \vec{\Psi})^{-1} = \frac{1}{\sigma^2} \vec{\Psi}^{-1}
    \] and \[
    |\Sigma| = |\sigma^2 \vec{\Psi}| = (\sigma^2)^n |\vec{\Psi}|.
    \] The PDF becomes: \[
    p(\vec{y}) = \frac{1}{\sqrt{(2\pi)^n (\sigma^2)^n |\vec{\Psi}|}} \exp\left[ -\frac{1}{2}(\vec{y} - \vec{1}\mu)^T \left(\frac{1}{\sigma^2} \vec{\Psi}^{-1}\right)(\vec{y} - \vec{1}\mu) \right]
    \] and simplifies to: \[
    p(\vec{y}) = \frac{1}{(2\pi \sigma^2)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left[ -\frac{1}{2\sigma^2}(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) \right].
    \] This is the \textbf{likelihood of the sample data \(\vec{y}\)}
    given the parameters \(\mu\), \(\sigma\), and the correlation
    structure defined by the parameters within \(\vec{\Psi}\) (i.e.,
    \(\vec{\theta}\) and \(\vec{p}\)).
  \end{itemize}
\end{enumerate}

In summary, the Equation~\ref{eq-likelihood-55-a} represents the
likelihood under a simplified assumption of independent errors, whereas
Equation~\ref{eq-likelihood-55} is the likelihood derived from the
assumption that the observed data comes from a \textbf{multivariate
normal distribution} where observations are correlated according to the
matrix \(\vec{\Psi}\). Equation~\ref{eq-likelihood-55}, using the sample
data vector \(\vec{y}\) and the correlation matrix \(\vec{\Psi}\),
properly accounts for the dependencies between data points inherent in
the stochastic process model. Maximizing this likelihood is how the
correlation parameters \(\vec{\theta}\) and \(\vec{p}\) are estimated in
Kriging.

\(\Box\)

\label{rem-likelihood-55}

\end{refremark}

Equation~\ref{eq-likelihood-55} can be formulated as the log-likelihood:
\begin{equation}\phantomsection\label{eq-loglikelihood-55}{
\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.
}\end{equation}

\subsection{\texorpdfstring{Differentiation with Respect to
\(\mu\)}{Differentiation with Respect to \textbackslash mu}}\label{differentiation-with-respect-to-mu}

Looking at the log-likelihood function, only the last term depends on
\(\mu\):

\[
\frac{1}{2 \sigma^2} (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu)
\]

To differentiate this with respect to the scalar \(\mu\), we can use
matrix calculus rules.

Let \(\mathbf{v} = \vec{y} - \vec{1}\mu\). \(\vec{y}\) is a constant
vector with respect to \(\mu\), and \(\vec{1}\mu\) is a vector whose
derivative with respect to the scalar \(\mu\) is \(\vec{1}\). So,
\(\frac{\partial \mathbf{v}}{\partial \mu} = -\vec{1}\).

The term is in the form \(\mathbf{v}^T \mathbf{A} \mathbf{v}\), where
\(\mathbf{A} = \vec{\Psi}^{-1}\) is a symmetric matrix. The derivative
of \(\mathbf{v}^T \mathbf{A} \mathbf{v}\) with respect to \(\mathbf{v}\)
is \(2 \mathbf{A} \mathbf{v}\) as explained in
Remark~\ref{rem-derivative-quadratic-form}.

\begin{refremark}[Derivative of a Quadratic Form]
Consider the derivative of \(\mathbf{v}^T \mathbf{A} \mathbf{v}\) with
respect to \(\mathbf{v}\):

\begin{itemize}
\tightlist
\item
  The derivative of a scalar function \(f(\mathbf{v})\) with respect to
  a vector \(\mathbf{v}\) is a vector (the gradient).
\item
  For a quadratic form \(\mathbf{v}^T \mathbf{A} \mathbf{v}\), where
  \(\mathbf{A}\) is a matrix and \(\mathbf{v}\) is a vector, the general
  formula for the derivative with respect to \(\mathbf{v}\) is
  \(\frac{\partial}{\partial \mathbf{v}} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = \mathbf{A} \mathbf{v} + \mathbf{A}^T \mathbf{v}\).
  (This is a standard result in matrix calculus and explained in
  Equation~\ref{eq-derivative-quadratic-form}).
\item
  Since \(\mathbf{A} = \vec{\Psi}^{-1}\) is a symmetric matrix, its
  transpose \(\mathbf{A}^T\) is equal to \(\mathbf{A}\).
\item
  Substituting \(\mathbf{A}^T = \mathbf{A}\) into the general derivative
  formula, we get
  \(\mathbf{A} \mathbf{v} + \mathbf{A} \mathbf{v} = 2 \mathbf{A} \mathbf{v}\).
\end{itemize}

\(\Box\)

\label{rem-derivative-quadratic-form}

\end{refremark}

Using the chain rule for differentiation with respect to the scalar
\(\mu\):
\[ \frac{\partial}{\partial \mu} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = 2 \left(\frac{\partial \mathbf{v}}{\partial \mu}\right)^T \mathbf{A} \mathbf{v} \]
Substituting \(\frac{\partial \mathbf{v}}{\partial \mu} = -\vec{1}\) and
\(\mathbf{v} = \vec{y} - \vec{1}\mu\): \[
\frac{\partial}{\partial \mu} (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = 2 (-\vec{1})^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = -2 \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu)
\]

Now, differentiate the full log-likelihood term depending on \(\mu\):

\[
\frac{\partial}{\partial \mu} \left( - \frac{1}{2 \sigma^2} (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) \right) = - \frac{1}{2 \sigma^2} \left( -2 \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) \right) = \frac{1}{\sigma^2} \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu)
\]

Setting this to zero for maximization gives:

\[
\frac{1}{\sigma^2} \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = 0.
\]

Rearranging gives: \[
\vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = 0.
\]

Solving for \(\mu\) gives: \[
\vec{1}^T \vec{\Psi}^{-1} \vec{y} = \mu \vec{1}^T \vec{\Psi}^{-1} \vec{1}.
\]

\subsection{\texorpdfstring{Differentiation with Respect to
\(\sigma\)}{Differentiation with Respect to \textbackslash sigma}}\label{differentiation-with-respect-to-sigma}

Let \(\nu = \sigma^2\) for simpler differentiation notation. The
log-likelihood becomes: \[
\ln(L) = C_1 - \frac{n}{2} \ln(\nu) - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2\nu},
\] where
\(C_1 = - \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln |\vec{\Psi}|\) is a
constant with respect to \(\nu = \sigma^2\).

We differentiate with respect to \(\nu\): \[
\frac{\partial \ln(L)}{\partial \nu} = \frac{\partial}{\partial \nu} \left( -\frac{n}{2} \ln(\nu) \right) + \frac{\partial}{\partial \nu} \left( - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2\nu} \right).
\]

The first term's derivative is straightforward: \[
\frac{\partial}{\partial \nu} \left( -\frac{n}{2} \ln(\nu) \right) = -\frac{n}{2} \cdot \frac{1}{\nu} = -\frac{n}{2\sigma^2}.
\]

For the second term, let
\(C_2 = (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)\).
This term is constant with respect to \(\sigma^2\). The derivative is:

\[
\frac{\partial}{\partial \nu} \left( - \frac{C_2}{2\nu} \right) = - \frac{C_2}{2} \frac{\partial}{\partial \nu} (\nu^{-1}) = - \frac{C_2}{2} (-\nu^{-2}) = \frac{C_2}{2\nu^2} = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2(\sigma^2)^2}.
\]

Combining the derivatives, the gradient of the log-likelihood with
respect to \(\sigma^2\) is: \[
\frac{\partial \ln(L)}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2(\sigma^2)^2}.
\]

Setting this to zero for maximization gives: \[
-\frac{n}{2\sigma^2} + \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2(\sigma^2)^2} = 0.
\]

\subsection{Results of the
Optimizations}\label{results-of-the-optimizations}

Optimization of the log-likelihood by taking derivatives with respect to
\(\mu\) and \(\sigma\) results in
\begin{equation}\phantomsection\label{eq-muhat-55}{
\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}
}\end{equation} and
\begin{equation}\phantomsection\label{eq-sigmahat-55}{
\hat{\sigma}^2 = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.
}\end{equation}

\subsection{The Concentrated Log-Likelihood
Function}\label{the-concentrated-log-likelihood-function}

Combining the equations, i.e., substituting Equation~\ref{eq-muhat-55}
and Equation~\ref{eq-sigmahat-55} into
Equation~\ref{eq-loglikelihood-55} leads to the concentrated
log-likelihood function:
\begin{equation}\phantomsection\label{eq-concentrated-loglikelihood}{
\ln(L) \approx - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|.
}\end{equation}

\begin{refremark}[The Concentrated Log-Likelihood]
\leavevmode

\begin{itemize}
\tightlist
\item
  The first term in Equation~\ref{eq-concentrated-loglikelihood}
  requires information about the measured point (observations) \(y_i\).
\item
  To maximize \(\ln(L)\), optimal values of \(\vec{\theta}\) and
  \(\vec{p}\) are determined numerically, because the function
  (Equation~\ref{eq-concentrated-loglikelihood}) is not differentiable.
\end{itemize}

\(\Box\)

\label{rem-concentrated-loglikelihood}

\end{refremark}

\subsection{\texorpdfstring{Optimizing the Parameters \(\vec{\theta}\)
and
\(\vec{p}\)}{Optimizing the Parameters \textbackslash vec\{\textbackslash theta\} and \textbackslash vec\{p\}}}\label{optimizing-the-parameters-vectheta-and-vecp}

The concentrated log-likelihood function is very quick to compute. We do
not need a statistical model, because we are only interested in the
maximum likelihood estimate (MLE) of \(\theta\) and \(p\). Optimizers
such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be
used to determine optimal values for \(\theta\) and \(p\). After the
optimization, the correlation matrix \(\Psi\) is build with the
optimized \(\theta\) and \(p\) values. This is best (most likely)
Kriging model for the given data \(y\).

Observing Figure~\ref{fig-theta12}, there's significant change between
\(\theta = 0.1\) and \(\theta = 1\), just as there is between
\(\theta = 1\) and \(\theta = 10\). Hence, it is sensible to search for
\(\theta\) on a logarithmic scale. Suitable search bounds typically
range from \(10^{-3}\) to \(10^2\), although this is not a stringent
requirement. Importantly, the scaling of the observed data does not
affect the values of \(\hat{\theta}\), but the scaling of the design
space does. Therefore, it is advisable to consistently scale variable
ranges between zero and one to ensure consistency in the degree of
activity \(\hat{\theta}_j\) represents across different problems.

\subsection{Correlation and Covariance Matrices
Revisited}\label{correlation-and-covariance-matrices-revisited}

The covariance matrix \(\Sigma\) is constructed by considering both the
variance \(\sigma^2\) and the correlation matrix \(\Psi\). They are
related as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Covariance vs.~Correlation:} Covariance is a measure of the
  joint variability of two random variables, while correlation is a
  standardized measure of this relationship, ranging from -1 to 1. The
  relationship between covariance and correlation for two random
  variables \(X\) and \(Y\) is given by
  \(\text{cor}(X, Y) = \text{cov}(X, Y) / (\sigma_X \sigma_Y)\), where
  \(\sigma_X\) and \(\sigma_Y\) are their standard deviations.
\item
  \textbf{The Covariance Matrix \(\Sigma\):} The \textbf{covariance
  matrix \(\Sigma\)} (or \(\text{Cov}(Y, Y)\) for the vector
  \(\vec{Y}\)) captures the \textbf{pairwise covariances} between all
  elements of the vector of observed responses.
\item
  \textbf{Connecting \(\sigma^2\) and \(\Psi\) to \(\Sigma\):} In the
  Kriging framework described, the variance of each observation is often
  assumed to be constant, \(\sigma^2\). The covariance between any two
  observations \(Y(\vec{x}^{(i)})\) and \(Y(\vec{x}^{(l)})\) is given by
  \(\sigma^2\) multiplied by their correlation. That is, \[
  \text{cov}(Y(\vec{x}^{(i)}), Y(\vec{x}^{(l)})) = \sigma^2 \text{cor}(Y(\vec{x}^{(i)}), Y(\vec{x}^{(l)})).
  \] This relationship holds for \emph{all} pairs of points. When
  expressed in matrix form, the covariance matrix \(\Sigma\) is the
  product of the variance \(\sigma^2\) (a scalar) and the correlation
  matrix \(\Psi\): \[
  \Sigma = \sigma^2 \Psi.
  \]
\end{enumerate}

In essence, the correlation matrix \(\Psi\) defines the \emph{structure}
or \emph{shape} of the dependencies between the data points based on
their locations. The parameter \(\sigma^2\) acts as a \textbf{scaling
factor} that converts these unitless correlation values (which are
between -1 and 1) into actual covariance values with units of variance,
setting the overall level of variability in the system.

So, \(\sigma^2\) tells us about the general spread or variability of the
underlying process, while \(\Psi\) tells you \emph{how} that variability
is distributed and how strongly points are related to each other based
on their positions. Together, they completely define the covariance
structure of your observed data in the multivariate normal distribution
used in Kriging.

\section{Implementing an MLE of the Model
Parameters}\label{implementing-an-mle-of-the-model-parameters}

The matrix algebra necessary for calculating the likelihood is the most
computationally intensive aspect of the Kriging process. It is crucial
to ensure that the code implementation is as efficient as possible.

Given that \(\Psi\) (our correlation matrix) is symmetric, only half of
the matrix needs to be computed before adding it to its transpose. When
calculating the log-likelihood, several matrix inversions are required.
The fastest approach is to conduct one Cholesky factorization and then
apply backward and forward substitution for each inverse.

The Cholesky factorization is applicable only to positive-definite
matrices, which \(\Psi\) generally is. However, if \(\Psi\) becomes
nearly singular, such as when the \(\vec{x}^{(i)}\)'s are densely
packed, the Cholesky factorization might fail. In these cases, one could
employ an LU-decomposition, though the result might be unreliable. When
\(\Psi\) is near singular, the best course of action is to either use
regression techniques or, as we do here, assign a poor likelihood value
to parameters generating the near singular matrix, thus diverting the
MLE search towards better-conditioned \(\Psi\) matrices.

When working with correlation matrices, increasing the values on the
main diagonal of a matrix will increase the absolute value of its
determinant. A critical numerical consideration in calculating the
concentrated log-likelihood is that for poorly conditioned matrices,
\(\det(\Psi)\) approaches zero, leading to potential numerical
instability. To address this issue, it is advisable to calculate
\(\ln(\lvert\Psi\rvert)\) in
Equation~\ref{eq-concentrated-loglikelihood} using twice the sum of the
logarithms of the diagonal elements of the Cholesky factorization. This
approach provides a more numerically stable method for computing the
log-determinant, as the Cholesky decomposition \(\Psi = L L^T\) allows
us to express \(\ln(\lvert\Psi\rvert) = 2\sum_{i=1}^{n} \ln(L_{ii})\),
avoiding the direct computation of potentially very small determinant
values.

\section{Kriging Prediction}\label{kriging-prediction}

We will use the Kriging correlation \(\Psi\) to predict new values based
on the observed data. The presentation follows the approach described in
(\textbf{Forr08a?}) and (\textbf{bart21i?}).

Main idea for prediction is that the new \(Y(\vec{x})\) should be
consistent with the old sample data \(X\). For a new prediction
\(\hat{y}\) at \(\vec{x}\), the value of \(\hat{y}\) is chosen so that
it maximizes the likelihood of the sample data \(X\) and the prediction,
given the (optimized) correlation parameter \(\vec{\theta}\) and
\(\vec{p}\) from above. The observed data \(\vec{y}\) is augmented with
the new prediction \(\hat{y}\) which results in the augmented vector
\(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\). A vector of correlations
between the observed data and the new prediction is defined as

\[ \vec{\psi} = \begin{pmatrix}
\text{cor}\left(
Y(\vec{x}^{(1)}),
Y(\vec{x}) 
\right) \\
\vdots  \\
\text{cor}\left(
Y(\vec{x}^{(n)}),
Y(\vec{x}) 
\right)
\end{pmatrix}
=
\begin{pmatrix}
\vec{\psi}^{(1)}\\
\vdots\\
\vec{\psi}^{(n)}
\end{pmatrix}.
\]

\begin{definition}[The Augmented Correlation
Matrix]\protect\hypertarget{def-augmented-correlation-matrix}{}\label{def-augmented-correlation-matrix}

The augmented correlation matrix is constructed as
\[ \tilde{\vec{\Psi}} =
\begin{pmatrix}
\vec{\Psi} & \vec{\psi} \\
\vec{\psi}^T & 1
\end{pmatrix}.
\]

\(\Box\)

\end{definition}

The log-likelihood of the augmented data is
\begin{equation}\phantomsection\label{eq-loglikelihood-augmented}{
\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2},
}\end{equation}

where \(\vec{1}\) is a vector of ones and \(\hat{\mu}\) and
\(\hat{\sigma}^2\) are the MLEs from Equation~\ref{eq-muhat-55} and
Equation~\ref{eq-sigmahat-55}. Only the last term in
Equation~\ref{eq-loglikelihood-augmented} depends on \(\hat{y}\), so we
need only consider this term in the maximization. Details can be found
in (\textbf{Forr08a?}). Finally, the MLE for \(\hat{y}\) can be
calculated as \begin{equation}\phantomsection\label{eq-mle-yhat}{
\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).
}\end{equation}

Equation~\ref{eq-mle-yhat} reveals two important properties of the
Kriging predictor:

\begin{itemize}
\tightlist
\item
  Basis functions: The basis function impacts the vector \(\vec{\psi}\),
  which contains the \(n\) correlations between the new point
  \(\vec{x}\) and the observed locations. Values from the \(n\) basis
  functions are added to a mean base term \(\mu\) with weightings \[
  \vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu}).
  \]
\item
  Interpolation: The predictions interpolate the sample data. When
  calculating the prediction at the \(i\)th sample point,
  \(\vec{x}^{(i)}\), the \(i\)th column of \(\vec{\Psi}^{-1}\) is
  \(\vec{\psi}\), and \(\vec{\psi}  \vec{\Psi}^{-1}\) is the \(i\)th
  unit vector. Hence,
\end{itemize}

\[
\hat{y}(\vec{x}^{(i)}) = y^{(i)}.
\]

\section{Kriging Example: Sinusoid
Function}\label{kriging-example-sinusoid-function}

Toy example in 1d where the response is a simple sinusoid measured at
eight equally spaced \(x\)-locations in the span of a single period of
oscillation.

\subsection{\texorpdfstring{Calculating the Correlation Matrix
\(\Psi\)}{Calculating the Correlation Matrix \textbackslash Psi}}\label{calculating-the-correlation-matrix-psi}

The correlation matrix \(\Psi\) is based on the pairwise squared
distances between the input locations. Here we will use \(n=8\) sample
locations and \(\theta\) is set to 1.0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{8}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(X, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]
\end{verbatim}

Evaluate at sample points

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(X)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(y, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]
\end{verbatim}

We have the data points shown in Table~\ref{tbl-sin-data}.

\begin{longtable}[]{@{}rr@{}}
\caption{Data points for the sinusoid
function}\label{tbl-sin-data}\tabularnewline
\toprule\noalign{}
\(x\) & \(y\) \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\(x\) & \(y\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.0 & 0.0 \\
0.79 & 0.71 \\
1.57 & 1.0 \\
2.36 & 0.71 \\
3.14 & 0.0 \\
3.93 & -0.71 \\
4.71 & -1.0 \\
5.5 & -0.71 \\
\end{longtable}

The data points are visualized in Figure~\ref{fig-sin-data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{)}
\NormalTok{plt.title(}\SpecialStringTok{f"Sin(x) evaluated at }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\NormalTok{plt.grid()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-sin-data-output-1.pdf}}

}

\caption{\label{fig-sin-data}Sin(x) evaluated at 8 points.}

\end{figure}%

\subsection{\texorpdfstring{Computing the \(\Psi\)
Matrix}{Computing the \textbackslash Psi Matrix}}\label{computing-the-psi-matrix}

We will use the \texttt{build\_Psi} function from
Example~\ref{exm-corr-matrix-existing} to compute the correlation matrix
\(\Psi\). \(\theta\) should be an array of one value, because we are
only working in one dimension (\(k=1\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X, theta)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(Psi, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1.   0.54 0.08 0.   0.   0.   0.   0.  ]
 [0.54 1.   0.54 0.08 0.   0.   0.   0.  ]
 [0.08 0.54 1.   0.54 0.08 0.   0.   0.  ]
 [0.   0.08 0.54 1.   0.54 0.08 0.   0.  ]
 [0.   0.   0.08 0.54 1.   0.54 0.08 0.  ]
 [0.   0.   0.   0.08 0.54 1.   0.54 0.08]
 [0.   0.   0.   0.   0.08 0.54 1.   0.54]
 [0.   0.   0.   0.   0.   0.08 0.54 1.  ]]
\end{verbatim}

Figure~\ref{fig-sin-corr} visualizes the \((8, 8)\) correlation matrix
\(\Psi\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-sin-corr-output-1.pdf}}

}

\caption{\label{fig-sin-corr}Correlation matrix \(\Psi\) for the
sinusoid function.}

\end{figure}%

\subsection{Selecting the New
Locations}\label{selecting-the-new-locations}

We would like to predict at \(m = 100\) new locations (or testign
locations) in the interval \([0, 2\pi]\). The new locations are stored
in the variable \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi, m, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Computing the \(\psi\)
Vector}{Computing the \textbackslash psi Vector}}\label{computing-the-psi-vector}

Distances between testing locations \(x\) and training data locations
\(X\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ build\_psi(X, x, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    k }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ x.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    theta }\OperatorTok{=}\NormalTok{ theta }\OperatorTok{*}\NormalTok{ ones(k)}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ zeros((n, m))}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ cdist(x.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              X.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, k),}
\NormalTok{              metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{,}
\NormalTok{              out}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{              w}\OperatorTok{=}\NormalTok{theta)    }
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}
    \CommentTok{\# return psi transpose to be consistent with the literature}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dimensions of psi: }\SpecialCharTok{\{}\NormalTok{psi}\SpecialCharTok{.}\NormalTok{T}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{(psi.T)}

\NormalTok{psi }\OperatorTok{=}\NormalTok{ build\_psi(X, x, theta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Dimensions of psi: (8, 100)
\end{verbatim}

Figure~\ref{fig-sin-corr-pred} visualizes the \((8, 100)\) prediction
matrix \(\psi\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/fig-sin-corr-pred-output-1.pdf}}

}

\caption{\label{fig-sin-corr-pred}Visualization of the predition matrix
\(\psi\)}

\end{figure}%

\subsection{Predicting at New
Locations}\label{predicting-at-new-locations}

Computation of the predictive equations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U }\OperatorTok{=}\NormalTok{ cholesky(Psi).T}
\NormalTok{one }\OperatorTok{=}\NormalTok{ np.ones(n).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ (one.T.dot(solve(U, solve(U.T, y)))) }\OperatorTok{/}\NormalTok{ one.T.dot(solve(U, solve(U.T, one)))}
\NormalTok{f }\OperatorTok{=}\NormalTok{ mu }\OperatorTok{*}\NormalTok{ ones(m).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ psi.T.dot(solve(U, solve(U.T, y }\OperatorTok{{-}}\NormalTok{ one }\OperatorTok{*}\NormalTok{ mu)))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dimensions of f: }\SpecialCharTok{\{}\NormalTok{f}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Dimensions of f: (100, 1)
\end{verbatim}

To compute \(f\), Equation~\ref{eq-mle-yhat} is used.

\subsection{Visualization}\label{visualization}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.plot(x, f, color }\OperatorTok{=} \StringTok{"orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Fitted"}\NormalTok{)}
\NormalTok{plt.plot(x, np.sin(x), color }\OperatorTok{=} \StringTok{"grey"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Original"}\NormalTok{)}
\NormalTok{plt.plot(X, y, }\StringTok{"bo"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Measurements"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Kriging prediction of sin(x) with }\SpecialCharTok{\{\}}\StringTok{ points.}\CharTok{\textbackslash{}n}\StringTok{ theta: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(n, theta[}\DecValTok{0}\NormalTok{]))}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper right\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/cell-21-output-1.pdf}}

\subsection{The Complete Python Code for the
Example}\label{sec-kriging-example-006}

Here is the self-contained Python code for direct use in a notebook:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ (array, zeros, power, ones, exp, multiply, eye, linspace, spacing, sqrt, arange, append, ravel)}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, solve}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ squareform, pdist, cdist}

\CommentTok{\# {-}{-}{-} 1. Kriging Basis Functions (Defining the Correlation) {-}{-}{-}}
\CommentTok{\# The core of Kriging uses a specialized basis function for correlation:}
\CommentTok{\# psi(x\^{}(i), x) = exp({-} sum\_\{j=1\}\^{}k theta\_j |x\_j\^{}(i) {-} x\_j|\^{}p\_j)}
\CommentTok{\# For this 1D example (k=1), and with p\_j=2 (squared Euclidean distance implicit from pdist usage)}
\CommentTok{\# and theta\_j = theta (a single value), it simplifies.}

\KeywordTok{def}\NormalTok{ build\_Psi(X, theta, eps}\OperatorTok{=}\NormalTok{sqrt(spacing(}\DecValTok{1}\NormalTok{))):}
    \CommentTok{"""}
\CommentTok{    Computes the correlation matrix Psi based on pairwise squared Euclidean distances}
\CommentTok{    between input locations, scaled by theta.}
\CommentTok{    Adds a small epsilon to the diagonal for numerical stability (nugget effect).}
\CommentTok{    """}
    \CommentTok{\# Calculate pairwise squared Euclidean distances (D) between points in X}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ squareform(pdist(X, metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{, out}\OperatorTok{=}\VariableTok{None}\NormalTok{, w}\OperatorTok{=}\NormalTok{theta))}
    \CommentTok{\# Compute Psi = exp({-}D)}
\NormalTok{    Psi }\OperatorTok{=}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}
    \CommentTok{\# Add a small value to the diagonal for numerical stability (nugget)}
    \CommentTok{\# This is often done in Kriging implementations, though a regression method}
    \CommentTok{\# with a \textquotesingle{}nugget\textquotesingle{} parameter (Lambda) is explicitly mentioned for noisy data later.}
    \CommentTok{\# The source code snippet for build\_Psi explicitly includes \textasciigrave{}multiply(eye(X.shape), eps)\textasciigrave{}.}
    \CommentTok{\# FIX: Use X.shape to get the number of rows for the identity matrix}
\NormalTok{    Psi }\OperatorTok{+=}\NormalTok{ multiply(eye(X.shape[}\DecValTok{0}\NormalTok{]), eps) }\CommentTok{\# Corrected line}
    \ControlFlowTok{return}\NormalTok{ Psi}

\KeywordTok{def}\NormalTok{ build\_psi(X\_train, x\_predict, theta):}
    \CommentTok{"""}
\CommentTok{    Computes the correlation vector (or matrix) psi between new prediction locations}
\CommentTok{    and training data locations.}
\CommentTok{    """}
    \CommentTok{\# Calculate pairwise squared Euclidean distances (D) between prediction points (x\_predict)}
    \CommentTok{\# and training points (X\_train).}
    \CommentTok{\# \textasciigrave{}cdist\textasciigrave{} computes distances between each pair of the two collections of inputs.}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ cdist(x\_predict, X\_train, metric}\OperatorTok{=}\StringTok{\textquotesingle{}sqeuclidean\textquotesingle{}}\NormalTok{, out}\OperatorTok{=}\VariableTok{None}\NormalTok{, w}\OperatorTok{=}\NormalTok{theta)}
    \CommentTok{\# Compute psi = exp({-}D)}
\NormalTok{    psi }\OperatorTok{=}\NormalTok{ exp(}\OperatorTok{{-}}\NormalTok{D)}
    \ControlFlowTok{return}\NormalTok{ psi.T }\CommentTok{\# Return transpose to be consistent with literature (n x m or n x 1)}

\CommentTok{\# {-}{-}{-} 2. Data Points for the Sinusoid Function Example {-}{-}{-}}
\CommentTok{\# The example uses a 1D sinusoid measured at eight equally spaced x{-}locations [153, Table 9.1].}
\NormalTok{n }\OperatorTok{=} \DecValTok{8} \CommentTok{\# Number of sample locations}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi, n, endpoint}\OperatorTok{=}\VariableTok{False}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# Generate x{-}locations}
\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ np.sin(X\_train) }\CommentTok{\# Corresponding y{-}values (sine of x)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-} Training Data (X\_train, y\_train) {-}{-}{-}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x values:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(X\_train, }\DecValTok{2}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"y values:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(y\_train, }\DecValTok{2}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{40}\NormalTok{)}

\CommentTok{\# Visualize the data points}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{plt.plot(X\_train, y\_train, }\StringTok{"bo"}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f"Measurements (}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ points)"}\NormalTok{)}
\NormalTok{plt.title(}\SpecialStringTok{f"Sin(x) evaluated at }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"x"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"sin(x)"}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}

\CommentTok{\# {-}{-}{-} 3. Calculating the Correlation Matrix (Psi) {-}{-}{-}}
\CommentTok{\# Psi is based on pairwise squared distances between input locations.}
\CommentTok{\# theta is set to 1.0 for this 1D example.}
\NormalTok{theta }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ build\_Psi(X\_train, theta)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{{-}{-}{-} Computed Correlation Matrix (Psi) {-}{-}{-}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dimensions of Psi:"}\NormalTok{, Psi.shape) }\CommentTok{\# Should be (8, 8)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"First 5x5 block of Psi:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(Psi[:}\DecValTok{5}\NormalTok{,:}\DecValTok{5}\NormalTok{], }\DecValTok{2}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{40}\NormalTok{)}

\CommentTok{\# {-}{-}{-} 4. Selecting New Locations (for Prediction) {-}{-}{-}}
\CommentTok{\# We want to predict at m = 100 new locations in the interval [0, 2*pi].}
\NormalTok{m }\OperatorTok{=} \DecValTok{100} \CommentTok{\# Number of new locations}
\NormalTok{x\_predict }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi, m, endpoint}\OperatorTok{=}\VariableTok{True}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{{-}{-}{-} New Locations for Prediction (x\_predict) {-}{-}{-}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of prediction points: }\SpecialCharTok{\{}\NormalTok{m}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"First 5 prediction points:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(x\_predict[:}\DecValTok{5}\NormalTok{], }\DecValTok{2}\NormalTok{).flatten())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{40}\NormalTok{)}

\CommentTok{\# {-}{-}{-} 5. Computing the psi Vector {-}{-}{-}}
\CommentTok{\# This vector contains correlations between each of the n observed data points}
\CommentTok{\# and each of the m new prediction locations.}
\NormalTok{psi }\OperatorTok{=}\NormalTok{ build\_psi(X\_train, x\_predict, theta)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{{-}{-}{-} Computed Prediction Correlation Matrix (psi) {-}{-}{-}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dimensions of psi:"}\NormalTok{, psi.shape) }\CommentTok{\# Should be (8, 100)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"First 5x5 block of psi:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(psi[:}\DecValTok{5}\NormalTok{,:}\DecValTok{5}\NormalTok{], }\DecValTok{2}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{40}\NormalTok{)}

\CommentTok{\# {-}{-}{-} 6. Predicting at New Locations (Kriging Prediction) {-}{-}{-}}
\CommentTok{\# The Maximum Likelihood Estimate (MLE) for y\_hat is calculated using the formula:}
\CommentTok{\# y\_hat(x) = mu\_hat + psi.T @ Psi\_inv @ (y {-} 1 * mu\_hat) [p. 2 of previous response, and 263]}
\CommentTok{\# Matrix inversion is efficiently performed using Cholesky factorization.}

\CommentTok{\# Step 6a: Cholesky decomposition of Psi}
\NormalTok{U }\OperatorTok{=}\NormalTok{ cholesky(Psi).T }\CommentTok{\# Note: \textasciigrave{}cholesky\textasciigrave{} in numpy returns lower triangular L, we need U (upper) so transpose L.}

\CommentTok{\# Step 6b: Calculate mu\_hat (estimated mean)}
\CommentTok{\# mu\_hat = (one.T @ Psi\_inv @ y) / (one.T @ Psi\_inv @ one) [p. 2 of previous response]}
\NormalTok{one }\OperatorTok{=}\NormalTok{ np.ones(n).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# Vector of ones}
\NormalTok{mu\_hat }\OperatorTok{=}\NormalTok{ (one.T }\OperatorTok{@}\NormalTok{ solve(U, solve(U.T, y\_train))) }\OperatorTok{/}\NormalTok{ (one.T }\OperatorTok{@}\NormalTok{ solve(U, solve(U.T, one)))}
\NormalTok{mu\_hat }\OperatorTok{=}\NormalTok{ mu\_hat.item() }\CommentTok{\# Extract scalar value}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{{-}{-}{-} Kriging Prediction Calculation {-}{-}{-}"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Estimated mean (mu\_hat): }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\BuiltInTok{round}\NormalTok{(mu\_hat, }\DecValTok{4}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Step 6c: Calculate predictions f (y\_hat) at new locations}
\CommentTok{\# f = mu\_hat * ones(m) + psi.T @ Psi\_inv @ (y {-} one * mu\_hat)}
\NormalTok{f\_predict }\OperatorTok{=}\NormalTok{ mu\_hat }\OperatorTok{*}\NormalTok{ np.ones(m).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ psi.T }\OperatorTok{@}\NormalTok{ solve(U, solve(U.T, y\_train }\OperatorTok{{-}}\NormalTok{ one }\OperatorTok{*}\NormalTok{ mu\_hat))}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dimensions of predicted values (f\_predict): }\SpecialCharTok{\{}\NormalTok{f\_predict}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{) }\CommentTok{\# Should be (100, 1)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"First 5 predicted f values:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(f\_predict[:}\DecValTok{5}\NormalTok{], }\DecValTok{2}\NormalTok{).flatten())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{40}\NormalTok{)}

\CommentTok{\# {-}{-}{-} 7. Visualization {-}{-}{-}}
\CommentTok{\# Plot the original sinusoid function, the measured points, and the Kriging predictions.}

\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plt.plot(x\_predict, f\_predict, color}\OperatorTok{=}\StringTok{"orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Kriging Prediction"}\NormalTok{)}
\NormalTok{plt.plot(x\_predict, np.sin(x\_predict), color}\OperatorTok{=}\StringTok{"grey"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"True Sinusoid Function"}\NormalTok{)}
\NormalTok{plt.plot(X\_train, y\_train, }\StringTok{"bo"}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{8}\NormalTok{, label}\OperatorTok{=}\StringTok{"Measurements"}\NormalTok{)}
\NormalTok{plt.title(}\SpecialStringTok{f"Kriging prediction of sin(x) with }\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ points. (theta: }\SpecialCharTok{\{}\NormalTok{theta}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"x"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"y"}\NormalTok{)}
\NormalTok{plt.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper right\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
--- Training Data (X_train, y_train) ---
x values:
 [[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]
y values:
 [[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]
----------------------------------------
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/cell-22-output-2.pdf}}

\begin{verbatim}

--- Computed Correlation Matrix (Psi) ---
Dimensions of Psi: (8, 8)
First 5x5 block of Psi:
 [[1.   0.54 0.08 0.   0.  ]
 [0.54 1.   0.54 0.08 0.  ]
 [0.08 0.54 1.   0.54 0.08]
 [0.   0.08 0.54 1.   0.54]
 [0.   0.   0.08 0.54 1.  ]]
----------------------------------------

--- New Locations for Prediction (x_predict) ---
Number of prediction points: 100
First 5 prediction points:
 [0.   0.06 0.13 0.19 0.25]
----------------------------------------

--- Computed Prediction Correlation Matrix (psi) ---
Dimensions of psi: (8, 100)
First 5x5 block of psi:
 [[1.   1.   0.98 0.96 0.94]
 [0.54 0.59 0.65 0.7  0.75]
 [0.08 0.1  0.12 0.15 0.18]
 [0.   0.01 0.01 0.01 0.01]
 [0.   0.   0.   0.   0.  ]]
----------------------------------------

--- Kriging Prediction Calculation ---
Estimated mean (mu_hat): -0.0499
Dimensions of predicted values (f_predict): (100, 1)
First 5 predicted f values:
 [0.   0.05 0.1  0.15 0.21]
----------------------------------------
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{006_num_gp_files/figure-pdf/cell-22-output-4.pdf}}

\section{Jupyter Notebook}\label{jupyter-notebook-6}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Matrices}\label{matrices}

\section{Derivatives of Quadratic
Forms}\label{sec-derivative-quadratic-form}

We present a step-by-step derivation of the general formula
\begin{equation}\phantomsection\label{eq-derivative-quadratic-form}{
\frac{\partial}{\partial \mathbf{v}} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = \mathbf{A} \mathbf{v} + \mathbf{A}^T \mathbf{v}.
}\end{equation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define the components. Let \(\mathbf{v}\) be a vector of size
  \(n \times 1\), and let \(\mathbf{A}\) be a matrix of size
  \(n \times n\).
\item
  Write out the quadratic form in summation notation. The product
  \(\mathbf{v}^T \mathbf{A} \mathbf{v}\) is a scalar. It can be expanded
  and be rewritten as a double summation: \[
  \mathbf{v}^T \mathbf{A} \mathbf{v} = \sum_{i=1}^n \sum_{j=1}^n v_i a_{ij} v_j.
  \]
\item
  Calculate the partial derivative with respect to a component \(v_k\):
  The derivative of the scalar \(\mathbf{v}^T \mathbf{A} \mathbf{v}\)
  with respect to the vector \(\mathbf{v}\) is the gradient vector,
  whose \(k\)-th component is
  \(\frac{\partial}{\partial v_k} (\mathbf{v}^T \mathbf{A} \mathbf{v})\).
  We need to find
  \(\frac{\partial}{\partial v_k} \left( \sum_{i=1}^n \sum_{j=1}^n v_i a_{ij} v_j \right)\).
  Consider the terms in the summation that involve \(v_k\). A term
  \(v_i a_{ij} v_j\) involves \(v_k\) if \(i=k\) or \(j=k\) (or both).

  \begin{itemize}
  \tightlist
  \item
    Terms where \(i=k\): \(v_k a_{kj} v_j\). The derivative with respect
    to \(v_k\) is \(a_{kj} v_j\).
  \item
    Terms where \(j=k\): \(v_i a_{ik} v_k\). The derivative with respect
    to \(v_k\) is \(v_i a_{ik}\).
  \item
    The term where \(i=k\) and \(j=k\):
    \(v_k a_{kk} v_k = a_{kk} v_k^2\). Its derivative with respect to
    \(v_k\) is \(2 a_{kk} v_k\). Notice this term is included in both
    cases above when \(i=k\) and \(j=k\). When \(i=k\), the term is
    \(v_k a_{kk} v_k\), derivative is \(a_{kk} v_k\). When \(j=k\), the
    term is \(v_k a_{kk} v_k\), derivative is \(v_k a_{kk}\). Summing
    these two gives \(2 a_{kk} v_k\).
  \end{itemize}
\item
  Let's differentiate the sum
  \(\sum_{i=1}^n \sum_{j=1}^n v_i a_{ij} v_j\) with respect to \(v_k\):
  \[
  \frac{\partial}{\partial v_k} \left( \sum_{i=1}^n \sum_{j=1}^n v_i a_{ij} v_j \right) = \sum_{i=1}^n \sum_{j=1}^n \frac{\partial}{\partial v_k} (v_i a_{ij} v_j).
  \]
\item
  The partial derivative
  \(\frac{\partial}{\partial v_k} (v_i a_{ij} v_j)\) is non-zero only if
  \(i=k\) or \(j=k\).

  \begin{itemize}
  \tightlist
  \item
    If \(i=k\) and \(j \ne k\):
    \(\frac{\partial}{\partial v_k} (v_k a_{kj} v_j) = a_{kj} v_j\).
  \item
    If \(i \ne k\) and \(j = k\):
    \(\frac{\partial}{\partial v_k} (v_i a_{ik} v_k) = v_i a_{ik}\).
  \item
    If \(i=k\) and \(j=k\):
    \(\frac{\partial}{\partial v_k} (v_k a_{kk} v_k) = \frac{\partial}{\partial v_k} (a_{kk} v_k^2) = 2 a_{kk} v_k\).
  \end{itemize}
\item
  So, the partial derivative is the sum of derivatives of all terms
  involving \(v_k\):
  \(\frac{\partial}{\partial v_k} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = \sum_{j \ne k} (a_{kj} v_j) + \sum_{i \ne k} (v_i a_{ik}) + (2 a_{kk} v_k)\).
\item
  We can rewrite this by including the \(i=k, j=k\) term back into the
  summations:
  \(\sum_{j \ne k} (a_{kj} v_j) + a_{kk} v_k + \sum_{i \ne k} (v_i a_{ik}) + v_k a_{kk}\)
  (since \(v_k a_{kk} = a_{kk} v_k\))
  \(= \sum_{j=1}^n a_{kj} v_j + \sum_{i=1}^n v_i a_{ik}\).
\item
  Convert back to matrix/vector notation: The first summation
  \(\sum_{j=1}^n a_{kj} v_j\) is the \(k\)-th component of the
  matrix-vector product \(\mathbf{A} \mathbf{v}\).The second summation
  \(\sum_{i=1}^n v_i a_{ik}\) can be written as
  \(\sum_{i=1}^n a_{ik} v_i\). Recall that the element in the \(k\)-th
  row and \(i\)-th column of the transpose matrix \(\mathbf{A}^T\) is
  \((A^T)_{ki} = a_{ik}\). So,
  \(\sum_{i=1}^n a_{ik} v_i = \sum_{i=1}^n (A^T)_{ki} v_i\), which is
  the \(k\)-th component of the matrix-vector product
  \(\mathbf{A}^T \mathbf{v}\).
\item
  Assemble the gradient vector: The \(k\)-th component of the gradient
  \(\frac{\partial}{\partial \mathbf{v}} (\mathbf{v}^T \mathbf{A} \mathbf{v})\)
  is \((\mathbf{A} \mathbf{v})_k + (\mathbf{A}^T \mathbf{v})_k\). Since
  this holds for all \(k = 1, \dots, n\), the gradient vector is the sum
  of the two vectors \(\mathbf{A} \mathbf{v}\) and
  \(\mathbf{A}^T \mathbf{v}\). Therefore, the general formula for the
  derivative is
  \(\frac{\partial}{\partial \mathbf{v}} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = \mathbf{A} \mathbf{v} + \mathbf{A}^T \mathbf{v}\).
\end{enumerate}

\section{The Condition Number}\label{sec-conditon-number}

A small value, \texttt{eps}, can be passed to the function
\texttt{build\_Psi} to improve the condition number. For example,
\texttt{eps=sqrt(spacing(1))} can be used. The numpy function
\texttt{spacing()} returns the distance between a number and its nearest
adjacent number.

The condition number of a matrix is a measure of its sensitivity to
small changes in its elements. It is used to estimate how much the
output of a function will change if the input is slightly altered.

A matrix with a low condition number is well-conditioned, which means
its behavior is relatively stable, while a matrix with a high condition
number is ill-conditioned, meaning its behavior is unstable with respect
to numerical precision.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a well{-}conditioned matrix (low condition number)}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{], [}\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number of A: "}\NormalTok{, np.linalg.cond(A))}

\CommentTok{\# Define an ill{-}conditioned matrix (high condition number)}
\NormalTok{B }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\FloatTok{0.99999999}\NormalTok{], [}\FloatTok{0.99999999}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Condition number of B: "}\NormalTok{, np.linalg.cond(B))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Condition number of A:  1.2222222222222225
Condition number of B:  200000000.57495335
\end{verbatim}

\section{The Moore-Penrose
Pseudoinverse}\label{sec-matrix-pseudoinverse}

\subsection{Definitions}\label{definitions}

The Moore-Penrose pseudoinverse is a generalization of the inverse
matrix for non-square or singular matrices. It is computed as

\[
A^+ = (A^* A)^{-1} A^*,
\] where \(A^*\) is the conjugate transpose of \(A\).

It satisfies the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(AA^+A = A\)
\item
  \(A^+AA^+ = A^+\)
\item
  \((AA^+)^* = AA^+\).
\item
  \((A^+A)^* = A^+A\)
\item
  \(A^+ = (A^*)^+\)
\item
  \(A^+ = A^T\) if \(A\) is a square matrix and \(A\) is invertible.
\end{enumerate}

The pseudoinverse can be computed using Singular Value Decomposition
(SVD).

\subsection{Implementation in Python}\label{implementation-in-python}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ pinv}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], [}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Matrix A:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{A}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{A\_pseudo\_inv }\OperatorTok{=}\NormalTok{ pinv(A)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Moore{-}Penrose Pseudoinverse:}\CharTok{\textbackslash{}n}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{A\_pseudo\_inv}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Matrix A:
 [[1 2]
 [3 4]
 [5 6]]
Moore-Penrose Pseudoinverse:
 [[-1.33333333 -0.33333333  0.66666667]
 [ 1.08333333  0.33333333 -0.41666667]]
\end{verbatim}

\section{Strictly Positive Definite
Kernels}\label{sec-strictly-positive-definite}

\subsection{Definition}\label{definition}

\begin{definition}[Strictly Positive Definite
Kernel]\protect\hypertarget{def-strictly-positive-definite}{}\label{def-strictly-positive-definite}

A kernel function \(k(x,y)\) is called strictly positive definite if for
any finite collection of distinct points \({x_1, x_2, \ldots, x_n}\) in
the input space and any non-zero vector of coefficients
\(\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)\), the following
inequality holds:

\begin{equation}\phantomsection\label{eq-strictly-positive-definite}{
\sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j k(x_i, x_j) > 0.
}\end{equation}

\end{definition}

In contrast, a kernel function \(k(x,y)\) is called positive definite
(but not strictly) if the ``\(>\)'' sign is replaced by ``\(\geq\)'' in
the above inequality.

\subsection{Connection to Positive Definite
Matrices}\label{connection-to-positive-definite-matrices}

The connection between strictly positive definite kernels and positive
definite matrices lies in the Gram matrix construction:

\begin{itemize}
\tightlist
\item
  When we evaluate a kernel function \(k(x,y)\) at all pairs of data
  points in our sample, we construct the Gram matrix \(K\) where
  \(K_{ij} = k(x_i, x_j)\).
\item
  If the kernel function \(k\) is strictly positive definite, then for
  any set of distinct points, the resulting Gram matrix will be
  symmetric positive definite.
\end{itemize}

A symmetric matrix is positive definite if and only if for any non-zero
vector \(\alpha\), the quadratic form \(\alpha^T K \alpha > 0\), which
directly corresponds to the kernel definition above.

\subsection{Connection to RBF Models}\label{connection-to-rbf-models}

For RBF models, the kernel function is the radial basis function itself:
\[
k(x,y) = \psi(||x-y||).
\]

The Gaussian RBF kernel \(\psi(r) = e^{-r^2/(2\sigma^2)}\) is strictly
positive definite in \(\mathbb{R}^n\) for any dimension \(n\). The
inverse multiquadric kernel \(\psi(r) = (r^2 + \sigma^2)^{-1/2}\) is
also strictly positive definite in any dimension.

This mathematical property guarantees that the interpolation problem has
a unique solution (the weight vector \(\vec{w}\) is uniquely
determined). The linear system \(\Psi \vec{w} = \vec{y}\) can be solved
reliably using Cholesky decomposition. The RBF interpolant exists and is
unique for any distinct set of centers.

\section{Cholesky Decomposition and Positive Definite
Matrices}\label{cholesky-decomposition-and-positive-definite-matrices}

We consider the definiteness of a matrix, before discussing the Cholesky
decomposition.

\begin{definition}[Positive Definite
Matrix]\protect\hypertarget{def-positive-definite}{}\label{def-positive-definite}

A symmetric matrix \(A\) is positive definite if all its eigenvalues are
positive.

\end{definition}

\begin{example}[Positive Definite
Matrix]\protect\hypertarget{exm-positive-definite}{}\label{exm-positive-definite}

Given a symmetric matrix
\(A = \begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix}\), the eigenvalues of
\(A\) are \(\lambda_1 = 13\) and \(\lambda_2 = 5\). Since both
eigenvalues are positive, the matrix \(A\) is positive definite.

\end{example}

\begin{definition}[Negative Definite, Positive Semidefinite, and
Negative Semidefinite
Matrices]\protect\hypertarget{def-negative-definite}{}\label{def-negative-definite}

Similarily, a symmetric matrix \(A\) is negative definite if all its
eigenvalues are negative. It is positive semidefinite if all its
eigenvalues are non-negative, and negative semidefinite if all its
eigenvalues are non-positive.

\end{definition}

The covariance matrix must be positive definite for a multivariate
normal distribution for a couple of reasons:

\begin{itemize}
\tightlist
\item
  Semidefinite vs Definite: A covariance matrix is always symmetric and
  positive semidefinite. However, for a multivariate normal
  distribution, it must be positive definite, not just semidefinite.
  This is because a positive semidefinite matrix can have zero
  eigenvalues, which would imply that some dimensions in the
  distribution have zero variance, collapsing the distribution in those
  dimensions. A positive definite matrix has all positive eigenvalues,
  ensuring that the distribution has positive variance in all
  dimensions.
\item
  Invertibility: The multivariate normal distribution's probability
  density function involves the inverse of the covariance matrix. If the
  covariance matrix is not positive definite, it may not be invertible,
  and the density function would be undefined.
\end{itemize}

In summary, the covariance matrix being positive definite ensures that
the multivariate normal distribution is well-defined and has positive
variance in all dimensions.

The definiteness of a matrix can be checked by examining the eigenvalues
of the matrix. If all eigenvalues are positive, the matrix is positive
definite.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ is\_positive\_definite(matrix):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{all}\NormalTok{(np.linalg.eigvals(matrix) }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(is\_positive\_definite(matrix))  }\CommentTok{\# Outputs: True}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

However, a more efficient way to check the definiteness of a matrix is
through the Cholesky decomposition.

\begin{definition}[Cholesky
Decomposition]\protect\hypertarget{def-cholesky-decomposition}{}\label{def-cholesky-decomposition}

For a given symmetric positive-definite matrix
\(A \in \mathbb{R}^{n \times n}\), there exists a unique lower
triangular matrix \(L \in \mathbb{R}^{n \times n}\) with positive
diagonal elements such that:

\[
A = L L^T.
\]

Here, \(L^T\) denotes the transpose of \(L\).

\end{definition}

\begin{example}[Cholesky decomposition using
\texttt{numpy}]\protect\hypertarget{exm-cholesky-decomposition}{}\label{exm-cholesky-decomposition}

\texttt{linalg.cholesky} computes the Cholesky decomposition of a
matrix, i.e., it computes a lower triangular matrix \(L\) such that
\(LL^T = A\). If the matrix is not positive definite, an error
(\texttt{LinAlgError}) is raised.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define a Hermitian, positive{-}definite matrix}
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]]) }

\CommentTok{\# Compute the Cholesky decomposition}
\NormalTok{L }\OperatorTok{=}\NormalTok{ np.linalg.cholesky(A)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"L = }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, L)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L*LT = }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, np.dot(L, L.T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
L = 
 [[3.         0.        ]
 [1.33333333 2.68741925]]
L*LT = 
 [[9. 4.]
 [4. 9.]]
\end{verbatim}

\end{example}

\begin{example}[Cholesky
Decomposition]\protect\hypertarget{exm-cholesky-decomposition}{}\label{exm-cholesky-decomposition}

Given a symmetric positive-definite matrix
\(A = \begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix}\), the Cholesky
decomposition computes the lower triangular matrix \(L\) such that
\(A = L L^T\). The matrix \(L\) is computed as: \[
L = \begin{pmatrix} 3 & 0 \\ 4/3 & 2 \end{pmatrix},
\] so that \[
L L^T = \begin{pmatrix} 3 & 0 \\ 4/3 & \sqrt{65}/3 \end{pmatrix} \begin{pmatrix} 3 & 4/3 \\ 0 & \sqrt{65}/3 \end{pmatrix} = \begin{pmatrix} 9 & 4 \\ 4 & 9 \end{pmatrix} = A.
\]

\end{example}

An efficient implementation of the definiteness-check based on Cholesky
is already available in the \texttt{numpy} library. It provides the
\texttt{np.linalg.cholesky} function to compute the Cholesky
decomposition of a matrix. This more efficient \texttt{numpy}-approach
can be used as follows:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ is\_pd(K):}
    \ControlFlowTok{try}\NormalTok{:}
\NormalTok{        np.linalg.cholesky(K)}
        \ControlFlowTok{return} \VariableTok{True}
    \ControlFlowTok{except}\NormalTok{ np.linalg.linalg.LinAlgError }\ImportTok{as}\NormalTok{ err:}
        \ControlFlowTok{if} \StringTok{\textquotesingle{}Matrix is not positive definite\textquotesingle{}} \KeywordTok{in}\NormalTok{ err.message:}
            \ControlFlowTok{return} \VariableTok{False}
        \ControlFlowTok{else}\NormalTok{:}
            \ControlFlowTok{raise}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{9}\NormalTok{, }\DecValTok{4}\NormalTok{], [}\DecValTok{4}\NormalTok{, }\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(is\_pd(matrix))  }\CommentTok{\# Outputs: True}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\subsection{Example of Cholesky
Decomposition}\label{example-of-cholesky-decomposition}

We consider dimension \(k=1\) and \(n=2\) sample points. The sample
points are located at \(x_1=1\) and \(x_2=5\). The response values are
\(y_1=2\) and \(y_2=10\). The correlation parameter is \(\theta=1\) and
\(p\) is set to \(1\). Using Equation~\ref{eq-krigingbase}, we can
compute the correlation matrix \(\Psi\):

\[
\Psi = \begin{pmatrix}
1 & e^{-1}\\
e^{-1} & 1
\end{pmatrix}.
\]

To determine MLE as in Equation~\ref{eq-mle-yhat}, we need to compute
\(\Psi^{-1}\):

\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e & -1\\
-1 & e
\end{pmatrix}.
\]

Cholesky-decomposition of \(\Psi\) is recommended to compute
\(\Psi^{-1}\). Cholesky decomposition is a decomposition of a positive
definite symmetric matrix into the product of a lower triangular matrix
\(L\), a diagonal matrix \(D\) and the transpose of \(L\), which is
denoted as \(L^T\). Consider the following example:

\[
LDL^T=
\begin{pmatrix}
1 & 0 \\
l_{21} & 1
\end{pmatrix}
\begin{pmatrix}
d_{11} & 0 \\
0 & d_{22}
\end{pmatrix}
\begin{pmatrix}
1 & l_{21} \\
0 & 1
\end{pmatrix}=
\]

\begin{equation}\phantomsection\label{eq-cholex}{
\begin{pmatrix}
d_{11} & 0 \\
d_{11} l_{21} & d_{22}
\end{pmatrix}
\begin{pmatrix}
1 & l_{21} \\
0 & 1
\end{pmatrix}
=
\begin{pmatrix}
d_{11} & d_{11} l_{21} \\
d_{11} l_{21} & d_{11} l_{21}^2 + d_{22}
\end{pmatrix}.
}\end{equation}

Using Equation~\ref{eq-cholex}, we can compute the Cholesky
decomposition of \(\Psi\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(d_{11} = 1\),
\item
  \(l_{21}d_{11} = e^{-1} \Rightarrow l_{21} = e^{-1}\), and
\item
  \(d_{11} l_{21}^2 + d_{22} = 1 \Rightarrow d_{22} = 1 - e^{-2}\).
\end{enumerate}

The Cholesky decomposition of \(\Psi\) is \[
\Psi = \begin{pmatrix}
1 & 0\\
e^{-1} & 1\\
\end{pmatrix}
\begin{pmatrix}
1 & 0\\
0 & 1 - e^{-2}\\
\end{pmatrix}
\begin{pmatrix}
1 & e^{-1}\\
0 & 1\\
\end{pmatrix}
= LDL^T\]

Some programs use \(U\) instead of \(L\). The Cholesky decomposition of
\(\Psi\) is \[
\Psi = LDL^T = U^TDU.
\]

Using \[
\sqrt{D} =\begin{pmatrix}
1 & 0\\
0 & \sqrt{1 - e^{-2}}\\
\end{pmatrix},
\] we can write the Cholesky decomposition of \(\Psi\) without a
diagonal matrix \(D\) as \[
\Psi = \begin{pmatrix}
1 & 0\\
e^{-1} & \sqrt{1 - e^{-2}}\\
\end{pmatrix}
\begin{pmatrix}
1 & e^{-1}\\
0 & \sqrt{1 - e^{-2}}\\
\end{pmatrix}
= U^TU.
\]

\subsection{Inverse Matrix Using Cholesky
Decomposition}\label{inverse-matrix-using-cholesky-decomposition}

To compute the inverse of a matrix using the Cholesky decomposition, you
can follow these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decompose the matrix \(A\) into \(L\) and \(L^T\), where \(L\) is a
  lower triangular matrix and \(L^T\) is the transpose of \(L\).
\item
  Compute \(L^{-1}\), the inverse of \(L\).
\item
  The inverse of \(A\) is then \((L^{-1})^T  L^-1\).
\end{enumerate}

Please note that this method only applies to symmetric,
positive-definite matrices.

The inverse of the matrix \(\Psi\) from above is:

\[
\Psi^{-1} = \frac{e}{e^2 -1} \begin{pmatrix}
e & -1\\
-1 & e
\end{pmatrix}.
\]

Here's an example of how to compute the inverse of a matrix using
Cholesky decomposition in Python:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, inv}
\NormalTok{E }\OperatorTok{=}\NormalTok{ np.exp(}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Psi is a symmetric, positive{-}definite matrix }
\NormalTok{Psi }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\NormalTok{E], [}\DecValTok{1}\OperatorTok{/}\NormalTok{E, }\DecValTok{1}\NormalTok{]])}
\NormalTok{L }\OperatorTok{=}\NormalTok{ cholesky(Psi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{L\_inv }\OperatorTok{=}\NormalTok{ inv(L)}
\CommentTok{\# The inverse of A is (L\^{}{-}1)\^{}T * L\^{}{-}1}
\NormalTok{Psi\_inv }\OperatorTok{=}\NormalTok{ np.dot(L\_inv.T, L\_inv)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Psi:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Psi)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Psi Inverse:}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, Psi\_inv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Psi:
 [[1.         0.36787944]
 [0.36787944 1.        ]]
Psi Inverse:
 [[ 1.15651764 -0.42545906]
 [-0.42545906  1.15651764]]
\end{verbatim}

\section{Nystrm Approximation}\label{nystruxf6m-approximation}

\subsection{What's the Big Idea?}\label{whats-the-big-idea}

Imagine you have a huge, detailed map of a country. Working with the
full, high-resolution map is slow and takes up a lot of computer memory.
The Nystrm method is like creating a smaller-scale summary map by only
looking at a few key, representative locations.

In machine learning, we often work with a \textbf{kernel matrix} (or
Gram matrix), which tells us how similar every pair of data points is to
each other. For very large datasets, this matrix can become massive,
making it computationally expensive to store and process.

The Nystrm method provides an efficient way to create a
\textbf{low-rank approximation} of this large kernel matrix. In simple
terms, it finds a ``simpler'' version of the matrix that captures its
most important properties without needing to compute or store the whole
thing.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{How Does It Work?}\label{how-does-it-work}

The core idea is to select a small, random subset of the columns of the
full kernel matrix and use them to reconstruct the entire matrix. Let's
say our full kernel matrix is \(K\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sample:} Randomly select \(l\) columns from the \(n\) total
  columns of \(K\). Let \(C\) be the \(n \times l\) matrix of these
  sampled columns.
\item
  \textbf{Intersect:} Take the rows of \(C\) corresponding to the
  sampled column indices to form the \(l \times l\) matrix \(W\).
\item
  \textbf{Approximate:} Using \(C\) and \(W\), calculate the Nystrm
  approximation \(\tilde{K}\) of \(K\): \[
   \tilde{K} \approx C W^{+} C^T
   \] where \(W^{+}\) is the pseudoinverse of \(W\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Example}\label{example}

Suppose we have 4 data points and the full kernel matrix \(K\) is: \[
K = \begin{pmatrix}
9 & 6 & 3 & 1 \\
6 & 4 & 2 & 0.5 \\
3 & 2 & 1 & 0.25 \\
1 & 0.5 & 0.25 & 0.1
\end{pmatrix}
\]

Let's approximate it by sampling 2 columns (\(l=2\)):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sample:} Pick the 1st and 3rd columns: \[
   C = \begin{pmatrix}
   9 & 3 \\
   6 & 2 \\
   3 & 1 \\
   1 & 0.25
   \end{pmatrix}
   \]
\item
  \textbf{Intersect:} Take the 1st and 3rd rows from \(C\) to form
  \(W\): \[
   W = \begin{pmatrix}
   9 & 3 \\
   3 & 1
   \end{pmatrix}
   \]
\item
  \textbf{Approximate:} Suppose the pseudoinverse of \(W\) is: \[
   W^{+} = \begin{pmatrix}
   0.09 & -0.27 \\
   -0.27 & 0.81
   \end{pmatrix}
   \] Then, \[
   \tilde{K} = C W^{+} C^T = \begin{pmatrix}
   9 & 6 & 3 & 0.675 \\
   6 & 4 & 2 & 0.45 \\
   3 & 2 & 1 & 0.225 \\
   0.675 & 0.45 & 0.225 & 0.05
   \end{pmatrix}
   \]
\end{enumerate}

\(\tilde{K}\) is a good approximation of the original \(K\), especially
in the top-left portion.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Why Is This Useful?}\label{why-is-this-useful}

\begin{itemize}
\tightlist
\item
  \textbf{Speed:} The Nystrm method is much faster than computing the
  full kernel matrix. The complexity is roughly \(O(l^2 n)\) instead of
  \(O(n^2 d)\) (where \(d\) is the number of features).
\item
  \textbf{Scalability:} It allows kernel methods (like SVM or Kernel
  PCA) to be used on much larger datasets.
\item
  \textbf{Feature Mapping:} The method can be used to project new data
  points into the same feature space for prediction tasks.
\end{itemize}

The quality of the approximation depends on the columns you sample.
Uniform random sampling is common and often effective, but more advanced
techniques exist to select more informative columns.

\subsection{Applying the Nystrm Approximation: How Nystrm
Approximation Helps
Kriging}\label{applying-the-nystruxf6m-approximation-how-nystruxf6m-approximation-helps-kriging}

Kriging can significantly benefit from the Nystrm approximation,
especially when dealing with large datasets. Kriging is a spatial
interpolation method used to estimate values at unmeasured locations
based on observed points. It relies on a \textbf{covariance matrix}
(often denoted as \textbf{K}) that describes the spatial correlation
between all observed data points.

\textbf{The Problem with Standard Kriging:}

The main computational challenge in Kriging is solving for the weights
needed for prediction, which requires \textbf{inverting the covariance
matrix K}. For \texttt{n} data points, \textbf{K} is an \texttt{n\ x\ n}
matrix, and inverting it has computational complexity \(O(n^3)\). This
becomes impractical for large datasets.

\textbf{The Nystrm Solution:}

Since the covariance matrix in Kriging is a type of kernel matrix, we
can use the Nystrm method to create a low-rank approximation,
\(\tilde{K}\). Instead of inverting the full matrix, we use the
\textbf{Woodbury matrix identity} on the Nystrm approximation, allowing
us to efficiently compute \(\tilde{K}^{-1}\) without forming the full
matrix. This reduces computational complexity to roughly \(O(l^2 n)\),
where \texttt{l} is the number of sampled columns.

In summary, Nystrm makes Kriging feasible for large-scale problems by
replacing expensive matrix inversion with a faster, memory-efficient
approximation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Example: Predicting Temperature with
Nystrm-Kriging}\label{example-predicting-temperature-with-nystruxf6m-kriging}

Suppose we have temperature readings from 100 weather stations
(\texttt{n=100}) and want to predict the temperature at a new location.

\textbf{Data:}

\begin{itemize}
\tightlist
\item
  Observed Locations (X): 100 coordinate pairs
\item
  Observed Temperatures (y): 100 values
\item
  Prediction Location (x*): Coordinates of the new location
\end{itemize}

\subsubsection{Step 1: Nystrm Approximation of the Covariance
Matrix}\label{step-1-nystruxf6m-approximation-of-the-covariance-matrix}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sample Representative Points:} Randomly select \texttt{l=10}
  stations as landmarks.
\item
  \textbf{Compute C and W:}

  \begin{itemize}
  \tightlist
  \item
    \textbf{C:} Covariance between all 100 stations and the 10 landmarks
    (\texttt{100x10} matrix)
  \item
    \textbf{W:} Covariance among the 10 landmarks (\texttt{10x10}
    matrix)
  \end{itemize}
\end{enumerate}

Nystrm approximation: \(\tilde{K} = C W^{+} C^T\)

\subsubsection{Step 2: Modeling and
Prediction}\label{step-2-modeling-and-prediction}

Standard Kriging prediction: \[
y(x^*) = \mathbf{k}^{*T} \mathbf{K}^{-1} \mathbf{y}
\] where \(\mathbf{k}^{*T}\) is the covariance vector between the
prediction location and all observed locations.

Nystrm-Kriging prediction: \[
y(x^*) \approx \mathbf{k}^{*T} (\text{fast\_approx\_inverse}(\mathbf{C}, \mathbf{W})) \mathbf{y}
\]

\textbf{Prediction Steps:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate \(\mathbf{k}^{*T}\): Covariance between new location and all
  stations.
\item
  Approximate the inverse term using the Woodbury identity with
  \textbf{C} and \textbf{W}.
\item
  Make the prediction: Take the dot product of \(\mathbf{k}^{*T}\) and
  the weights vector.
\end{enumerate}

This yields an accurate prediction efficiently, enabling rapid mapping
for large regions.

\subsection{Details: Woodbury Matrix Identity for Avoiding the Big
Inversion}\label{details-woodbury-matrix-identity-for-avoiding-the-big-inversion}

First, what is the \textbf{Woodbury matrix identity}? It's a
mathematical rule that tells you how to find the inverse of a matrix
that's been modified slightly. Its most useful form is for a ``low-rank
update'':

\[
(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}
\]

This looks complicated, but the core idea is simple:

\begin{itemize}
\tightlist
\item
  If you have a matrix \(A\) that is \textbf{easy to invert} (like a
  diagonal matrix).
\item
  And you add a low-rank matrix to it (the \(UCV\) part, where \(C\) is
  small).
\item
  You can find the new inverse without directly inverting the big
  \((A + UCV)\) matrix. Instead, you only need to invert the much
  smaller matrix in the middle of the formula: \((C^{-1} + VA^{-1}U)\).
\end{itemize}

\textbf{How does this apply to the Nystrm approximation?}

In many machine learning and Kriging applications, we don't just need
the kernel matrix \(\tilde{K}\), but a ``regularized'' version,
\((\lambda I + \tilde{K})\), where \(\lambda I\) is a diagonal matrix
that helps prevent overfitting. We need to find the inverse of this:

\[
(\lambda I + \tilde{K})^{-1}
\]

Substituting the Nystrm formula \(\tilde{K} = C W^{+} C^T\), we get:

\[
(\lambda I + C W^{+} C^T)^{-1}
\]

This expression fits the Woodbury identity perfectly!

\begin{itemize}
\tightlist
\item
  \(A = \lambda I\) (very easy to invert:
  \(A^{-1} = \frac{1}{\lambda}I\))
\item
  \(U = C\) (our \(n \times l\) matrix)
\item
  \(C\) (middle matrix) \(= W^{+}\) (our small \(l \times l\) matrix)
\item
  \(V = C^T\) (our \(l \times n\) matrix)
\end{itemize}

By plugging these into the Woodbury formula, we get an expression for
the inverse that only requires inverting a small \(l \times l\) matrix.
This means we never have to build the full \(n \times n\) matrix
\(\tilde{K}\) or invert it directly. This is the source of the massive
speed-up.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{The Example: Step-by-Step}\label{the-example-step-by-step}

Let's reuse our 4-point example and show both the slow way and the fast
Woodbury way.

\textbf{Recall our matrices:}

\begin{itemize}
\tightlist
\item
  \(C = \begin{pmatrix} 9 & 3 \\ 6 & 2 \\ 3 & 1 \\ 1 & 0.25 \end{pmatrix}\)
\item
  \(W^{+} = \begin{pmatrix} 0.09 & -0.27 \\ -0.27 & 0.81 \end{pmatrix}\)
\item
  Let's use a regularization value \(\lambda = 0.1\).
\end{itemize}

\subsubsection{Method 1: The Slow Way (Forming the full
matrix)}\label{method-1-the-slow-way-forming-the-full-matrix}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Construct \(\tilde{K}\):} First, we explicitly calculate the
  full \(4 \times 4\) Nystrm approximation \(\tilde{K} = C W^{+} C^T\).

  \[
   \tilde{K} = \begin{pmatrix}
   9 & 6 & 3 & 0.675 \\
   6 & 4 & 2 & 0.45 \\
   3 & 2 & 1 & 0.225 \\
   0.675 & 0.45 & 0.225 & 0.05
   \end{pmatrix}
   \]
\item
  \textbf{Add the regularization:} Now we compute
  \((\lambda I + \tilde{K})\).

  \[
   (\lambda I + \tilde{K}) = \begin{pmatrix}
   9.1 & 6 & 3 & 0.675 \\
   6 & 4.1 & 2 & 0.45 \\
   3 & 2 & 1.1 & 0.225 \\
   0.675 & 0.45 & 0.225 & 0.15
   \end{pmatrix}
   \]
\item
  \textbf{Invert the \(4 \times 4\) matrix:} This is the expensive step.
  The result is:

  \[
   (\lambda I + \tilde{K})^{-1} \approx
   \begin{pmatrix}
   9.85 & -14.78 & -0.07 & 0.27 \\
   -14.78 & 22.22 & 0.09 & -0.41 \\
   -0.07 & 0.09 & 0.91 & -0.03 \\
   0.27 & -0.41 & -0.03 & 6.67
   \end{pmatrix}
   \]
\end{enumerate}

This works for our tiny \(4 \times 4\) example, but it would be
computationally infeasible if \(n\) was 10,000.

\subsubsection{Method 2: The Fast Way (Using Woodbury
Identity)}\label{method-2-the-fast-way-using-woodbury-identity}

We use the Woodbury formula to get the same result without ever creating
a \(4 \times 4\) matrix. The formula simplifies to:

\[
(\lambda I + \tilde{K})^{-1} = \frac{1}{\lambda}I - \frac{1}{\lambda^2} C \left(W + \frac{1}{\lambda}C^T C\right)^{-1} C^T
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Compute the small \(2 \times 2\) pieces:}

  \begin{itemize}
  \tightlist
  \item
    \(C^T C = \begin{pmatrix} 127 & 42.25 \\ 42.25 & 14.0625 \end{pmatrix}\)
  \item
    \(W = \begin{pmatrix} 9 & 3 \\ 3 & 1 \end{pmatrix}\)
  \item
    The matrix to invert is
    \(W + \frac{1}{0.1}C^T C = W + 10 \cdot (C^T C)\), which is: \[
    \begin{pmatrix} 9 & 3 \\ 3 & 1 \end{pmatrix} +
    \begin{pmatrix} 1270 & 422.5 \\ 422.5 & 140.625 \end{pmatrix} =
    \begin{pmatrix} 1279 & 425.5 \\ 425.5 & 141.625 \end{pmatrix}
    \]
  \end{itemize}
\item
  \textbf{Invert the small \(2 \times 2\) matrix:} This is the only
  inversion we need, and it's extremely fast.

  \[
   (W + \frac{1}{\lambda}C^T C)^{-1} \approx
   \begin{pmatrix}
   0.22 & -0.66 \\
   -0.66 & 1.99
   \end{pmatrix}
   \]
\item
  \textbf{Combine the results:} Now we plug this small inverse back into
  the full formula. The rest is just matrix multiplication, no more
  inversions.

  \begin{itemize}
  \tightlist
  \item
    First, calculate the middle term:
    \(M = \frac{1}{\lambda^2} C (\dots)^{-1} C^T\). This will result in
    a \(4 \times 4\) matrix.
  \item
    Then, calculate the final result: \(\frac{1}{\lambda}I - M\).
  \end{itemize}
\end{enumerate}

After performing these multiplications, you will get the \textbf{exact
same \(4 \times 4\) inverse matrix} as in the slow method.

The crucial difference is that the most expensive operation---the matrix
inversion---was performed on a tiny \(2 \times 2\) matrix instead of a
\(4 \times 4\) one. For a large-scale problem, this is the difference
between a calculation that takes seconds and one that could take hours
or even be impossible.

\section{Extending spotpython's Kriging Surrogate with Nystrm
Approximation for Enhanced
Scalability}\label{extending-spotpythons-kriging-surrogate-with-nystruxf6m-approximation-for-enhanced-scalability}

\subsection{Introduction: Overcoming the Scalability Challenge in
Kriging for Sequential
Optimization}\label{introduction-overcoming-the-scalability-challenge-in-kriging-for-sequential-optimization}

The Sequential Parameter Optimization Toolbox (spotpython) is a
framework for hyperparameter tuning and black-box optimization based on
Sequential Model-Based Optimization (SMBO). At the core of SMBO lies a
surrogate model that approximates the true, expensive objective. Kriging
(Gaussian Process regression) is a premier choice because it provides
both predictions and a principled measure of uncertainty. This
uncertainty enables a balance between exploration and exploitation. In
each SMBO iteration, the Kriging model is updated with new evaluations,
refining its approximation and proposing the next points.

Standard Kriging requires constructing and inverting an \(n \times n\)
covariance matrix, where \(n\) is the number of data points. Matrix
inversion scales as \(O(n^3)\). During SMBO, \(n\) can reach hundreds or
thousands; refitting the surrogate each iteration becomes prohibitively
expensive. This cubic scaling is the key obstacle to applying Kriging at
larger scales.

We integrate the Nystrm method into the spotpython Kriging class. The
Nystrm method yields a low-rank approximation of a symmetric positive
semidefinite (SPSD) kernel matrix by selecting \(l \ll n\) ``landmark''
points. It approximates the full \(n \times n\) covariance while
requiring inversion of only an \(l \times l\) matrix, reducing fitting
cost from \(O(n^3)\) to \(O(n\,l^2)\). This makes Kriging viable even
when the number of function evaluations is large.

\subsection{Report Objectives and
Structure}\label{report-objectives-and-structure}

\begin{itemize}
\tightlist
\item
  Review theoretical foundations of Kriging and Nystrm approximation
\item
  Present documented Python code updates for Kriging (as in kriging.py)
\item
  Explain changes to \texttt{\_\_init\_\_}, \texttt{fit}, and
  \texttt{predict}
\item
  Show how mixed variable types are preserved via \texttt{build\_Psi}
  and \texttt{build\_psi\_vec}
\item
  Provide practical usage guidance and a formal complexity analysis
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Theoretical Foundations: The Nystrm--Kriging
Framework}\label{theoretical-foundations-the-nystruxf6mkriging-framework}

\subsection{A Primer on Kriging (Gaussian Process
Regression)}\label{a-primer-on-kriging-gaussian-process-regression}

Kriging models \(f(x)\) as a Gaussian Process with mean function
\(m(\cdot)\) and covariance (kernel) \(k(\cdot,\cdot)\). For training
inputs \(X = \{x_1,\dots,x_n\}\) and observations
\(y = \{y_1,\dots,y_n\}\): \[
y \sim \mathcal{N}\!\big(m(X),\, K(X,X) + \sigma_n^2 I\big)
\] For a new point \(x_\ast\): \[
\mu(x_\ast) = k(x_\ast, X)\,[K(X,X) + \sigma_n^2 I]^{-1} y
\] \[
\sigma^2(x_\ast) = k(x_\ast, x_\ast) - k(x_\ast, X)\,[K(X,X) + \sigma_n^2 I]^{-1} k(X, x_\ast)
\] The challenge is inverting the \(n \times n\) matrix
\(K(X,X) + \sigma_n^2 I\).

\subsection{The Nystrm Method for Low-Rank Kernel
Approximation}\label{the-nystruxf6m-method-for-low-rank-kernel-approximation}

Select \(l\) landmark points \(X_m \subset X\). Let: -
\(C = K_{nm} = K(X, X_m) \in \mathbb{R}^{n \times l}\) -
\(W = K_{mm} = K(X_m, X_m) \in \mathbb{R}^{l \times l}\) Then the
Nystrm approximation is: \[
\tilde{K}_{nn} = C\,W^{+}\,C^\top = K_{nm}\,K_{mm}^{+}\,K_{mn}
\] where \(W^{+}\) is the pseudoinverse of \(W\). The approximation has
rank \(\le l\).

\subsection{Justification for Landmark
Selection}\label{justification-for-landmark-selection}

Uniform sampling without replacement is an effective and inexpensive
strategy for selecting landmarks across varied datasets and kernels.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Implementation: A Scalable Kriging Class for
spotpython}\label{implementation-a-scalable-kriging-class-for-spotpython}

\subsection{Updated kriging.py with Nystrm Approximation
(excerpt)}\label{updated-kriging.py-with-nystruxf6m-approximation-excerpt}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{"""}
\CommentTok{Kriging surrogate with optional Nystrm approximation.}
\CommentTok{"""}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.spatial.distance }\ImportTok{import}\NormalTok{ cdist}
\ImportTok{from}\NormalTok{ scipy.linalg }\ImportTok{import}\NormalTok{ cholesky, cho\_solve, solve\_triangular}

\KeywordTok{class}\NormalTok{ Kriging:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, fun\_control, n\_theta}\OperatorTok{=}\VariableTok{None}\NormalTok{, theta}\OperatorTok{=}\VariableTok{None}\NormalTok{, p}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,}
\NormalTok{                 corr}\OperatorTok{=}\StringTok{"squared\_exponential"}\NormalTok{, isotropic}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                 approximation}\OperatorTok{=}\StringTok{"None"}\NormalTok{, n\_landmarks}\OperatorTok{=}\DecValTok{100}\NormalTok{):}
        \VariableTok{self}\NormalTok{.fun\_control }\OperatorTok{=}\NormalTok{ fun\_control}
        \VariableTok{self}\NormalTok{.dim }\OperatorTok{=} \VariableTok{self}\NormalTok{.fun\_control[}\StringTok{"lower"}\NormalTok{].shape}
        \VariableTok{self}\NormalTok{.p }\OperatorTok{=}\NormalTok{ p}
        \VariableTok{self}\NormalTok{.corr }\OperatorTok{=}\NormalTok{ corr}
        \VariableTok{self}\NormalTok{.isotropic }\OperatorTok{=}\NormalTok{ isotropic}
        \VariableTok{self}\NormalTok{.approximation }\OperatorTok{=}\NormalTok{ approximation}
        \VariableTok{self}\NormalTok{.n\_landmarks }\OperatorTok{=}\NormalTok{ n\_landmarks}
        \VariableTok{self}\NormalTok{.factor\_mask }\OperatorTok{=} \VariableTok{self}\NormalTok{.fun\_control[}\StringTok{"var\_type"}\NormalTok{] }\OperatorTok{==} \StringTok{"factor"}
        \VariableTok{self}\NormalTok{.ordered\_mask }\OperatorTok{=} \OperatorTok{\textasciitilde{}}\VariableTok{self}\NormalTok{.factor\_mask}
        \VariableTok{self}\NormalTok{.n\_theta }\OperatorTok{=} \DecValTok{1} \ControlFlowTok{if}\NormalTok{ isotropic }\ControlFlowTok{else}\NormalTok{ (n\_theta }\KeywordTok{or} \VariableTok{self}\NormalTok{.dim)}
        \VariableTok{self}\NormalTok{.theta }\OperatorTok{=}\NormalTok{ np.full(}\VariableTok{self}\NormalTok{.n\_theta, }\FloatTok{0.1}\NormalTok{) }\ControlFlowTok{if}\NormalTok{ theta }\KeywordTok{is} \VariableTok{None} \ControlFlowTok{else}\NormalTok{ theta}
        \VariableTok{self}\NormalTok{.X\_, }\VariableTok{self}\NormalTok{.y\_, }\VariableTok{self}\NormalTok{.L\_, }\VariableTok{self}\NormalTok{.alpha\_ }\OperatorTok{=} \VariableTok{None}\NormalTok{, }\VariableTok{None}\NormalTok{, }\VariableTok{None}\NormalTok{, }\VariableTok{None}
        \VariableTok{self}\NormalTok{.landmarks\_, }\VariableTok{self}\NormalTok{.W\_cho\_, }\VariableTok{self}\NormalTok{.nystrom\_alpha\_ }\OperatorTok{=} \VariableTok{None}\NormalTok{, }\VariableTok{None}\NormalTok{, }\VariableTok{None}

    \KeywordTok{def}\NormalTok{ fit(}\VariableTok{self}\NormalTok{, X, y):}
        \VariableTok{self}\NormalTok{.X\_, }\VariableTok{self}\NormalTok{.y\_ }\OperatorTok{=}\NormalTok{ X, y}
\NormalTok{        n\_samples }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.approximation.lower() }\OperatorTok{==} \StringTok{"nystroem"} \KeywordTok{and}\NormalTok{ n\_samples }\OperatorTok{\textgreater{}} \VariableTok{self}\NormalTok{.n\_landmarks:}
            \ControlFlowTok{return} \VariableTok{self}\NormalTok{.\_fit\_nystrom(X, y)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.\_fit\_standard(X, y)}

    \KeywordTok{def}\NormalTok{ \_fit\_standard(}\VariableTok{self}\NormalTok{, X, y):}
\NormalTok{        Psi }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_Psi(X, X)}
\NormalTok{        Psi[np.diag\_indices\_from(Psi)] }\OperatorTok{+=} \FloatTok{1e{-}8}
        \ControlFlowTok{try}\NormalTok{:}
            \VariableTok{self}\NormalTok{.L\_ }\OperatorTok{=}\NormalTok{ cholesky(Psi, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
            \VariableTok{self}\NormalTok{.alpha\_ }\OperatorTok{=}\NormalTok{ cho\_solve((}\VariableTok{self}\NormalTok{.L\_, }\VariableTok{True}\NormalTok{), y)}
        \ControlFlowTok{except}\NormalTok{ np.linalg.LinAlgError:}
            \VariableTok{self}\NormalTok{.L\_ }\OperatorTok{=} \VariableTok{None}
            \VariableTok{self}\NormalTok{.alpha\_ }\OperatorTok{=}\NormalTok{ np.linalg.pinv(Psi) }\OperatorTok{@}\NormalTok{ y}

    \KeywordTok{def}\NormalTok{ \_fit\_nystrom(}\VariableTok{self}\NormalTok{, X, y):}
\NormalTok{        n\_samples }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{        idx }\OperatorTok{=}\NormalTok{ np.random.choice(n\_samples, }\VariableTok{self}\NormalTok{.n\_landmarks, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
        \VariableTok{self}\NormalTok{.landmarks\_ }\OperatorTok{=}\NormalTok{ X[idx, :]}
\NormalTok{        W }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_Psi(}\VariableTok{self}\NormalTok{.landmarks\_, }\VariableTok{self}\NormalTok{.landmarks\_) }\OperatorTok{+} \FloatTok{1e{-}8} \OperatorTok{*}\NormalTok{ np.eye(}\VariableTok{self}\NormalTok{.n\_landmarks)}
\NormalTok{        C }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_Psi(X, }\VariableTok{self}\NormalTok{.landmarks\_)}
        \ControlFlowTok{try}\NormalTok{:}
            \VariableTok{self}\NormalTok{.W\_cho\_ }\OperatorTok{=}\NormalTok{ cholesky(W, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
            \VariableTok{self}\NormalTok{.nystrom\_alpha\_ }\OperatorTok{=}\NormalTok{ cho\_solve((}\VariableTok{self}\NormalTok{.W\_cho\_, }\VariableTok{True}\NormalTok{), C.T }\OperatorTok{@}\NormalTok{ y)}
        \ControlFlowTok{except}\NormalTok{ np.linalg.LinAlgError:}
            \VariableTok{self}\NormalTok{.W\_cho\_ }\OperatorTok{=} \VariableTok{None}
            \VariableTok{self}\NormalTok{.\_fit\_standard(X, y)}

    \KeywordTok{def}\NormalTok{ predict(}\VariableTok{self}\NormalTok{, X\_star):}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.approximation.lower() }\OperatorTok{==} \StringTok{"nystroem"} \KeywordTok{and} \VariableTok{self}\NormalTok{.landmarks\_ }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
            \ControlFlowTok{return} \VariableTok{self}\NormalTok{.\_predict\_nystrom(X\_star)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.\_predict\_standard(X\_star)}

    \KeywordTok{def}\NormalTok{ \_predict\_standard(}\VariableTok{self}\NormalTok{, X\_star):}
\NormalTok{        psi }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_Psi(X\_star, }\VariableTok{self}\NormalTok{.X\_)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ psi }\OperatorTok{@} \VariableTok{self}\NormalTok{.alpha\_}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.L\_ }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            v }\OperatorTok{=}\NormalTok{ solve\_triangular(}\VariableTok{self}\NormalTok{.L\_, psi.T, lower}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{            y\_mse }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(v}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            Psi }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_Psi(}\VariableTok{self}\NormalTok{.X\_, }\VariableTok{self}\NormalTok{.X\_) }\OperatorTok{+} \FloatTok{1e{-}8} \OperatorTok{*}\NormalTok{ np.eye(}\VariableTok{self}\NormalTok{.X\_.shape[}\DecValTok{0}\NormalTok{])}
\NormalTok{            pi\_Psi }\OperatorTok{=}\NormalTok{ np.linalg.pinv(Psi)}
\NormalTok{            y\_mse }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((psi }\OperatorTok{@}\NormalTok{ pi\_Psi) }\OperatorTok{*}\NormalTok{ psi, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{        y\_mse[y\_mse }\OperatorTok{\textless{}} \DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{return}\NormalTok{ y\_pred, y\_mse.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ \_predict\_nystrom(}\VariableTok{self}\NormalTok{, X\_star):}
\NormalTok{        psi\_star\_m }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_Psi(X\_star, }\VariableTok{self}\NormalTok{.landmarks\_)}
\NormalTok{        y\_pred }\OperatorTok{=}\NormalTok{ psi\_star\_m }\OperatorTok{@} \VariableTok{self}\NormalTok{.nystrom\_alpha\_}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.W\_cho\_ }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{            v }\OperatorTok{=}\NormalTok{ cho\_solve((}\VariableTok{self}\NormalTok{.W\_cho\_, }\VariableTok{True}\NormalTok{), psi\_star\_m.T)}
\NormalTok{            quad }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(psi\_star\_m }\OperatorTok{*}\NormalTok{ v.T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{            y\_mse }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ quad}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            y\_mse }\OperatorTok{=}\NormalTok{ np.ones(X\_star.shape[}\DecValTok{0}\NormalTok{])}
\NormalTok{        y\_mse[y\_mse }\OperatorTok{\textless{}} \DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{return}\NormalTok{ y\_pred, y\_mse.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ build\_Psi(}\VariableTok{self}\NormalTok{, X1, X2):}
\NormalTok{        n1 }\OperatorTok{=}\NormalTok{ X1.shape[}\DecValTok{0}\NormalTok{]}
\NormalTok{        Psi }\OperatorTok{=}\NormalTok{ np.zeros((n1, X2.shape[}\DecValTok{0}\NormalTok{]))}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n1):}
\NormalTok{            Psi[i, :] }\OperatorTok{=} \VariableTok{self}\NormalTok{.build\_psi\_vec(X1[i, :], X2)}
        \ControlFlowTok{return}\NormalTok{ Psi}

    \KeywordTok{def}\NormalTok{ build\_psi\_vec(}\VariableTok{self}\NormalTok{, x, X\_):}
\NormalTok{        theta10 }\OperatorTok{=}\NormalTok{ np.full(}\VariableTok{self}\NormalTok{.dim, }\DecValTok{10}\OperatorTok{**}\VariableTok{self}\NormalTok{.theta) }\ControlFlowTok{if} \VariableTok{self}\NormalTok{.isotropic }\ControlFlowTok{else} \DecValTok{10}\OperatorTok{**}\VariableTok{self}\NormalTok{.theta}
\NormalTok{        D }\OperatorTok{=}\NormalTok{ np.zeros(X\_.shape[}\DecValTok{0}\NormalTok{])}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.ordered\_mask.}\BuiltInTok{any}\NormalTok{():}
\NormalTok{            Xo }\OperatorTok{=}\NormalTok{ X\_[:, }\VariableTok{self}\NormalTok{.ordered\_mask]}
\NormalTok{            xo }\OperatorTok{=}\NormalTok{ x[}\VariableTok{self}\NormalTok{.ordered\_mask]}
\NormalTok{            D }\OperatorTok{+=}\NormalTok{ cdist(xo.reshape(}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{), Xo, metric}\OperatorTok{=}\StringTok{"sqeuclidean"}\NormalTok{,}
\NormalTok{                       w}\OperatorTok{=}\NormalTok{theta10[}\VariableTok{self}\NormalTok{.ordered\_mask]).ravel()}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.factor\_mask.}\BuiltInTok{any}\NormalTok{():}
\NormalTok{            Xf }\OperatorTok{=}\NormalTok{ X\_[:, }\VariableTok{self}\NormalTok{.factor\_mask]}
\NormalTok{            xf }\OperatorTok{=}\NormalTok{ x[}\VariableTok{self}\NormalTok{.factor\_mask]}
\NormalTok{            D }\OperatorTok{+=}\NormalTok{ cdist(xf.reshape(}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{), Xf, metric}\OperatorTok{=}\StringTok{"hamming"}\NormalTok{,}
\NormalTok{                       w}\OperatorTok{=}\NormalTok{theta10[}\VariableTok{self}\NormalTok{.factor\_mask]).ravel() }\OperatorTok{*} \VariableTok{self}\NormalTok{.factor\_mask.}\BuiltInTok{sum}\NormalTok{()}
        \ControlFlowTok{return}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{D) }\ControlFlowTok{if} \VariableTok{self}\NormalTok{.corr }\OperatorTok{==} \StringTok{"squared\_exponential"} \ControlFlowTok{else}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{(D}\OperatorTok{**}\VariableTok{self}\NormalTok{.p))}
\end{Highlighting}
\end{Shaded}

\section{Implementation Details}\label{implementation-details}

\subsection{\texorpdfstring{Architectural Enhancements to
\texttt{init}}{Architectural Enhancements to init}}\label{architectural-enhancements-to-init}

\begin{itemize}
\tightlist
\item
  New argument \texttt{approximation="None"} for backward-compatible
  selection between exact Kriging and Nystrm
\item
  New argument \texttt{n\_landmarks} (default 100) controls the number
  of inducing points when using Nystrm
\item
  State attributes for both exact and Nystrm paths are maintained
  separately
\end{itemize}

\subsection{\texorpdfstring{The \texttt{fit()} Method: A Dual-Pathway
Approach}{The fit() Method: A Dual-Pathway Approach}}\label{the-fit-method-a-dual-pathway-approach}

\begin{itemize}
\tightlist
\item
  Dispatcher selecting exact or Nystrm pathway
\item
  The Nystrm fit Pathway (\texttt{\_fit\_nystrom}):

  \begin{itemize}
  \tightlist
  \item
    Landmark selection via uniform sampling without replacement
  \item
    Core matrices:

    \begin{itemize}
    \tightlist
    \item
      \(W = K_{mm}\) (landmark-landmark)
    \item
      \(C = K_{nm}\) (data-landmark)
    \end{itemize}
  \item
    Cholesky factorization of \(W\) (with jitter) for stability
  \item
    Pre-computation: \(\alpha_{nys} = W^{-1} C^T y\) via
    \texttt{cho\_solve}
  \end{itemize}
\item
  The Standard fit Pathway (\texttt{\_fit\_standard}):

  \begin{itemize}
  \tightlist
  \item
    Full \(\Psi\) construction, Cholesky decomposition, and solve for
    \(\alpha\)
  \item
    Fallback to pseudoinverse if Cholesky fails
  \end{itemize}
\end{itemize}

\subsection{\texorpdfstring{The \texttt{predict()} Method: Conditional
Prediction
Logic}{The predict() Method: Conditional Prediction Logic}}\label{the-predict-method-conditional-prediction-logic}

\begin{itemize}
\tightlist
\item
  Routes to Nystrm or standard prediction path based on fitted model
  state
\item
  The Nystrm predict Pathway (\texttt{\_predict\_nystrom}):

  \begin{itemize}
  \tightlist
  \item
    Cross-covariance \(\psi\) between test points and landmarks
  \item
    Mean: \(\psi \cdot \alpha_{nys}\)
  \item
    Variance: uses \texttt{cho\_solve} with \(W\) Cholesky; non-negative
    clipping
  \end{itemize}
\item
  The Standard predict Pathway (\texttt{\_predict\_standard}):

  \begin{itemize}
  \tightlist
  \item
    Cross-covariance with all training points
  \item
    Mean from \(\alpha\); variance via triangular solves or
    pseudoinverse fallback
  \end{itemize}
\end{itemize}

\subsection{Critical Detail: Preserving Mixed Variable Type
Functionality}\label{critical-detail-preserving-mixed-variable-type-functionality}

The Significance of \texttt{build\_psi\_vec}:

\begin{itemize}
\tightlist
\item
  Mixed spaces: continuous (ordered) and categorical (factor) variables
\item
  Distances:

  \begin{itemize}
  \tightlist
  \item
    Weighted squared Euclidean for ordered variables
  \item
    Weighted Hamming for factors
  \end{itemize}
\item
  Anisotropic kernel via per-dimension length-scales \(\theta\)
\item
  Nystrm path reuses \texttt{build\_Psi}  \texttt{build\_psi\_vec},
  preserving mixed-type handling
\end{itemize}

\subsection{Seamless Integration into the Nystrm
Workflow}\label{seamless-integration-into-the-nystruxf6m-workflow}

All covariance computations (\(W\), \(C\), predictive cross-covariance)
use \texttt{build\_Psi}, ensuring identical handling for mixed variable
types in both standard and Nystrm modes.

\section{Jupyter Notebook}\label{jupyter-notebook-7}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_matrices.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\chapter{Infill Criteria}\label{infill-criteria}

In the context of computer experiments and surrogate modeling, a
sampling plan refers to the set of input values, often denoted as \(X\),
at which a computer code is evaluated. The primary objective of a
sampling plan is to efficiently explore the input space to understand
the behavior of the computer code and to construct a surrogate model
that accurately represents that behavior. Historically, Response Surface
Methodology (RSM) provided methods for designing such plans, often based
on rectangular grids or factorial designs. More recently, Design and
Analysis of Computer Experiments (DACE) has emerged as a more flexible
and powerful approach for this purpose.

A surrogate model, or \(\hat{f}\), is built to approximate the expensive
response of a black-box function \(f(x)\). Since evaluating \(f\) is
costly, only a sparse set of samples is used to construct \(\hat{f}\),
which can then provide inexpensive predictions for any point in the
design space. However, as a surrogate model is inherently an
approximation of the true function, its accuracy and predictive
capabilities can be significantly improved by incorporating new data
points, known as infill points. Infill points are strategically chosen
to either reduce uncertainty, improve predictions in specific regions of
interest, or enhance the model's ability to identify optima or trends.

The process of updating a surrogate model with infill points is
iterative. It typically involves:

\begin{itemize}
\tightlist
\item
  \textbf{Identifying Regions of Interest}: Analyzing the current
  surrogate model to determine areas where it is inaccurate, has high
  uncertainty, or predicts promising results (e.g., potential optima).
\item
  \textbf{Selecting Infill Points}: Choosing new data points based on
  specific criteria that balance different objectives.
\item
  \textbf{Evaluating the True Function}: Running the actual simulation
  or experiment at the selected infill points to obtain their
  corresponding outputs.
\item
  \textbf{Updating the Surrogate Model}: Retraining or updating the
  surrogate model using the new, augmented dataset.
\item
  \textbf{Repeating}: Iterating this process until the model meets
  predefined accuracy criteria or the computational budget is exhausted.
\end{itemize}

\section{Balancing Exploitation and
Exploration}\label{balancing-exploitation-and-exploration}

A crucial aspect of selecting infill points is navigating the inherent
trade-off between exploitation and exploration.

\begin{definition}[Exploitation]\protect\hypertarget{def-exploitation}{}\label{def-exploitation}

Exploitation refers to sampling near predicted optima to refine the
solution. This strategy aims to rapidly converge on a good solution by
focusing computational effort where the surrogate model suggests the
best values might lie.

\end{definition}

\begin{definition}[Exploration]\protect\hypertarget{def-exploration}{}\label{def-exploration}

Exploration involves sampling in regions of high uncertainty to improve
the global accuracy of the model. This approach ensures that the model
is well-informed across the entire design space, preventing it from
getting stuck in local optima.

\end{definition}

(\textbf{Forr08a?}) emphasizes that effective infill criteria are
designed to combine both exploitation and exploration.

\section{Expected Improvement (EI)}\label{expected-improvement-ei}

Expected Improvement (EI) is one of the most influential and widely-used
infill criteria. Formalized by (\textbf{Jones1998?}) and building upon
the work of (\textbf{mockus1978toward?}), EI provides a mathematically
elegant framework that naturally balances exploitation and exploration.
Rather than simply picking the point with the best predicted value (pure
exploitation) or the point with the highest uncertainty (pure
exploration), EI asks a more nuanced question: ``How much improvement
over the current best solution can we \emph{expect} to gain by
evaluating the true function at a new point \(x\)?''.

The Expected Improvement, \(EI(x)\), can be calculated using the
following formula:

\[
EI(x) = \sigma(x) \left[ Z \Phi(Z) + \phi(Z) \right]
\] where:

\begin{itemize}
\tightlist
\item
  \(\mu(x)\) (or \(\hat{y}(x)\)) is the Kriging prediction (mean of the
  stochastic process) at a new, unobserved point \(x\).
\item
  \(\sigma(x)\) (or \(\hat{s}(x)\)) is the estimated standard deviation
  (square root of the variance \(\hat{s}^2(x)\)) of the prediction at
  point \(x\).
\item
  \(f_{best}\) (or \(y_{min}\)) is the best (minimum, for minimization
  problems) observed function value found so far.
\item
  \(Z = \frac{f_{best} - \mu(x)}{\sigma(x)}\) is the standardized
  improvement.
\item
  \(\Phi(Z)\) is the cumulative distribution function (CDF) of the
  standard normal distribution.
\item
  \(\phi(Z)\) is the probability density function (PDF) of the standard
  normal distribution.
\end{itemize}

If \(\sigma(x) = 0\) (meaning there is no uncertainty at point \(x\),
typically because it's an already sampled point), then \(EI(x) = 0\),
reflecting the intuition that no further improvement can be expected at
a known point. A maximization of Expected Improvement as an infill
criterion will eventually lead to the global optimum.

The elegance of the EI formula lies in its combination of two distinct
terms:

\begin{itemize}
\tightlist
\item
  \textbf{Exploitation Term}: \((f_{best} - \mu(x)) \Phi(Z)\). This part
  of the formula contributes more when the predicted value \(\mu(x)\) is
  significantly lower (better) than the current best observed value
  \(f_{best}\). It is weighted by the probability \(\Phi(Z)\) that the
  true function value at \(x\) will indeed be an improvement over
  \(f_{best}\).
\item
  \textbf{Exploration Term}: \(\sigma(x) \phi(Z)\). This term becomes
  larger when there is high uncertainty (\(\sigma(x)\) is large) in the
  model's prediction at \(x\). It accounts for the potential of
  discovering unexpectedly good values in areas that have not been
  thoroughly explored, even if the current mean prediction there is not
  the absolute best.
\end{itemize}

Expected Improvement offers several significant practical benefits:

\begin{itemize}
\tightlist
\item
  \textbf{Automatic Balance}: It inherently balances exploitation and
  exploration without requiring any manual adjustment of weights or
  parameters.
\item
  \textbf{Scale Invariance}: EI is relatively insensitive to the scaling
  of the objective function, making it robust across various problem
  types.
\item
  \textbf{Theoretical Foundation}: It is underpinned by a strong
  theoretical basis derived from decision theory and information theory.
\item
  \textbf{Efficient Optimization}: The smooth and differentiable nature
  of the EI function allows for efficient optimization using
  gradient-based algorithms to find the next infill point.
\item
  \textbf{Proven Performance}: EI has demonstrated consistent and strong
  performance in numerous real-world applications across various
  domains.
\end{itemize}

\section{Expected Improvement in the Hyperparameter Tuning Cookbook
(Python
Implementation)}\label{expected-improvement-in-the-hyperparameter-tuning-cookbook-python-implementation}

Within the context of the Hyperparameter Tuning Cookbook, Expected
Improvement serves a critical role in Sequential Model-Based
Optimization. It systematically guides the selection of which
hyperparameter configurations to evaluate next, facilitating the
efficient utilization of computational resources. By intelligently
balancing the need to exploit promising regions and explore uncertain
areas, EI helps identify optimal hyperparameters with a reduced number
of expensive model training runs. This provides a principled and
automated method for navigating complex hyperparameter spaces without
extensive manual intervention.

While the foundational concepts in (\textbf{Forr08a?}) are often
illustrated with MATLAB code, the Hyperparameter Tuning Cookbook
emphasizes and provides implementations in Python. The
\texttt{spotpython} package, consistent with the Cookbook's approach,
provides a Python implementation of Expected Improvement within its
Kriging class. For minimization problems, \texttt{spotpython} typically
calculates and returns the negative Expected Improvement, aligning with
standard optimization algorithm conventions. Furthermore, to enhance
numerical stability and mitigate issues when EI values are very small,
\texttt{spotpython} often works with a logarithmic transformation of EI
and incorporates a small epsilon value.

\section{Jupyter Notebook}\label{jupyter-notebook-8}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, toptitle=1mm, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colframe=quarto-callout-note-color-frame, titlerule=0mm, opacityback=0, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, bottomtitle=1mm, toprule=.15mm, breakable, arc=.35mm, leftrule=.75mm, left=2mm, colback=white]

\begin{itemize}
\tightlist
\item
  The Jupyter-Notebook of this lecture is available on GitHub in the
  \href{https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_infill.ipynb}{Hyperparameter-Tuning-Cookbook
  Repository}
\end{itemize}

\end{tcolorbox}

\part{Sequential Parameter Optimization Toolbox (SPOT)}

\chapter{Reproducibility in
SpotOptim}\label{reproducibility-in-spotoptim}

\section{Introduction}\label{introduction}

SpotOptim provides full support for reproducible optimization runs
through the \texttt{seed} parameter. This is essential for:

\begin{itemize}
\tightlist
\item
  \textbf{Scientific research}: Ensuring experiments can be replicated
\item
  \textbf{Debugging}: Reproducing specific optimization behaviors
\item
  \textbf{Benchmarking}: Fair comparison between different
  configurations
\item
  \textbf{Production}: Consistent results in deployed applications
\end{itemize}

When you specify a seed, SpotOptim guarantees that running the same
optimization multiple times will produce identical results. Without a
seed, each run explores the search space differently, which can be
useful for robustness testing.

\section{Basic Usage}\label{basic-usage}

\subsection{Making Optimization
Reproducible}\label{making-optimization-reproducible}

To ensure reproducible results, simply specify the \texttt{seed}
parameter when creating the optimizer:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ sphere(X):}
    \CommentTok{"""Simple sphere function: f(x) = sum(x\^{}2)"""}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Reproducible optimization}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,  }\CommentTok{\# This ensures reproducibility}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best solution: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
TensorBoard logging disabled
Initial best: f(x) = 5.542803
Iteration 1: New best f(x) = 0.001607
Iteration 2: f(x) = 0.007183
Iteration 3: f(x) = 0.009504
Iteration 4: New best f(x) = 0.000000
Iteration 5: f(x) = 0.000003
Iteration 6: f(x) = 0.000002
Iteration 7: f(x) = 0.000001
Iteration 8: f(x) = 0.000001
Iteration 9: f(x) = 0.000002
Iteration 10: f(x) = 0.000000
Iteration 11: f(x) = 0.000001
Iteration 12: f(x) = 0.000003
Iteration 13: f(x) = 0.000000
Iteration 14: f(x) = 0.000000
Iteration 15: f(x) = 0.000001
Best solution: [-0.0003343  -0.00013835]
Best value: 1.3089731064068852e-07
\end{verbatim}

\textbf{Key Point}: Running this code multiple times (even on different
days or machines) will always produce the same result.

\subsection{Running Independent
Experiments}\label{running-independent-experiments}

If you don't specify a seed, each optimization run will explore the
search space differently:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Non{-}reproducible: different results each time}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}
    \CommentTok{\# No seed specified}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\CommentTok{\# Results will vary between runs}
\end{Highlighting}
\end{Shaded}

This is useful when you want to: - Explore different regions of the
search space - Test the robustness of your results - Run multiple
independent optimization attempts

\section{Practical Examples}\label{practical-examples}

\subsection{Example 1: Comparing Different
Configurations}\label{example-1-comparing-different-configurations}

When comparing different optimizer settings, use the same seed for fair
comparison:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ rosenbrock(X):}
    \CommentTok{"""Rosenbrock function"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\CommentTok{\# Configuration 1: More initial points}
\NormalTok{opt1 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rosenbrock,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}  \CommentTok{\# Same seed for fair comparison}
\NormalTok{)}
\NormalTok{result1 }\OperatorTok{=}\NormalTok{ opt1.optimize()}

\CommentTok{\# Configuration 2: Fewer initial points, more iterations}
\NormalTok{opt2 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rosenbrock,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}  \CommentTok{\# Same seed}
\NormalTok{)}
\NormalTok{result2 }\OperatorTok{=}\NormalTok{ opt2.optimize()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Config 1 (more initial): }\SpecialCharTok{\{}\NormalTok{result1}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Config 2 (fewer initial): }\SpecialCharTok{\{}\NormalTok{result2}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Config 1 (more initial): 0.036226
Config 2 (fewer initial): 0.015384
\end{verbatim}

\subsection{Example 2: Reproducible Research
Experiment}\label{example-2-reproducible-research-experiment}

For scientific papers or reports, always use a fixed seed and document
it:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ rastrigin(X):}
    \CommentTok{"""Rastrigin function (multimodal)"""}
\NormalTok{    A }\OperatorTok{=} \DecValTok{10}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ A }\OperatorTok{*}\NormalTok{ n }\OperatorTok{+}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}}\NormalTok{ A }\OperatorTok{*}\NormalTok{ np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Documented seed for reproducibility}
\NormalTok{RANDOM\_SEED }\OperatorTok{=} \DecValTok{12345}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rastrigin,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\FloatTok{5.12}\NormalTok{, }\FloatTok{5.12}\NormalTok{), (}\OperatorTok{{-}}\FloatTok{5.12}\NormalTok{, }\FloatTok{5.12}\NormalTok{), (}\OperatorTok{{-}}\FloatTok{5.12}\NormalTok{, }\FloatTok{5.12}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\NormalTok{RANDOM\_SEED,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Experiment Results (seed=}\SpecialCharTok{\{}\NormalTok{RANDOM\_SEED}\SpecialCharTok{\}}\SpecialStringTok{):"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best solution: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Iterations: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{nit}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Function evaluations: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{nfev}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# These results can now be cited in a paper}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
TensorBoard logging disabled
Initial best: f(x) = 20.392774
Iteration 1: f(x) = 21.363476
Iteration 2: f(x) = 47.301823
Iteration 3: f(x) = 54.722105
Iteration 4: f(x) = 26.938125
Iteration 5: f(x) = 47.976435
Iteration 6: New best f(x) = 11.815352
Iteration 7: f(x) = 42.986117
Iteration 8: f(x) = 18.405281
Iteration 9: New best f(x) = 9.416725
Iteration 10: New best f(x) = 8.466069
Iteration 11: f(x) = 30.100210
Iteration 12: New best f(x) = 2.903089
Iteration 13: f(x) = 14.397238
Iteration 14: f(x) = 6.076849
Iteration 15: f(x) = 3.075001
Iteration 16: f(x) = 8.238939
Iteration 17: f(x) = 32.184621
Iteration 18: f(x) = 24.864179
Iteration 19: f(x) = 47.118258
Iteration 20: f(x) = 39.419666
Iteration 21: f(x) = 53.085310
Iteration 22: f(x) = 9.975504
Iteration 23: f(x) = 35.001393
Iteration 24: f(x) = 38.805263
Iteration 25: f(x) = 53.761437
Iteration 26: f(x) = 22.263620
Iteration 27: f(x) = 36.329232
Iteration 28: f(x) = 43.768628
Iteration 29: f(x) = 48.136701
Iteration 30: f(x) = 33.012990
Iteration 31: f(x) = 39.630292
Iteration 32: f(x) = 56.076800
Iteration 33: f(x) = 56.395240
Iteration 34: f(x) = 38.348341
Iteration 35: f(x) = 49.557146
Iteration 36: f(x) = 59.083493
Iteration 37: f(x) = 30.011270
Iteration 38: f(x) = 41.465815
Iteration 39: f(x) = 27.727924
Iteration 40: f(x) = 42.088395
Iteration 41: f(x) = 28.204654
Iteration 42: f(x) = 18.178644
Iteration 43: f(x) = 35.087961
Iteration 44: f(x) = 57.612698
Iteration 45: f(x) = 38.620443
Iteration 46: f(x) = 54.041182
Iteration 47: f(x) = 28.335777
Iteration 48: f(x) = 31.404859
Iteration 49: f(x) = 68.751895
Iteration 50: f(x) = 62.442382
Iteration 51: f(x) = 53.043276
Iteration 52: f(x) = 49.678065
Iteration 53: f(x) = 51.523703
Iteration 54: f(x) = 45.533620
Iteration 55: New best f(x) = 2.181277
Iteration 56: New best f(x) = 1.996482
Iteration 57: f(x) = 51.513483
Iteration 58: f(x) = 43.295834
Iteration 59: f(x) = 3.690653
Iteration 60: f(x) = 33.624017
Iteration 61: f(x) = 37.625215
Iteration 62: f(x) = 67.144123
Iteration 63: f(x) = 25.844100
Iteration 64: f(x) = 54.673898
Iteration 65: f(x) = 11.388202
Iteration 66: f(x) = 39.385158
Iteration 67: f(x) = 4.226770
Iteration 68: f(x) = 67.022923
Iteration 69: f(x) = 28.240244
Iteration 70: f(x) = 36.207929

Experiment Results (seed=12345):
Best solution: [ 0.99166413 -0.99787425 -0.0037072 ]
Best value: 1.9964821694753638
Iterations: 70
Function evaluations: 100
\end{verbatim}

\subsection{Example 3: Multiple Independent
Runs}\label{example-3-multiple-independent-runs}

To test robustness, run the same optimization with different seeds:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ ackley(X):}
    \CommentTok{"""Ackley function"""}
\NormalTok{    a }\OperatorTok{=} \DecValTok{20}
\NormalTok{    b }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{    c }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
    
\NormalTok{    sum\_sq }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    sum\_cos }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.cos(c }\OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{a }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{b }\OperatorTok{*}\NormalTok{ np.sqrt(sum\_sq }\OperatorTok{/}\NormalTok{ n)) }\OperatorTok{{-}}\NormalTok{ np.exp(sum\_cos }\OperatorTok{/}\NormalTok{ n) }\OperatorTok{+}\NormalTok{ a }\OperatorTok{+}\NormalTok{ np.e}

\CommentTok{\# Run 5 independent optimizations}
\NormalTok{results }\OperatorTok{=}\NormalTok{ []}
\NormalTok{seeds }\OperatorTok{=}\NormalTok{ [}\DecValTok{42}\NormalTok{, }\DecValTok{123}\NormalTok{, }\DecValTok{456}\NormalTok{, }\DecValTok{789}\NormalTok{, }\DecValTok{1011}\NormalTok{]}

\ControlFlowTok{for}\NormalTok{ seed }\KeywordTok{in}\NormalTok{ seeds:}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{        fun}\OperatorTok{=}\NormalTok{ackley,}
\NormalTok{        bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{        max\_iter}\OperatorTok{=}\DecValTok{40}\NormalTok{,}
\NormalTok{        n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{        seed}\OperatorTok{=}\NormalTok{seed,}
\NormalTok{        verbose}\OperatorTok{=}\VariableTok{False}
\NormalTok{    )}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\NormalTok{    results.append(result.fun)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Run with seed }\SpecialCharTok{\{}\NormalTok{seed}\SpecialCharTok{:4d\}}\SpecialStringTok{: f(x) = }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Analyze robustness}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Best result: }\SpecialCharTok{\{}\BuiltInTok{min}\NormalTok{(results)}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Worst result: }\SpecialCharTok{\{}\BuiltInTok{max}\NormalTok{(results)}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Mean: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(results)}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Std dev: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{std(results)}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Run with seed   42: f(x) = 0.145422
Run with seed  123: f(x) = 0.023381
Run with seed  456: f(x) = 0.022664
Run with seed  789: f(x) = 0.035378
Run with seed 1011: f(x) = 0.115351

Best result: 0.022664
Worst result: 0.145422
Mean: 0.068439
Std dev: 0.051664
\end{verbatim}

\subsection{Example 4: Reproducible Initial
Design}\label{example-4-reproducible-initial-design}

The seed ensures that even the initial design points are reproducible:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ simple\_quadratic(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((X }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Create two optimizers with same seed}
\NormalTok{opt1 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{simple\_quadratic,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{25}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{999}
\NormalTok{)}

\NormalTok{opt2 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{simple\_quadratic,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{25}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{999}  \CommentTok{\# Same seed}
\NormalTok{)}

\CommentTok{\# Run both optimizations}
\NormalTok{result1 }\OperatorTok{=}\NormalTok{ opt1.optimize()}
\NormalTok{result2 }\OperatorTok{=}\NormalTok{ opt2.optimize()}

\CommentTok{\# Verify identical results}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Initial design points are identical:"}\NormalTok{, }
\NormalTok{      np.allclose(opt1.X\_[:}\DecValTok{10}\NormalTok{], opt2.X\_[:}\DecValTok{10}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"All evaluated points are identical:"}\NormalTok{, }
\NormalTok{      np.allclose(opt1.X\_, opt2.X\_))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"All function values are identical:"}\NormalTok{, }
\NormalTok{      np.allclose(opt1.y\_, opt2.y\_))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best solutions are identical:"}\NormalTok{, }
\NormalTok{      np.allclose(result1.x, result2.x))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Initial design points are identical: True
All evaluated points are identical: True
All function values are identical: True
Best solutions are identical: True
\end{verbatim}

\subsection{Example 5: Custom Initial Design with
Seed}\label{example-5-custom-initial-design-with-seed}

Even when providing a custom initial design, the seed ensures
reproducible subsequent iterations:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ beale(X):}
    \CommentTok{"""Beale function"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{    term1 }\OperatorTok{=}\NormalTok{ (}\FloatTok{1.5} \OperatorTok{{-}}\NormalTok{ x }\OperatorTok{+}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y)}\OperatorTok{**}\DecValTok{2}
\NormalTok{    term2 }\OperatorTok{=}\NormalTok{ (}\FloatTok{2.25} \OperatorTok{{-}}\NormalTok{ x }\OperatorTok{+}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}
\NormalTok{    term3 }\OperatorTok{=}\NormalTok{ (}\FloatTok{2.625} \OperatorTok{{-}}\NormalTok{ x }\OperatorTok{+}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y}\OperatorTok{**}\DecValTok{3}\NormalTok{)}\OperatorTok{**}\DecValTok{2}
    \ControlFlowTok{return}\NormalTok{ term1 }\OperatorTok{+}\NormalTok{ term2 }\OperatorTok{+}\NormalTok{ term3}

\CommentTok{\# Custom initial design (e.g., from previous knowledge)}
\NormalTok{X\_start }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{    [}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{],}
\NormalTok{    [}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{],}
\NormalTok{    [}\FloatTok{2.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{],}
\NormalTok{    [}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{]}
\NormalTok{])}

\CommentTok{\# Run twice with same seed and initial design}
\NormalTok{opt1 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{beale,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\FloatTok{4.5}\NormalTok{, }\FloatTok{4.5}\NormalTok{), (}\OperatorTok{{-}}\FloatTok{4.5}\NormalTok{, }\FloatTok{4.5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{777}
\NormalTok{)}
\NormalTok{result1 }\OperatorTok{=}\NormalTok{ opt1.optimize(X0}\OperatorTok{=}\NormalTok{X\_start)}

\NormalTok{opt2 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{beale,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\FloatTok{4.5}\NormalTok{, }\FloatTok{4.5}\NormalTok{), (}\OperatorTok{{-}}\FloatTok{4.5}\NormalTok{, }\FloatTok{4.5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{777}  \CommentTok{\# Same seed}
\NormalTok{)}
\NormalTok{result2 }\OperatorTok{=}\NormalTok{ opt2.optimize(X0}\OperatorTok{=}\NormalTok{X\_start)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Results are identical:"}\NormalTok{, np.allclose(result1.x, result2.x))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value: }\SpecialCharTok{\{}\NormalTok{result1}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Results are identical: True
Best value: 1.024940
\end{verbatim}

\section{Advanced Topics}\label{advanced-topics}

\subsection{Seed and Noisy Functions}\label{seed-and-noisy-functions}

When optimizing noisy functions with repeated evaluations, the seed
ensures reproducible noise:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ noisy\_sphere(X):}
    \CommentTok{"""Sphere function with Gaussian noise"""}
\NormalTok{    base }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, size}\OperatorTok{=}\NormalTok{base.shape)}
    \ControlFlowTok{return}\NormalTok{ base }\OperatorTok{+}\NormalTok{ noise}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{noisy\_sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{40}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    repeats\_initial}\OperatorTok{=}\DecValTok{3}\NormalTok{,  }\CommentTok{\# 3 evaluations per point}
\NormalTok{    repeats\_surrogate}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}  \CommentTok{\# Ensures same noise pattern}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best mean value: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{min\_mean\_y}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Variance at best: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{min\_var\_y}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Best mean value: 0.066466
Variance at best: 0.008717
\end{verbatim}

\textbf{Important}: With the same seed, even the noise will be identical
across runs!

\subsection{Different Seeds for Different
Exploration}\label{different-seeds-for-different-exploration}

Use different seeds to explore different regions systematically:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ griewank(X):}
    \CommentTok{"""Griewank function"""}
\NormalTok{    sum\_sq }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2} \OperatorTok{/} \DecValTok{4000}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    prod\_cos }\OperatorTok{=}\NormalTok{ np.prod(np.cos(X }\OperatorTok{/}\NormalTok{ np.sqrt(np.arange(}\DecValTok{1}\NormalTok{, X.shape[}\DecValTok{1}\NormalTok{] }\OperatorTok{+} \DecValTok{1}\NormalTok{))), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ sum\_sq }\OperatorTok{{-}}\NormalTok{ prod\_cos }\OperatorTok{+} \DecValTok{1}

\CommentTok{\# Systematic exploration with different seeds}
\NormalTok{best\_overall }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)}
\NormalTok{best\_seed }\OperatorTok{=} \VariableTok{None}

\ControlFlowTok{for}\NormalTok{ seed }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{):  }\CommentTok{\# Seeds 10{-}19}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{        fun}\OperatorTok{=}\NormalTok{griewank,}
\NormalTok{        bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{600}\NormalTok{, }\DecValTok{600}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{600}\NormalTok{, }\DecValTok{600}\NormalTok{)],}
\NormalTok{        max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{        n\_initial}\OperatorTok{=}\DecValTok{25}\NormalTok{,}
\NormalTok{        seed}\OperatorTok{=}\NormalTok{seed}
\NormalTok{    )}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
    
    \ControlFlowTok{if}\NormalTok{ result.fun }\OperatorTok{\textless{}}\NormalTok{ best\_overall:}
\NormalTok{        best\_overall }\OperatorTok{=}\NormalTok{ result.fun}
\NormalTok{        best\_seed }\OperatorTok{=}\NormalTok{ seed}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Seed }\SpecialCharTok{\{}\NormalTok{seed}\SpecialCharTok{\}}\SpecialStringTok{: f(x) = }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Best result with seed }\SpecialCharTok{\{}\NormalTok{best\_seed}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{best\_overall}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Seed 10: f(x) = 0.796656
Seed 11: f(x) = 0.042354
Seed 12: f(x) = 1.156801
Seed 13: f(x) = 0.720165
Seed 14: f(x) = 1.193134
Seed 15: f(x) = 1.617529
Seed 16: f(x) = 0.345078
Seed 17: f(x) = 0.677278
Seed 18: f(x) = 1.458146
Seed 19: f(x) = 1.349480

Best result with seed 11: 0.042354
\end{verbatim}

\section{Best Practices}\label{best-practices}

\subsection{1. Always Use Seeds for Production
Code}\label{always-use-seeds-for-production-code}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good: Reproducible}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{objective, bounds}\OperatorTok{=}\NormalTok{bounds, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Risky: Non{-}reproducible}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{objective, bounds}\OperatorTok{=}\NormalTok{bounds)}
\end{Highlighting}
\end{Shaded}

\subsection{2. Document Your Seeds}\label{document-your-seeds}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Configuration for experiment reported in Section 4.2}
\NormalTok{EXPERIMENT\_SEED }\OperatorTok{=} \DecValTok{2024}
\NormalTok{MAX\_ITERATIONS }\OperatorTok{=} \DecValTok{100}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{my\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{my\_bounds,}
\NormalTok{    max\_iter}\OperatorTok{=}\NormalTok{MAX\_ITERATIONS,}
\NormalTok{    seed}\OperatorTok{=}\NormalTok{EXPERIMENT\_SEED}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{3. Use Different Seeds for Different
Experiments}\label{use-different-seeds-for-different-experiments}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Different experiments should use different seeds}
\NormalTok{BASELINE\_SEED }\OperatorTok{=} \DecValTok{100}
\NormalTok{EXPERIMENT\_A\_SEED }\OperatorTok{=} \DecValTok{200}
\NormalTok{EXPERIMENT\_B\_SEED }\OperatorTok{=} \DecValTok{300}
\end{Highlighting}
\end{Shaded}

\subsection{4. Test Robustness Across Multiple
Seeds}\label{test-robustness-across-multiple-seeds}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run same optimization with multiple seeds}
\ControlFlowTok{for}\NormalTok{ seed }\KeywordTok{in}\NormalTok{ [}\DecValTok{42}\NormalTok{, }\DecValTok{123}\NormalTok{, }\DecValTok{456}\NormalTok{, }\DecValTok{789}\NormalTok{, }\DecValTok{1011}\NormalTok{]:}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{objective, bounds}\OperatorTok{=}\NormalTok{bounds, seed}\OperatorTok{=}\NormalTok{seed)}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
    \CommentTok{\# Analyze results}
\end{Highlighting}
\end{Shaded}

\section{What the Seed Controls}\label{what-the-seed-controls}

The \texttt{seed} parameter ensures reproducibility by controlling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initial Design Generation}: Latin Hypercube Sampling produces
  the same initial points
\item
  \textbf{Surrogate Model}: Gaussian Process random initialization is
  identical
\item
  \textbf{Acquisition Optimization}: Differential evolution explores the
  same candidates
\item
  \textbf{Random Sampling}: Any random exploration uses the same random
  numbers
\end{enumerate}

This guarantees that the entire optimization pipeline is deterministic
and reproducible.

\section{Common Questions}\label{common-questions}

\textbf{Q: Can I use seed=0?}\\
A: Yes, any integer (including 0) is a valid seed.

\textbf{Q: Will different Python versions give the same results?}\\
A: Generally yes, but minor numerical differences may occur due to
underlying library changes. Use the same environment for exact
reproducibility.

\textbf{Q: Does the seed affect the objective function?}\\
A: No, the seed only affects SpotOptim's internal random processes. If
your objective function has its own randomness, you'll need to control
that separately.

\textbf{Q: How do I choose a good seed value?}\\
A: Any integer works. Common choices are 42, 123, or dates (e.g.,
20241112). What matters is consistency, not the specific value.

\section{Summary}\label{summary}

\begin{itemize}
\tightlist
\item
  Use \texttt{seed} parameter for reproducible optimization
\item
  Same seed  identical results (every time)
\item
  No seed  different results (random exploration)\\
\item
  Essential for research, debugging, and production
\item
  Document your seeds for transparency
\item
  Test robustness with multiple different seeds
\end{itemize}

\chapter{Acquisition Failure Handling in
SpotOptim}\label{acquisition-failure-handling-in-spotoptim}

SpotOptim provides sophisticated fallback strategies for handling
acquisition function failures during optimization. This ensures robust
optimization even when the surrogate model struggles to suggest new
points.

\section{What is Acquisition
Failure?}\label{what-is-acquisition-failure}

During surrogate-based optimization, the acquisition function suggests
new points to evaluate. However, sometimes the suggested point is
\textbf{too close} to existing points (within \texttt{tolerance\_x}
distance), which would provide little new information. When this
happens, SpotOptim uses a \textbf{fallback strategy} to propose an
alternative point.

\section{Fallback Strategies}\label{fallback-strategies}

SpotOptim supports two fallback strategies, controlled by the
\texttt{acquisition\_failure\_strategy} parameter:

\subsection{1. Random Space-Filling Design
(Default)}\label{random-space-filling-design-default}

\textbf{Strategy name}: \texttt{"random"}

This strategy uses Latin Hypercube Sampling (LHS) to generate a new
space-filling point. LHS ensures good coverage of the search space by
dividing each dimension into equal-probability intervals.

\textbf{When to use}:

\begin{itemize}
\tightlist
\item
  General-purpose optimization
\item
  When you want simplicity and good space-filling properties
\item
  Default choice for most problems
\end{itemize}

\textbf{Example}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ sphere(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"random"}\NormalTok{,  }\CommentTok{\# Default}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{2. Morris-Mitchell Minimizing
Point}\label{morris-mitchell-minimizing-point}

\textbf{Strategy name}: \texttt{"mm"}

This strategy finds a point that \textbf{maximizes the minimum distance}
to all existing points. It evaluates 100 candidate points and selects
the one with the largest minimum distance to the already-evaluated
points, providing excellent space-filling properties.

\textbf{When to use}:

\begin{itemize}
\tightlist
\item
  When you want to ensure maximum exploration
\item
  For problems where avoiding clustering of points is critical
\item
  When the search space has been heavily sampled in some regions
\end{itemize}

\textbf{Example}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ rosenbrock(X):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rosenbrock,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}\NormalTok{,  }\CommentTok{\# Morris{-}Mitchell}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\section{How It Works}\label{how-it-works}

The acquisition failure handling is integrated into the optimization
process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Acquisition optimization}: SpotOptim uses differential
  evolution to optimize the acquisition function
\item
  \textbf{Distance check}: The proposed point is checked against
  existing points using \texttt{tolerance\_x}
\item
  \textbf{Fallback activation}: If the point is too close,
  \texttt{\_handle\_acquisition\_failure()} is called
\item
  \textbf{Strategy execution}: The configured fallback strategy
  generates a new point
\item
  \textbf{Evaluation}: The fallback point is evaluated and added to the
  dataset
\end{enumerate}

\section{Comparison of Strategies}\label{comparison-of-strategies}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & Random (LHS) & Morris-Mitchell \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computation} & Very fast & Moderate (100 candidates) \\
\textbf{Space-filling} & Good & Excellent \\
\textbf{Exploration} & Balanced & Maximum distance \\
\textbf{Clustering avoidance} & Good & Best \\
\textbf{Recommended for} & General use & Heavily sampled spaces \\
\end{longtable}

\section{Complete Example: Comparing
Strategies}\label{complete-example-comparing-strategies}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ ackley(X):}
    \CommentTok{"""Ackley function {-} multimodal test function"""}
\NormalTok{    a }\OperatorTok{=} \DecValTok{20}
\NormalTok{    b }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{    c }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
    
\NormalTok{    sum\_sq }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    sum\_cos }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.cos(c }\OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{a }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{b }\OperatorTok{*}\NormalTok{ np.sqrt(sum\_sq }\OperatorTok{/}\NormalTok{ n)) }\OperatorTok{{-}}\NormalTok{ np.exp(sum\_cos }\OperatorTok{/}\NormalTok{ n) }\OperatorTok{+}\NormalTok{ a }\OperatorTok{+}\NormalTok{ np.e}

\CommentTok{\# Test with random strategy}
\BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{60}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Testing with Random Space{-}Filling Strategy"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{60}\NormalTok{)}

\NormalTok{opt\_random }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{ackley,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"random"}\NormalTok{,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,  }\CommentTok{\# Relatively large tolerance to trigger failures}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result\_random }\OperatorTok{=}\NormalTok{ opt\_random.optimize()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Random Strategy Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best value: }\SpecialCharTok{\{}\NormalTok{result\_random}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best point: }\SpecialCharTok{\{}\NormalTok{result\_random}\SpecialCharTok{.}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Total evaluations: }\SpecialCharTok{\{}\NormalTok{result\_random}\SpecialCharTok{.}\NormalTok{nfev}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Test with Morris{-}Mitchell strategy}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"} \OperatorTok{+} \StringTok{"="} \OperatorTok{*} \DecValTok{60}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Testing with Morris{-}Mitchell Strategy"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{60}\NormalTok{)}

\NormalTok{opt\_mm }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{ackley,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}\NormalTok{,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,  }\CommentTok{\# Same tolerance}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result\_mm }\OperatorTok{=}\NormalTok{ opt\_mm.optimize()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Morris{-}Mitchell Strategy Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best value: }\SpecialCharTok{\{}\NormalTok{result\_mm}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best point: }\SpecialCharTok{\{}\NormalTok{result\_mm}\SpecialCharTok{.}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Total evaluations: }\SpecialCharTok{\{}\NormalTok{result\_mm}\SpecialCharTok{.}\NormalTok{nfev}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Compare}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"} \OperatorTok{+} \StringTok{"="} \OperatorTok{*} \DecValTok{60}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Comparison"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{60}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Random strategy:        }\SpecialCharTok{\{}\NormalTok{result\_random}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Morris{-}Mitchell strategy: }\SpecialCharTok{\{}\NormalTok{result\_mm}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{ result\_random.fun }\OperatorTok{\textless{}}\NormalTok{ result\_mm.fun:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" Random strategy found better solution"}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" Morris{-}Mitchell strategy found better solution"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Advanced Usage: Setting
Tolerance}\label{advanced-usage-setting-tolerance}

The \texttt{tolerance\_x} parameter controls when the fallback strategy
is triggered. A larger tolerance means points need to be farther apart,
triggering the fallback more often:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Strict tolerance (smaller value) {-} fewer fallbacks}
\NormalTok{optimizer\_strict }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{,  }\CommentTok{\# Very small {-} almost never triggers fallback}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}
\NormalTok{)}

\CommentTok{\# Relaxed tolerance (larger value) {-} more fallbacks}
\NormalTok{optimizer\_relaxed }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    tolerance\_x}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,  }\CommentTok{\# Larger {-} triggers fallback more often}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Best Practices}\label{best-practices-1}

\subsection{1. Use Random for Most
Problems}\label{use-random-for-most-problems}

The random strategy (default) is sufficient for most optimization
problems:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"random"}  \CommentTok{\# Good default choice}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{2. Use Morris-Mitchell for Intensive
Sampling}\label{use-morris-mitchell-for-intensive-sampling}

When you have a large budget and want maximum exploration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{expensive\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{200}\NormalTok{,  }\CommentTok{\# Large budget}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}  \CommentTok{\# Maximize space coverage}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{3. Monitor Fallback
Activations}\label{monitor-fallback-activations}

Enable verbose mode to see when fallbacks are triggered:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}  \CommentTok{\# Shows fallback messages}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{4. Adjust Tolerance Based on Problem
Scale}\label{adjust-tolerance-based-on-problem-scale}

For problems with small search spaces, use smaller tolerance:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Small search space}
\NormalTok{optimizer\_small }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)],}
\NormalTok{    tolerance\_x}\OperatorTok{=}\FloatTok{0.01}\NormalTok{,  }\CommentTok{\# Small tolerance for small space}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"random"}
\NormalTok{)}

\CommentTok{\# Large search space}
\NormalTok{optimizer\_large }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{)],}
\NormalTok{    tolerance\_x}\OperatorTok{=}\FloatTok{1.0}\NormalTok{,  }\CommentTok{\# Larger tolerance for large space}
\NormalTok{    acquisition\_failure\_strategy}\OperatorTok{=}\StringTok{"mm"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Technical Details}\label{technical-details}

\subsection{Morris-Mitchell
Implementation}\label{morris-mitchell-implementation}

The Morris-Mitchell strategy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generates 100 candidate points using Latin Hypercube Sampling
\item
  For each candidate, calculates the minimum distance to all existing
  points
\item
  Selects the candidate with the maximum minimum distance
\end{enumerate}

This ensures the new point is as far as possible from the densest region
of evaluated points.

\subsection{Random Strategy
Implementation}\label{random-strategy-implementation}

The random strategy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generates a single point using Latin Hypercube Sampling
\item
  Ensures the point is within bounds
\item
  Applies variable type repairs (rounding for int/factor variables)
\end{enumerate}

This is computationally efficient while maintaining good space-filling
properties.

\section{Summary}\label{summary-1}

\begin{itemize}
\tightlist
\item
  \textbf{Default strategy} (\texttt{"random"}): Fast, good
  space-filling, suitable for most problems
\item
  \textbf{Morris-Mitchell} (\texttt{"mm"}): Better space-filling,
  maximizes minimum distance, ideal for intensive sampling
\item
  \textbf{Trigger}: Activated when acquisition-proposed point is too
  close to existing points (within \texttt{tolerance\_x})
\item
  \textbf{Control}: Set via \texttt{acquisition\_failure\_strategy}
  parameter
\item
  \textbf{Monitoring}: Enable \texttt{verbose=True} to see when
  fallbacks occur
\end{itemize}

Choose the strategy that best matches your optimization goals: - Use
\texttt{"random"} for general-purpose optimization - Use \texttt{"mm"}
when you want maximum exploration and have a generous function
evaluation budget

\chapter{Diabetes Dataset Utilities}\label{diabetes-dataset-utilities}

SpotOptim provides convenient utilities for working with the sklearn
diabetes dataset, including PyTorch \texttt{Dataset} and
\texttt{DataLoader} implementations. These utilities simplify data
loading, preprocessing, and model training for regression tasks.

\section{Overview}\label{overview}

The diabetes dataset contains 10 baseline variables (age, sex, body mass
index, average blood pressure, and six blood serum measurements) for 442
diabetes patients. The target is a quantitative measure of disease
progression one year after baseline.

\textbf{Module}: \texttt{spotoptim.data.diabetes}

\textbf{Key Components}:

\begin{itemize}
\tightlist
\item
  \texttt{DiabetesDataset}: PyTorch Dataset class
\item
  \texttt{get\_diabetes\_dataloaders()}: Convenience function for
  complete data pipeline
\end{itemize}

\section{Quick Start}\label{quick-start}

\subsection{Basic Usage}\label{basic-usage-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Load data with default settings}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders()}

\CommentTok{\# Iterate through batches}
\ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch features: }\SpecialCharTok{\{}\NormalTok{batch\_X}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# (32, 10)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batch targets: }\SpecialCharTok{\{}\NormalTok{batch\_y}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)   }\CommentTok{\# (32, 1)}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\subsection{Training a Model}\label{training-a-model}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}

\CommentTok{\# Load data}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\CommentTok{\# Create model}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{    input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{    l1}\OperatorTok{=}\DecValTok{64}\NormalTok{,}
\NormalTok{    num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    activation}\OperatorTok{=}\StringTok{"ReLU"}
\NormalTok{)}

\CommentTok{\# Setup training}
\NormalTok{criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\FloatTok{0.01}\NormalTok{)}

\CommentTok{\# Training loop}
\NormalTok{num\_epochs }\OperatorTok{=} \DecValTok{100}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
\NormalTok{    model.train()}
\NormalTok{    train\_loss }\OperatorTok{=} \FloatTok{0.0}
    
    \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
        \CommentTok{\# Forward pass}
\NormalTok{        predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
        
        \CommentTok{\# Backward pass}
\NormalTok{        optimizer.zero\_grad()}
\NormalTok{        loss.backward()}
\NormalTok{        optimizer.step()}
        
\NormalTok{        train\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
    
\NormalTok{    avg\_train\_loss }\OperatorTok{=}\NormalTok{ train\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(train\_loader)}
    
    \ControlFlowTok{if}\NormalTok{ (epoch }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{\%} \DecValTok{20} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{num\_epochs}\SpecialCharTok{\}}\SpecialStringTok{: Loss = }\SpecialCharTok{\{}\NormalTok{avg\_train\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Evaluation}
\NormalTok{model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{test\_loss }\OperatorTok{=} \FloatTok{0.0}

\ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
    \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{        predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{        test\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}

\NormalTok{avg\_test\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test MSE: }\SpecialCharTok{\{}\NormalTok{avg\_test\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Function Reference}\label{function-reference}

\subsection{get\_diabetes\_dataloaders()}\label{get_diabetes_dataloaders}

Loads the sklearn diabetes dataset and returns configured PyTorch
DataLoaders.

\textbf{Signature:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_diabetes\_dataloaders(}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{    shuffle\_train}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    shuffle\_test}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    num\_workers}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    pin\_memory}\OperatorTok{=}\VariableTok{False}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Parameters:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2821}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{test\_size} & float & 0.2 & Proportion of dataset for testing
(0.0 to 1.0) \\
\texttt{batch\_size} & int & 32 & Number of samples per batch \\
\texttt{shuffle\_train} & bool & True & Whether to shuffle training
data \\
\texttt{shuffle\_test} & bool & False & Whether to shuffle test data \\
\texttt{random\_state} & int & 42 & Random seed for train/test split \\
\texttt{scale\_features} & bool & True & Whether to standardize
features \\
\texttt{num\_workers} & int & 0 & Number of subprocesses for data
loading \\
\texttt{pin\_memory} & bool & False & Whether to pin memory (useful for
GPU) \\
\end{longtable}

\textbf{Returns:}

\begin{itemize}
\tightlist
\item
  \texttt{train\_loader} (DataLoader): Training data loader
\item
  \texttt{test\_loader} (DataLoader): Test data loader
\item
  \texttt{scaler} (StandardScaler or None): Fitted scaler if
  \texttt{scale\_features=True}, else None
\end{itemize}

\textbf{Example:}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Custom configuration}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.3}\NormalTok{,}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{64}\NormalTok{,}
\NormalTok{    shuffle\_train}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training batches: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test batches: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(test\_loader)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Scaler mean: }\SpecialCharTok{\{}\NormalTok{scaler}\SpecialCharTok{.}\NormalTok{mean\_[:}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# First 3 features}
\end{Highlighting}
\end{Shaded}

\section{DiabetesDataset Class}\label{diabetesdataset-class}

PyTorch Dataset implementation for the diabetes dataset.

\textbf{Signature:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiabetesDataset(X, y, transform}\OperatorTok{=}\VariableTok{None}\NormalTok{, target\_transform}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Parameters:}

\begin{itemize}
\tightlist
\item
  \texttt{X} (np.ndarray): Feature matrix of shape (n\_samples,
  n\_features)
\item
  \texttt{y} (np.ndarray): Target values of shape (n\_samples,) or
  (n\_samples, 1)
\item
  \texttt{transform} (callable, optional): Transform to apply to
  features
\item
  \texttt{target\_transform} (callable, optional): Transform to apply to
  targets
\end{itemize}

\textbf{Attributes:}

\begin{itemize}
\tightlist
\item
  \texttt{X} (torch.Tensor): Feature tensor (n\_samples, n\_features)
\item
  \texttt{y} (torch.Tensor): Target tensor (n\_samples, 1)
\item
  \texttt{n\_features} (int): Number of features (10 for diabetes)
\item
  \texttt{n\_samples} (int): Number of samples
\end{itemize}

\textbf{Methods:}

\begin{itemize}
\tightlist
\item
  \texttt{\_\_len\_\_()}: Returns number of samples
\item
  \texttt{\_\_getitem\_\_(idx)}: Returns tuple (features, target) for
  given index
\end{itemize}

\subsection{Manual Dataset Creation}\label{manual-dataset-creation}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ DiabetesDataset}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_diabetes}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}

\CommentTok{\# Load raw data}
\NormalTok{diabetes }\OperatorTok{=}\NormalTok{ load\_diabetes()}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ diabetes.data, diabetes.target}

\CommentTok{\# Split data}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{    X, y, test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\CommentTok{\# Scale features}
\NormalTok{scaler }\OperatorTok{=}\NormalTok{ StandardScaler()}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ scaler.fit\_transform(X\_train)}
\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ scaler.transform(X\_test)}

\CommentTok{\# Create datasets}
\NormalTok{train\_dataset }\OperatorTok{=}\NormalTok{ DiabetesDataset(X\_train, y\_train)}
\NormalTok{test\_dataset }\OperatorTok{=}\NormalTok{ DiabetesDataset(X\_test, y\_test)}

\CommentTok{\# Create dataloaders}
\NormalTok{train\_loader }\OperatorTok{=}\NormalTok{ DataLoader(train\_dataset, batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{, shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{test\_loader }\OperatorTok{=}\NormalTok{ DataLoader(test\_dataset, batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Inspect dataset}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dataset size: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Features shape: }\SpecialCharTok{\{}\NormalTok{train\_dataset}\SpecialCharTok{.}\NormalTok{X}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Targets shape: }\SpecialCharTok{\{}\NormalTok{train\_dataset}\SpecialCharTok{.}\NormalTok{y}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Get a sample}
\NormalTok{features, target }\OperatorTok{=}\NormalTok{ train\_dataset[}\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Sample features: }\SpecialCharTok{\{}\NormalTok{features}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# (10,)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Sample target: }\SpecialCharTok{\{}\NormalTok{target}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)      }\CommentTok{\# (1,)}
\end{Highlighting}
\end{Shaded}

\section{Advanced Usage}\label{advanced-usage}

\subsection{Custom Transforms}\label{custom-transforms}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ DiabetesDataset}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_diabetes}
\ImportTok{import}\NormalTok{ torch}

\CommentTok{\# Define custom transforms}
\KeywordTok{def}\NormalTok{ add\_noise(x):}
    \CommentTok{"""Add Gaussian noise to features."""}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{+}\NormalTok{ torch.randn\_like(x) }\OperatorTok{*} \FloatTok{0.01}

\KeywordTok{def}\NormalTok{ log\_transform(y):}
    \CommentTok{"""Apply log transform to target."""}
    \ControlFlowTok{return}\NormalTok{ torch.log1p(y)}

\CommentTok{\# Load data}
\NormalTok{diabetes }\OperatorTok{=}\NormalTok{ load\_diabetes()}
\NormalTok{X, y }\OperatorTok{=}\NormalTok{ diabetes.data, diabetes.target}

\CommentTok{\# Create dataset with transforms}
\NormalTok{dataset }\OperatorTok{=}\NormalTok{ DiabetesDataset(}
\NormalTok{    X, y,}
\NormalTok{    transform}\OperatorTok{=}\NormalTok{add\_noise,}
\NormalTok{    target\_transform}\OperatorTok{=}\NormalTok{log\_transform}
\NormalTok{)}

\CommentTok{\# Transforms are applied when accessing items}
\NormalTok{features, target }\OperatorTok{=}\NormalTok{ dataset[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\subsection{Different Train/Test
Splits}\label{different-traintest-splits}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# 70/30 split}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.3}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader.dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# \textasciitilde{}310}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(test\_loader.dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)       }\CommentTok{\# \textasciitilde{}132}

\CommentTok{\# 90/10 split}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    test\_size}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader.dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# \textasciitilde{}398}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(test\_loader.dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)       }\CommentTok{\# \textasciitilde{}44}
\end{Highlighting}
\end{Shaded}

\subsection{Without Feature Scaling}\label{without-feature-scaling}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Load without scaling (useful for tree{-}based models)}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{False}
\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Scaler: }\SpecialCharTok{\{}\NormalTok{scaler}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# None}

\CommentTok{\# Data is in original scale}
\ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Mean: }\SpecialCharTok{\{}\NormalTok{batch\_X}\SpecialCharTok{.}\NormalTok{mean(dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)[:}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# Non{-}zero values}
    \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\subsection{Larger Batch Sizes}\label{larger-batch-sizes}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Larger batches for faster training (if memory allows)}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{128}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batches per epoch: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# Fewer batches}

\CommentTok{\# Smaller batches for more gradient updates}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{8}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batches per epoch: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# More batches}
\end{Highlighting}
\end{Shaded}

\subsection{GPU Training with Pin
Memory}\label{gpu-training-with-pin-memory}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Enable pin\_memory for faster GPU transfer}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{    pin\_memory}\OperatorTok{=}\VariableTok{True}  \CommentTok{\# Set to True when using GPU}
\NormalTok{)}

\CommentTok{\# Move model to GPU}
\NormalTok{device }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{"cuda"} \ControlFlowTok{if}\NormalTok{ torch.cuda.is\_available() }\ControlFlowTok{else} \StringTok{"cpu"}\NormalTok{)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ model.to(device)}

\CommentTok{\# Training loop with GPU}
\ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
    \CommentTok{\# Data is already pinned, faster transfer to GPU}
\NormalTok{    batch\_X }\OperatorTok{=}\NormalTok{ batch\_X.to(device, non\_blocking}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    batch\_y }\OperatorTok{=}\NormalTok{ batch\_y.to(device, non\_blocking}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    
    \CommentTok{\# ... training code ...}
\end{Highlighting}
\end{Shaded}

\section{Complete Training Example}\label{complete-training-example}

Here's a complete example showing data loading, model training, and
evaluation:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}

\KeywordTok{def}\NormalTok{ train\_diabetes\_model():}
    \CommentTok{"""Train a neural network on the diabetes dataset."""}
    
    \CommentTok{\# Load data}
\NormalTok{    train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{        test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{        batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{        scale\_features}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{    )}
    
    \CommentTok{\# Create model}
\NormalTok{    model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{        input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{        output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{        l1}\OperatorTok{=}\DecValTok{128}\NormalTok{,}
\NormalTok{        num\_hidden\_layers}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{        activation}\OperatorTok{=}\StringTok{"ReLU"}
\NormalTok{    )}
    
    \CommentTok{\# Setup training}
\NormalTok{    criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\FloatTok{0.001}\NormalTok{, weight\_decay}\OperatorTok{=}\FloatTok{1e{-}5}\NormalTok{)}
    
    \CommentTok{\# Training configuration}
\NormalTok{    num\_epochs }\OperatorTok{=} \DecValTok{200}
\NormalTok{    best\_test\_loss }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)}
    
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Starting training..."}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader.dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test samples: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(test\_loader.dataset)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Batches per epoch: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(train\_loader)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{60}\NormalTok{)}
    
    \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
        \CommentTok{\# Training phase}
\NormalTok{        model.train()}
\NormalTok{        train\_loss }\OperatorTok{=} \FloatTok{0.0}
        
        \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
            \CommentTok{\# Forward pass}
\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
            
            \CommentTok{\# Backward pass}
\NormalTok{            optimizer.zero\_grad()}
\NormalTok{            loss.backward()}
\NormalTok{            optimizer.step()}
            
\NormalTok{            train\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
        
\NormalTok{        avg\_train\_loss }\OperatorTok{=}\NormalTok{ train\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(train\_loader)}
        
        \CommentTok{\# Evaluation phase}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
        
\NormalTok{        avg\_test\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
        
        \CommentTok{\# Track best model}
        \ControlFlowTok{if}\NormalTok{ avg\_test\_loss }\OperatorTok{\textless{}}\NormalTok{ best\_test\_loss:}
\NormalTok{            best\_test\_loss }\OperatorTok{=}\NormalTok{ avg\_test\_loss}
            \CommentTok{\# Could save model here: torch.save(model.state\_dict(), \textquotesingle{}best\_model.pt\textquotesingle{})}
        
        \CommentTok{\# Print progress}
        \ControlFlowTok{if}\NormalTok{ (epoch }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{\%} \DecValTok{20} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{:3d\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{num\_epochs}\SpecialCharTok{\}}\SpecialStringTok{: "}
                  \SpecialStringTok{f"Train Loss = }\SpecialCharTok{\{}\NormalTok{avg\_train\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{, "}
                  \SpecialStringTok{f"Test Loss = }\SpecialCharTok{\{}\NormalTok{avg\_test\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{60}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training complete!"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best test loss: }\SpecialCharTok{\{}\NormalTok{best\_test\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \ControlFlowTok{return}\NormalTok{ model, best\_test\_loss}

\CommentTok{\# Run training}
\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{"\_\_main\_\_"}\NormalTok{:}
\NormalTok{    model, best\_loss }\OperatorTok{=}\NormalTok{ train\_diabetes\_model()}
\end{Highlighting}
\end{Shaded}

\section{Integration with SpotOptim}\label{integration-with-spotoptim}

Use the diabetes dataset for hyperparameter optimization with SpotOptim:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}

\KeywordTok{def}\NormalTok{ evaluate\_model(X):}
    \CommentTok{"""Objective function for SpotOptim.}
\CommentTok{    }
\CommentTok{    Args:}
\CommentTok{        X: Array of hyperparameters [lr, l1, num\_hidden\_layers]}
\CommentTok{        }
\CommentTok{    Returns:}
\CommentTok{        Array of validation losses}
\CommentTok{    """}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        lr, l1, num\_hidden\_layers }\OperatorTok{=}\NormalTok{ params}
\NormalTok{        lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ lr  }\CommentTok{\# Log scale for learning rate}
\NormalTok{        l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(l1)}
\NormalTok{        num\_hidden\_layers }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(num\_hidden\_layers)}
        
        \CommentTok{\# Load data}
\NormalTok{        train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{            test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{            batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{            random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{        )}
        
        \CommentTok{\# Create model}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\NormalTok{l1,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\NormalTok{num\_hidden\_layers,}
\NormalTok{            activation}\OperatorTok{=}\StringTok{"ReLU"}
\NormalTok{        )}
        
        \CommentTok{\# Train briefly}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\NormalTok{lr)}
        
\NormalTok{        num\_epochs }\OperatorTok{=} \DecValTok{50}
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
\NormalTok{            model.train()}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluate}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
        
\NormalTok{        results.append(test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader))}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\CommentTok{\# Optimize hyperparameters}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{evaluate\_model,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{        (}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{),   }\CommentTok{\# log10(lr): 0.0001 to 0.01}
\NormalTok{        (}\DecValTok{16}\NormalTok{, }\DecValTok{128}\NormalTok{),  }\CommentTok{\# l1: number of neurons}
\NormalTok{        (}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{)      }\CommentTok{\# num\_hidden\_layers}
\NormalTok{    ],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"int"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best hyperparameters found:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Learning rate: }\SpecialCharTok{\{}\DecValTok{10}\OperatorTok{**}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Hidden neurons (l1): }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Hidden layers: }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best MSE: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Best Practices}\label{best-practices-2}

\subsection{1. Always Use Feature
Scaling}\label{always-use-feature-scaling}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good: Features are standardized}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Neural networks typically perform better with normalized inputs.

\subsection{2. Set Random Seeds for
Reproducibility}\label{set-random-seeds-for-reproducibility}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reproducible train/test splits}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\CommentTok{\# Also set PyTorch seed}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{torch.manual\_seed(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{3. Don't Shuffle Test Data}\label{dont-shuffle-test-data}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good: Test data in consistent order}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    shuffle\_train}\OperatorTok{=}\VariableTok{True}\NormalTok{,   }\CommentTok{\# Shuffle training data}
\NormalTok{    shuffle\_test}\OperatorTok{=}\VariableTok{False}    \CommentTok{\# Don\textquotesingle{}t shuffle test data}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ensures consistent evaluation metrics across runs.

\subsection{4. Choose Appropriate Batch
Size}\label{choose-appropriate-batch-size}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Small dataset (442 samples) {-} moderate batch size works well}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{32}  \CommentTok{\# Good balance for this dataset}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Too large: Fewer gradient updates per epoch\\
Too small: Noisy gradients, slower training

\subsection{5. Save the Scaler for
Production}\label{save-the-scaler-for-production}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pickle}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Train with scaling}
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\CommentTok{\# Save scaler for production use}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}scaler.pkl\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    pickle.dump(scaler, f)}

\CommentTok{\# Later: Load and use on new data}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}scaler.pkl\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}rb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    scaler }\OperatorTok{=}\NormalTok{ pickle.load(f)}

\NormalTok{new\_data\_scaled }\OperatorTok{=}\NormalTok{ scaler.transform(new\_data)}
\end{Highlighting}
\end{Shaded}

\section{Troubleshooting}\label{troubleshooting}

\subsection{Issue: Out of Memory}\label{issue-out-of-memory}

\textbf{Solution}: Reduce batch size or disable pin\_memory

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{16}\NormalTok{,      }\CommentTok{\# Smaller batches}
\NormalTok{    pin\_memory}\OperatorTok{=}\VariableTok{False}    \CommentTok{\# Disable if not using GPU}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Different Data
Ranges}\label{issue-different-data-ranges}

\textbf{Symptom}: Model not converging, loss is NaN

\textbf{Solution}: Ensure feature scaling is enabled

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    scale\_features}\OperatorTok{=}\VariableTok{True}  \CommentTok{\# Must be True for neural networks}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Non-Reproducible
Results}\label{issue-non-reproducible-results}

\textbf{Solution}: Set all random seeds

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Set all seeds}
\NormalTok{torch.manual\_seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}

\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    shuffle\_train}\OperatorTok{=}\VariableTok{False}  \CommentTok{\# Disable shuffle for full reproducibility}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Slow Data Loading}\label{issue-slow-data-loading}

\textbf{Solution}: Use multiple workers (if not on Windows)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train\_loader, test\_loader, scaler }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    num\_workers}\OperatorTok{=}\DecValTok{4}\NormalTok{,      }\CommentTok{\# Use 4 subprocesses}
\NormalTok{    pin\_memory}\OperatorTok{=}\VariableTok{True}     \CommentTok{\# Enable for GPU}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note: On Windows, set \texttt{num\_workers=0} to avoid multiprocessing
issues.

\section{Summary}\label{summary-2}

The diabetes dataset utilities in SpotOptim provide:

\begin{itemize}
\tightlist
\item
  \textbf{Easy data loading}: One function call gets complete data
  pipeline
\item
  \textbf{PyTorch integration}: Native Dataset and DataLoader support
\item
  \textbf{Preprocessing included}: Automatic feature scaling and
  train/test splitting
\item
  \textbf{Flexible configuration}: Control batch size, splitting,
  scaling, and more
\item
  \textbf{Production ready}: Save scalers and ensure reproducibility
\end{itemize}

For more examples, see: -
\texttt{examples/diabetes\_dataset\_example.py} -
\texttt{notebooks/demos.ipynb} - Test suite:
\texttt{tests/test\_diabetes\_dataset.py}

\chapter{Factor Variables for Categorical
Hyperparameters}\label{factor-variables-for-categorical-hyperparameters}

SpotOptim supports factor variables for optimizing categorical
hyperparameters, such as activation functions, optimizers, or any
discrete string-based choices. Factor variables are automatically
converted between string values (external interface) and integers
(internal optimization), making categorical optimization seamless.

\section{Overview}\label{overview-1}

\textbf{What are Factor Variables?}

Factor variables allow you to specify categorical choices as tuples of
strings in the bounds. SpotOptim handles the conversion:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{String tuples in bounds}  Internal integer mapping (0, 1, 2,
  \ldots)
\item
  \textbf{Optimization uses integers} internally for surrogate modeling
\item
  \textbf{Objective function receives strings} after automatic
  conversion
\item
  \textbf{Results return strings} (not integers)
\end{enumerate}

\textbf{Module}: \texttt{spotoptim.SpotOptim}

\textbf{Key Features}:

\begin{itemize}
\tightlist
\item
  Define categorical choices as string tuples:
  \texttt{("ReLU",\ "Sigmoid",\ "Tanh")}
\item
  Automatic integerstring conversion
\item
  Seamless integration with neural network hyperparameters
\item
  Mix factor variables with numeric/integer variables
\end{itemize}

\section{Quick Start}\label{quick-start-1}

\subsection{Basic Factor Variable
Usage}\label{basic-factor-variable-usage}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ objective\_function(X):}
    \CommentTok{"""Objective function receives string values."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# This is a string!}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Testing activation: }\SpecialCharTok{\{}\NormalTok{activation}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        
        \CommentTok{\# Simple scoring based on activation choice (for demonstration)}
        \CommentTok{\# In real use, you would train a model and return actual performance}
\NormalTok{        scores }\OperatorTok{=}\NormalTok{ \{}
            \StringTok{"ReLU"}\NormalTok{: }\FloatTok{3500.0}\NormalTok{,}
            \StringTok{"Sigmoid"}\NormalTok{: }\FloatTok{4200.0}\NormalTok{,}
            \StringTok{"Tanh"}\NormalTok{: }\FloatTok{3800.0}\NormalTok{,}
            \StringTok{"LeakyReLU"}\NormalTok{: }\FloatTok{3600.0}
\NormalTok{        \}}
\NormalTok{        score }\OperatorTok{=}\NormalTok{ scores.get(activation, }\FloatTok{5000.0}\NormalTok{) }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{        results.append(score)}
    \ControlFlowTok{return}\NormalTok{ np.array(results)  }\CommentTok{\# Return numpy array}

\CommentTok{\# Define bounds with factor variable}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective\_function,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{)],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"factor"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Best activation: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# Returns string, e.g., "ReLU"}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best score: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Neural Network Activation Function
Optimization}\label{neural-network-activation-function-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ train\_and\_evaluate(X):}
    \CommentTok{"""Train models with different activation functions."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# String: "ReLU", "Sigmoid", etc.}
        
        \CommentTok{\# Load data}
\NormalTok{        train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders()}
        
        \CommentTok{\# Create model with the activation function}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\DecValTok{64}\NormalTok{,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{            activation}\OperatorTok{=}\NormalTok{activation  }\CommentTok{\# Pass string directly!}
\NormalTok{        )}
        
        \CommentTok{\# Train model}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\FloatTok{0.01}\NormalTok{)}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{50}\NormalTok{):}
\NormalTok{            model.train()}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluate}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
        
\NormalTok{        avg\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\NormalTok{        results.append(avg\_loss)}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)  }\CommentTok{\# Return numpy array}

\CommentTok{\# Optimize activation function choice}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{train\_and\_evaluate,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{, }\StringTok{"ELU"}\NormalTok{)],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"factor"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best activation function: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best test MSE: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Mixed Variable Types}\label{mixed-variable-types}

\subsection{Combining Factor, Integer, and Continuous
Variables}\label{combining-factor-integer-and-continuous-variables}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}

\KeywordTok{def}\NormalTok{ comprehensive\_optimization(X):}
    \CommentTok{"""Optimize learning rate, layer size, depth, and activation."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        log\_lr }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]      }\CommentTok{\# Continuous (log scale)}
\NormalTok{        l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{1}\NormalTok{])     }\CommentTok{\# Integer}
\NormalTok{        n\_layers }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{2}\NormalTok{])  }\CommentTok{\# Integer}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{3}\NormalTok{]   }\CommentTok{\# Factor (string)}
        
\NormalTok{        lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ log\_lr  }\CommentTok{\# Convert from log scale}
        
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"lr=}\SpecialCharTok{\{}\NormalTok{lr}\SpecialCharTok{:.6f\}}\SpecialStringTok{, l1=}\SpecialCharTok{\{}\NormalTok{l1}\SpecialCharTok{\}}\SpecialStringTok{, layers=}\SpecialCharTok{\{}\NormalTok{n\_layers}\SpecialCharTok{\}}\SpecialStringTok{, activation=}\SpecialCharTok{\{}\NormalTok{activation}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        
        \CommentTok{\# Load data}
\NormalTok{        train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{            batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{            random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{        )}
        
        \CommentTok{\# Create model}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\NormalTok{l1,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\NormalTok{n\_layers,}
\NormalTok{            activation}\OperatorTok{=}\NormalTok{activation}
\NormalTok{        )}
        
        \CommentTok{\# Train}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\NormalTok{lr)}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{30}\NormalTok{):}
\NormalTok{            model.train()}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluate}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
        
\NormalTok{        results.append(test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader))}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\CommentTok{\# Optimize all four hyperparameters simultaneously}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{comprehensive\_optimization,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{        (}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{),                                    }\CommentTok{\# log10(learning\_rate)}
\NormalTok{        (}\DecValTok{16}\NormalTok{, }\DecValTok{128}\NormalTok{),                                   }\CommentTok{\# l1 (neurons per layer)}
\NormalTok{        (}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{),                                      }\CommentTok{\# num\_hidden\_layers}
\NormalTok{        (}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{)   }\CommentTok{\# activation function}
\NormalTok{    ],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Results contain original string values}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Optimization Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best learning rate: }\SpecialCharTok{\{}\DecValTok{10}\OperatorTok{**}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best layer size: }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best num layers: }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best activation: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# String value!}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best test MSE: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Multiple Factor Variables}\label{multiple-factor-variables}

\subsection{Optimizing Both Activation and
Optimizer}\label{optimizing-both-activation-and-optimizer}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ optimize\_activation\_and\_optimizer(X):}
    \CommentTok{"""Optimize both activation function and optimizer choice."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]      }\CommentTok{\# Factor variable 1}
\NormalTok{        optimizer\_name }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# Factor variable 2}
\NormalTok{        lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ params[}\DecValTok{2}\NormalTok{]        }\CommentTok{\# Continuous variable}
        
\NormalTok{        train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders()}
        
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\DecValTok{64}\NormalTok{,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{            activation}\OperatorTok{=}\NormalTok{activation}
\NormalTok{        )}
        
        \CommentTok{\# Use the optimizer string}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(optimizer\_name, lr}\OperatorTok{=}\NormalTok{lr)}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \CommentTok{\# Train}
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{30}\NormalTok{):}
\NormalTok{            model.train()}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluate}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
        
\NormalTok{        results.append(test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader))}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)  }\CommentTok{\# Return numpy array}

\CommentTok{\# Two factor variables + one continuous}
\NormalTok{opt }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{optimize\_activation\_and\_optimizer,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{        (}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{),    }\CommentTok{\# Activation}
\NormalTok{        (}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{, }\StringTok{"AdamW"}\NormalTok{),         }\CommentTok{\# Optimizer}
\NormalTok{        (}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{)                                      }\CommentTok{\# log10(lr)}
\NormalTok{    ],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"factor"}\NormalTok{, }\StringTok{"factor"}\NormalTok{, }\StringTok{"num"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{40}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ opt.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best activation: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best optimizer: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best learning rate: }\SpecialCharTok{\{}\DecValTok{10}\OperatorTok{**}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Advanced Usage}\label{advanced-usage-1}

\subsection{Custom Categorical
Choices}\label{custom-categorical-choices}

Factor variables work with any string values, not just activation
functions:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ train\_model\_with\_config(dropout\_policy, batch\_norm, weight\_init):}
    \CommentTok{"""Simulate model training with different configurations."""}
    \CommentTok{\# In real use, this would train an actual model}
    \CommentTok{\# Here we return synthetic scores for demonstration}
\NormalTok{    base\_score }\OperatorTok{=} \FloatTok{3000.0}
    
    \CommentTok{\# Dropout impact}
\NormalTok{    dropout\_scores }\OperatorTok{=}\NormalTok{ \{}\StringTok{"none"}\NormalTok{: }\DecValTok{200}\NormalTok{, }\StringTok{"light"}\NormalTok{: }\DecValTok{0}\NormalTok{, }\StringTok{"heavy"}\NormalTok{: }\DecValTok{100}\NormalTok{\}}
    \CommentTok{\# Batch norm impact}
\NormalTok{    bn\_scores }\OperatorTok{=}\NormalTok{ \{}\StringTok{"before"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{50}\NormalTok{, }\StringTok{"after"}\NormalTok{: }\DecValTok{0}\NormalTok{, }\StringTok{"none"}\NormalTok{: }\DecValTok{150}\NormalTok{\}}
    \CommentTok{\# Weight init impact}
\NormalTok{    init\_scores }\OperatorTok{=}\NormalTok{ \{}\StringTok{"xavier"}\NormalTok{: }\DecValTok{0}\NormalTok{, }\StringTok{"kaiming"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{30}\NormalTok{, }\StringTok{"normal"}\NormalTok{: }\DecValTok{100}\NormalTok{\}}
    
\NormalTok{    score }\OperatorTok{=}\NormalTok{ (base\_score }\OperatorTok{+} 
\NormalTok{             dropout\_scores.get(dropout\_policy, }\DecValTok{0}\NormalTok{) }\OperatorTok{+} 
\NormalTok{             bn\_scores.get(batch\_norm, }\DecValTok{0}\NormalTok{) }\OperatorTok{+} 
\NormalTok{             init\_scores.get(weight\_init, }\DecValTok{0}\NormalTok{) }\OperatorTok{+}
\NormalTok{             np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))}
    
    \ControlFlowTok{return}\NormalTok{ score}

\KeywordTok{def}\NormalTok{ train\_with\_config(X):}
    \CommentTok{"""Objective function with various categorical choices."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        dropout\_policy }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# "none", "light", "heavy"}
\NormalTok{        batch\_norm }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]       }\CommentTok{\# "before", "after", "none"}
\NormalTok{        weight\_init }\OperatorTok{=}\NormalTok{ params[}\DecValTok{2}\NormalTok{]      }\CommentTok{\# "xavier", "kaiming", "normal"}
        
        \CommentTok{\# Use these strings to configure your model}
\NormalTok{        score }\OperatorTok{=}\NormalTok{ train\_model\_with\_config(}
\NormalTok{            dropout\_policy}\OperatorTok{=}\NormalTok{dropout\_policy,}
\NormalTok{            batch\_norm}\OperatorTok{=}\NormalTok{batch\_norm,}
\NormalTok{            weight\_init}\OperatorTok{=}\NormalTok{weight\_init}
\NormalTok{        )}
\NormalTok{        results.append(score)}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)  }\CommentTok{\# Return numpy array}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{train\_with\_config,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{        (}\StringTok{"none"}\NormalTok{, }\StringTok{"light"}\NormalTok{, }\StringTok{"heavy"}\NormalTok{),           }\CommentTok{\# Dropout policy}
\NormalTok{        (}\StringTok{"before"}\NormalTok{, }\StringTok{"after"}\NormalTok{, }\StringTok{"none"}\NormalTok{),          }\CommentTok{\# Batch norm position}
\NormalTok{        (}\StringTok{"xavier"}\NormalTok{, }\StringTok{"kaiming"}\NormalTok{, }\StringTok{"normal"}\NormalTok{)       }\CommentTok{\# Weight initialization}
\NormalTok{    ],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"factor"}\NormalTok{, }\StringTok{"factor"}\NormalTok{, }\StringTok{"factor"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{25}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best configuration:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Dropout: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Batch norm: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Weight init: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Score: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Viewing All Evaluated
Configurations}\label{viewing-all-evaluated-configurations}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ train\_and\_evaluate(X):}
    \CommentTok{"""Train models with different activation functions."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{0}\NormalTok{])         }\CommentTok{\# Integer: layer size}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]       }\CommentTok{\# String: activation function}
        
        \CommentTok{\# Load data}
\NormalTok{        train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders()}
        
        \CommentTok{\# Create model with the activation function}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\NormalTok{l1,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{            activation}\OperatorTok{=}\NormalTok{activation  }\CommentTok{\# Pass string directly!}
\NormalTok{        )}
        
        \CommentTok{\# Train model}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\FloatTok{0.01}\NormalTok{)}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{50}\NormalTok{):}
\NormalTok{            model.train()}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluate}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
        
\NormalTok{        avg\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\NormalTok{        results.append(avg\_loss)}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{train\_and\_evaluate,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{        (}\DecValTok{16}\NormalTok{, }\DecValTok{128}\NormalTok{),                                   }\CommentTok{\# Layer size}
\NormalTok{        (}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{)   }\CommentTok{\# Activation}
\NormalTok{    ],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{],  }\CommentTok{\# IMPORTANT: Specify variable types!}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Access all evaluated configurations}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{All evaluated configurations:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Layer Size | Activation | Test MSE"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{42}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{min}\NormalTok{(}\DecValTok{10}\NormalTok{, }\BuiltInTok{len}\NormalTok{(result.X))):  }\CommentTok{\# Show first 10}
\NormalTok{    l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(result.X[i, }\DecValTok{0}\NormalTok{])}
\NormalTok{    activation }\OperatorTok{=}\NormalTok{ result.X[i, }\DecValTok{1}\NormalTok{]  }\CommentTok{\# String value!}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ result.y[i]}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{l1}\SpecialCharTok{:10d\}}\SpecialStringTok{ | }\SpecialCharTok{\{}\NormalTok{activation}\SpecialCharTok{:10s\}}\SpecialStringTok{ | }\SpecialCharTok{\{}\NormalTok{loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Find top 5 configurations}
\NormalTok{sorted\_indices }\OperatorTok{=}\NormalTok{ result.y.argsort()[:}\DecValTok{5}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Top 5 configurations:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ idx }\KeywordTok{in}\NormalTok{ sorted\_indices:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"l1=}\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.X[idx, }\DecValTok{0}\NormalTok{])}\SpecialCharTok{:3d\}}\SpecialStringTok{, "}
          \SpecialStringTok{f"activation=}\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{X[idx, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{:10s\}}\SpecialStringTok{, "}
          \SpecialStringTok{f"MSE=}\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{y[idx]}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{How It Works}\label{how-it-works-1}

\subsection{Internal Mechanism}\label{internal-mechanism}

SpotOptim handles factor variables through automatic conversion:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initialization}: String tuples in bounds are detected

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{)]}
\CommentTok{\# Internally mapped to: \{0: "ReLU", 1: "Sigmoid", 2: "Tanh"\}}
\CommentTok{\# Bounds become: [(0, 2)]}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Sampling}: Initial design samples from
  \texttt{{[}0,\ n\_levels-1{]}} and rounds to integers

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Samples might be: [0.3, 1.8, 2.1]}
\CommentTok{\# After rounding: [0, 2, 2]}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Evaluation}: Before calling objective function, integers 
  strings

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# [0, 2, 2]  ["ReLU", "Tanh", "Tanh"]}
\CommentTok{\# Objective function receives strings}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Optimization}: Surrogate model works with integers
  \texttt{{[}0,\ n\_levels-1{]}}
\item
  \textbf{Results}: Final results mapped back to strings

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result.x[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# Returns "ReLU", not 0}
\NormalTok{result.X     }\CommentTok{\# All rows contain strings for factor variables}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\subsection{Variable Type
Auto-Detection}\label{variable-type-auto-detection}

If you don't specify \texttt{var\_type}, SpotOptim automatically detects
factor variables:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example 1: Explicit var\_type (recommended)}
\CommentTok{\# This shows the syntax {-} replace my\_function with your actual function}

\CommentTok{\# optimizer = SpotOptim(}
\CommentTok{\#     fun=my\_function,}
\CommentTok{\#     bounds=[({-}4, {-}2), ("ReLU", "Tanh")],}
\CommentTok{\#     var\_type=["num", "factor"]  \# Explicit}
\CommentTok{\# )}

\CommentTok{\# Example 2: Auto{-}detection (works but less explicit)}
\CommentTok{\# optimizer = SpotOptim(}
\CommentTok{\#     fun=my\_function,}
\CommentTok{\#     bounds=[({-}4, {-}2), ("ReLU", "Tanh")]}
\CommentTok{\#     \# var\_type automatically set to ["float", "factor"]}
\CommentTok{\# )}

\CommentTok{\# Here\textquotesingle{}s a working example:}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ demo\_function(X):}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ params[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# Continuous parameter}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# Factor parameter}
\NormalTok{        score }\OperatorTok{=} \DecValTok{3000} \OperatorTok{+}\NormalTok{ lr }\OperatorTok{*} \DecValTok{100} \OperatorTok{+}\NormalTok{ \{}\StringTok{"ReLU"}\NormalTok{: }\DecValTok{0}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{: }\DecValTok{50}\NormalTok{\}.get(activation, }\DecValTok{100}\NormalTok{)}
\NormalTok{        results.append(score }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{))}
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\CommentTok{\# With explicit var\_type (recommended)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{demo\_function,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{), (}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{)],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"factor"}\NormalTok{],  }\CommentTok{\# Explicit is clearer}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best lr: }\SpecialCharTok{\{}\DecValTok{10}\OperatorTok{**}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{, Best activation: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Complete Example: Full
Workflow}\label{complete-example-full-workflow}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{"""}
\CommentTok{Complete example: Neural network hyperparameter optimization with factor variables.}
\CommentTok{"""}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}


\KeywordTok{def}\NormalTok{ objective\_function(X):}
    \CommentTok{"""Train and evaluate models with given hyperparameters."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
        \CommentTok{\# Extract hyperparameters}
\NormalTok{        log\_lr }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]}
\NormalTok{        l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{1}\NormalTok{])}
\NormalTok{        num\_layers }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{2}\NormalTok{])}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{3}\NormalTok{]  }\CommentTok{\# String!}
        
\NormalTok{        lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ log\_lr}
        
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Testing: lr=}\SpecialCharTok{\{}\NormalTok{lr}\SpecialCharTok{:.6f\}}\SpecialStringTok{, l1=}\SpecialCharTok{\{}\NormalTok{l1}\SpecialCharTok{\}}\SpecialStringTok{, layers=}\SpecialCharTok{\{}\NormalTok{num\_layers}\SpecialCharTok{\}}\SpecialStringTok{, "}
              \SpecialStringTok{f"activation=}\SpecialCharTok{\{}\NormalTok{activation}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
        
        \CommentTok{\# Load data}
\NormalTok{        train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{            test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{            batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{            random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{        )}
        
        \CommentTok{\# Create and train model}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\NormalTok{l1,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\NormalTok{num\_layers,}
\NormalTok{            activation}\OperatorTok{=}\NormalTok{activation}
\NormalTok{        )}
        
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\NormalTok{lr)}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \CommentTok{\# Training loop}
\NormalTok{        num\_epochs }\OperatorTok{=} \DecValTok{30}
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
\NormalTok{            model.train()}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluation}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
        
\NormalTok{        avg\_test\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\NormalTok{        results.append(avg\_test\_loss)}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"   Test MSE: }\SpecialCharTok{\{}\NormalTok{avg\_test\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)}


\KeywordTok{def}\NormalTok{ main():}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Neural Network Hyperparameter Optimization with Factor Variables"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    
    \CommentTok{\# Define optimization problem}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{        fun}\OperatorTok{=}\NormalTok{objective\_function,}
\NormalTok{        bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{            (}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{),                                    }\CommentTok{\# log10(learning\_rate)}
\NormalTok{            (}\DecValTok{16}\NormalTok{, }\DecValTok{128}\NormalTok{),                                   }\CommentTok{\# l1 (neurons)}
\NormalTok{            (}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{),                                      }\CommentTok{\# num\_hidden\_layers}
\NormalTok{            (}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{, }\StringTok{"LeakyReLU"}\NormalTok{)   }\CommentTok{\# activation (factor!)}
\NormalTok{        ],}
\NormalTok{        var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{],}
\NormalTok{        max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{        seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{    )}
    
    \CommentTok{\# Run optimization}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Starting optimization..."}\NormalTok{)}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
    
    \CommentTok{\# Display results}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"} \OperatorTok{+} \StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"OPTIMIZATION RESULTS"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best learning rate: }\SpecialCharTok{\{}\DecValTok{10}\OperatorTok{**}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best layer size (l1): }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best num hidden layers: }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best activation function: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# String value!}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best test MSE: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \CommentTok{\# Show top 5 configurations}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"} \OperatorTok{+} \StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"TOP 5 CONFIGURATIONS"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
\NormalTok{    sorted\_indices }\OperatorTok{=}\NormalTok{ result.y.argsort()[:}\DecValTok{5}\NormalTok{]}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\StringTok{\textquotesingle{}Rank\textquotesingle{}}\SpecialCharTok{:\textless{}6\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}LR\textquotesingle{}}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}L1\textquotesingle{}}\SpecialCharTok{:\textless{}6\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}Layers\textquotesingle{}}\SpecialCharTok{:\textless{}8\}}\SpecialStringTok{ "}
          \SpecialStringTok{f"}\SpecialCharTok{\{}\StringTok{\textquotesingle{}Activation\textquotesingle{}}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}MSE\textquotesingle{}}\SpecialCharTok{:\textless{}10\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ rank, idx }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(sorted\_indices, }\DecValTok{1}\NormalTok{):}
\NormalTok{        lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ result.X[idx, }\DecValTok{0}\NormalTok{]}
\NormalTok{        l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(result.X[idx, }\DecValTok{1}\NormalTok{])}
\NormalTok{        layers }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(result.X[idx, }\DecValTok{2}\NormalTok{])}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ result.X[idx, }\DecValTok{3}\NormalTok{]}
\NormalTok{        mse }\OperatorTok{=}\NormalTok{ result.y[idx]}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{rank}\SpecialCharTok{:\textless{}6\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{lr}\SpecialCharTok{:\textless{}12.6f\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{l1}\SpecialCharTok{:\textless{}6\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{layers}\SpecialCharTok{:\textless{}8\}}\SpecialStringTok{ "}
              \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{activation}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{mse}\SpecialCharTok{:\textless{}10.4f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \CommentTok{\# Train final model with best configuration}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"} \OperatorTok{+} \StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"TRAINING FINAL MODEL"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}
    
\NormalTok{    best\_lr }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ result.x[}\DecValTok{0}\NormalTok{]}
\NormalTok{    best\_l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{1}\NormalTok{])}
\NormalTok{    best\_layers }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{2}\NormalTok{])}
\NormalTok{    best\_activation }\OperatorTok{=}\NormalTok{ result.x[}\DecValTok{3}\NormalTok{]}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Configuration: lr=}\SpecialCharTok{\{}\NormalTok{best\_lr}\SpecialCharTok{:.6f\}}\SpecialStringTok{, l1=}\SpecialCharTok{\{}\NormalTok{best\_l1}\SpecialCharTok{\}}\SpecialStringTok{, "}
          \SpecialStringTok{f"layers=}\SpecialCharTok{\{}\NormalTok{best\_layers}\SpecialCharTok{\}}\SpecialStringTok{, activation=}\SpecialCharTok{\{}\NormalTok{best\_activation}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    
\NormalTok{    train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{        test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{        batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{,}
\NormalTok{        random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{    )}
    
\NormalTok{    final\_model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{        input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{        output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{        l1}\OperatorTok{=}\NormalTok{best\_l1,}
\NormalTok{        num\_hidden\_layers}\OperatorTok{=}\NormalTok{best\_layers,}
\NormalTok{        activation}\OperatorTok{=}\NormalTok{best\_activation}
\NormalTok{    )}
    
\NormalTok{    optimizer\_final }\OperatorTok{=}\NormalTok{ final\_model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\NormalTok{best\_lr)}
\NormalTok{    criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
    
    \CommentTok{\# Extended training}
\NormalTok{    num\_epochs }\OperatorTok{=} \DecValTok{100}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Training for }\SpecialCharTok{\{}\NormalTok{num\_epochs}\SpecialCharTok{\}}\SpecialStringTok{ epochs..."}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
\NormalTok{        final\_model.train()}
\NormalTok{        train\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ final\_model(batch\_X)}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{            optimizer\_final.zero\_grad()}
\NormalTok{            loss.backward()}
\NormalTok{            optimizer\_final.step()}
\NormalTok{            train\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
        
        \ControlFlowTok{if}\NormalTok{ (epoch }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{\%} \DecValTok{20} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
\NormalTok{            avg\_train\_loss }\OperatorTok{=}\NormalTok{ train\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(train\_loader)}
            \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{/}\SpecialCharTok{\{}\NormalTok{num\_epochs}\SpecialCharTok{\}}\SpecialStringTok{: Train MSE = }\SpecialCharTok{\{}\NormalTok{avg\_train\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \CommentTok{\# Final evaluation}
\NormalTok{    final\_model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{    final\_test\_loss }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
        \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ final\_model(batch\_X)}
\NormalTok{            final\_test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
    
\NormalTok{    final\_avg\_loss }\OperatorTok{=}\NormalTok{ final\_test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Final Test MSE: }\SpecialCharTok{\{}\NormalTok{final\_avg\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{80}\NormalTok{)}


\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{"\_\_main\_\_"}\NormalTok{:}
\NormalTok{    main()}
\end{Highlighting}
\end{Shaded}

\section{Best Practices}\label{best-practices-3}

\subsection{Do's}\label{dos}

 \textbf{Use descriptive string values}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bounds}\OperatorTok{=}\NormalTok{[(}\StringTok{"xavier\_uniform"}\NormalTok{, }\StringTok{"kaiming\_normal"}\NormalTok{, }\StringTok{"orthogonal"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

 \textbf{Explicitly specify var\_type for clarity}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

 \textbf{Access results as strings}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Accessing factor variable results as strings}
\CommentTok{\# (This assumes you\textquotesingle{}ve run an optimization with activation as a factor variable)}

\CommentTok{\# If you have a result from the previous examples:}
\CommentTok{\# best\_activation = result.x[3]  \# For 4{-}parameter optimization}
\CommentTok{\# Or for simpler cases:}
\CommentTok{\# best\_activation = result.x[0]  \# For single{-}parameter optimization}

\CommentTok{\# Example with inline optimization:}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ quick\_test(X):}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        activation }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]}
\NormalTok{        score }\OperatorTok{=}\NormalTok{ \{}\StringTok{"ReLU"}\NormalTok{: }\DecValTok{3500}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{: }\DecValTok{3600}\NormalTok{\}.get(activation, }\DecValTok{4000}\NormalTok{)}
\NormalTok{        results.append(score }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))}
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\NormalTok{opt }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{quick\_test,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{)],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"factor"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ opt.optimize()}

\CommentTok{\# Access as string {-} this is the correct way}
\NormalTok{best\_activation }\OperatorTok{=}\NormalTok{ result.x[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# String value like "ReLU"}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best activation: }\SpecialCharTok{\{}\NormalTok{best\_activation}\SpecialCharTok{\}}\SpecialStringTok{ (type: }\SpecialCharTok{\{}\BuiltInTok{type}\NormalTok{(best\_activation)}\SpecialCharTok{.}\VariableTok{\_\_name\_\_}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{)}

\CommentTok{\# You can use it directly in your model}
\CommentTok{\# model = LinearRegressor(activation=best\_activation)}
\end{Highlighting}
\end{Shaded}

 \textbf{Mix factor variables with numeric/integer variables}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{), (}\DecValTok{16}\NormalTok{, }\DecValTok{128}\NormalTok{), (}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{)]}
\NormalTok{var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\subsection{Don'ts}\label{donts}

 \textbf{Don't use integers in factor bounds}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wrong: Use strings, not integers}
\NormalTok{bounds}\OperatorTok{=}\NormalTok{[(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)]  }\CommentTok{\# Wrong!}
\NormalTok{bounds}\OperatorTok{=}\NormalTok{[(}\StringTok{"ReLU"}\NormalTok{, }\StringTok{"Sigmoid"}\NormalTok{, }\StringTok{"Tanh"}\NormalTok{)]  }\CommentTok{\# Correct!}
\end{Highlighting}
\end{Shaded}

 \textbf{Don't expect integers in objective function}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ objective(X):}
\NormalTok{    activation }\OperatorTok{=}\NormalTok{ X[}\DecValTok{0}\NormalTok{][}\DecValTok{2}\NormalTok{]}
    \CommentTok{\# activation is a string, not an integer!}
    \CommentTok{\# Don\textquotesingle{}t do: if activation == 0:  \# Wrong!}
    \CommentTok{\# Do: if activation == "ReLU":   \# Correct!}
\end{Highlighting}
\end{Shaded}

 \textbf{Don't manually convert factor variables}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# SpotOptim handles conversion automatically}
\CommentTok{\# Don\textquotesingle{}t do manual mapping in your objective function}
\end{Highlighting}
\end{Shaded}

 \textbf{Don't use empty tuples}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wrong: Empty tuple}
\NormalTok{bounds}\OperatorTok{=}\NormalTok{[()]}

\CommentTok{\# Correct: At least one string}
\NormalTok{bounds}\OperatorTok{=}\NormalTok{[(}\StringTok{"ReLU"}\NormalTok{,)]  }\CommentTok{\# Single choice (will be treated as fixed)}
\end{Highlighting}
\end{Shaded}

\section{Troubleshooting}\label{troubleshooting-1}

\subsection{Common Issues}\label{common-issues}

\textbf{Issue}: Objective function receives integers instead of strings

\textbf{Solution}: Ensure you're using the latest version of SpotOptim
with factor variable support. Factor variables are automatically
converted before calling the objective function.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Issue}:
\texttt{ValueError:\ could\ not\ convert\ string\ to\ float}

\textbf{Solution}: This occurs if there's a version mismatch. Update
SpotOptim to ensure the object array conversion is implemented
correctly.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Issue}: Results show integers instead of strings

\textbf{Solution}: Check that you're accessing \texttt{result.x} (mapped
values) instead of internal arrays. The result object automatically maps
factor variables to their original strings.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Issue}: Single-level factor variables cause dimension reduction

\textbf{Behavior}: If a factor variable has only one choice, e.g.,
\texttt{("ReLU",)}, SpotOptim treats it as a fixed dimension and may
reduce the dimensionality. This is expected behavior.

\textbf{Solution}: Use at least two choices for optimization, or remove
single-choice dimensions from bounds.

\section{Summary}\label{summary-3}

Factor variables in SpotOptim enable:

\begin{itemize}
\tightlist
\item
   \textbf{Categorical optimization}: Optimize over discrete string
  choices
\item
   \textbf{Automatic conversion}: Seamless integerstring mapping
\item
   \textbf{Neural network hyperparameters}: Optimize activation
  functions, optimizers, etc.
\item
   \textbf{Mixed variable types}: Combine with continuous and integer
  variables
\item
   \textbf{Clean interface}: Objective functions work with strings
  directly
\item
   \textbf{String results}: Final results contain original string
  values
\end{itemize}

Factor variables make categorical hyperparameter optimization as easy as
continuous optimization!

\section{See Also}\label{see-also}

\begin{itemize}
\tightlist
\item
  \href{https://sequential-parameter-optimization.github.io/spotPython/reference/}{LinearRegressor
  Documentation} - Neural network class supporting string-based
  activation functions
\item
  \href{diabetes_dataset.md}{Diabetes Dataset Utilities} - Data loading
  utilities used in examples
\item
  \href{var_type.md}{Variable Types} - Overview of all variable types in
  SpotOptim
\item
  \href{save_load.md}{Save and Load} - Saving and loading optimization
  results with factor variables
\end{itemize}

\chapter{Kriging Surrogate Integration
Summary}\label{kriging-surrogate-integration-summary}

\section{Overview}\label{overview-2}

Implementation of a Kriging (Gaussian Process) surrogate model to
SpotOptim, providing an alternative to scikit-learn's
GaussianProcessRegressor.

\section{Module Structure}\label{module-structure}

\begin{verbatim}
src/spotoptim/surrogate/
 __init__.py          # Module exports
 kriging.py           # Kriging implementation (~350 lines)
 README.md            # Module documentation
\end{verbatim}

\section{\texorpdfstring{Kriging Class
(\texttt{src/spotoptim/surrogate/kriging.py})}{Kriging Class (src/spotoptim/surrogate/kriging.py)}}\label{kriging-class-srcspotoptimsurrogatekriging.py}

\textbf{Key Features:}

\begin{itemize}
\tightlist
\item
  Scikit-learn compatible interface (\texttt{fit()}, \texttt{predict()})
\item
  Gaussian (RBF) kernel: R = exp(-D)
\item
  Automatic hyperparameter optimization via maximum likelihood
\item
  Cholesky decomposition for efficient linear algebra
\item
  Prediction with uncertainty (\texttt{return\_std=True})
\item
  Reproducible results via seed parameter
\end{itemize}

\textbf{Implementation Details:}

\begin{itemize}
\tightlist
\item
  lean, well-documented code
\item
  No external dependencies beyond NumPy, SciPy
\item
  Simplified from spotpython.surrogate.kriging
\item
  Focused on core functionality needed for SpotOptim
\end{itemize}

\textbf{Parameters:}

\begin{itemize}
\tightlist
\item
  \texttt{noise}: Regularization (nugget effect)
\item
  \texttt{kernel}: Currently `gauss' (Gaussian/RBF)
\item
  \texttt{n\_theta}: Number of length scale parameters
\item
  \texttt{min\_theta}, \texttt{max\_theta}: Bounds for hyperparameter
  optimization
\item
  \texttt{seed}: Random seed for reproducibility
\end{itemize}

\section{Integration with SpotOptim}\label{integration-with-spotoptim-1}

\textbf{No Changes Required to SpotOptim Core!}

The existing \texttt{surrogate} parameter already supports any
scikit-learn compatible model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim, Kriging}

\NormalTok{kriging }\OperatorTok{=}\NormalTok{ Kriging(seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    surrogate}\OperatorTok{=}\NormalTok{kriging,  }\CommentTok{\# Just pass the Kriging instance}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Documentation}\label{documentation}

Added Example to \texttt{notebooks/demos.ipynb}

\begin{itemize}
\tightlist
\item
  Demonstrates Kriging vs GP comparison
\item
  Shows custom parameter usage
\end{itemize}

\section{Usage Examples}\label{usage-examples}

\subsection{Basic Usage}\label{basic-usage-2}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim, Kriging}

\NormalTok{kriging }\OperatorTok{=}\NormalTok{ Kriging(noise}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{objective, bounds}\OperatorTok{=}\NormalTok{bounds, surrogate}\OperatorTok{=}\NormalTok{kriging)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{Custom Parameters}\label{custom-parameters}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kriging }\OperatorTok{=}\NormalTok{ Kriging(}
\NormalTok{    noise}\OperatorTok{=}\FloatTok{1e{-}4}\NormalTok{,}
\NormalTok{    min\_theta}\OperatorTok{={-}}\FloatTok{2.0}\NormalTok{,}
\NormalTok{    max\_theta}\OperatorTok{=}\FloatTok{3.0}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{123}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Prediction with
Uncertainty}\label{prediction-with-uncertainty}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ Kriging(seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{model.fit(X\_train, y\_train)}
\NormalTok{y\_pred, y\_std }\OperatorTok{=}\NormalTok{ model.predict(X\_test, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Technical Details}\label{technical-details-1}

\subsection{Kriging vs
GaussianProcessRegressor}\label{kriging-vs-gaussianprocessregressor}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & Kriging & GaussianProcessRegressor \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Lines of code & \textasciitilde350 & Complex internal implementation \\
Dependencies & NumPy, SciPy & scikit-learn + dependencies \\
Kernel & Gaussian (RBF) & Multiple types (Matern, RQ, etc.) \\
Hyperparameter opt & Differential Evolution & L-BFGS-B with restarts \\
Use case & Simplified, explicit & Production, flexible \\
\end{longtable}

\subsection{Algorithm}\label{algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Correlation Matrix:}

  \begin{itemize}
  \tightlist
  \item
    Compute squared distances: D\_ij = \_k \_k(x\_ik - x\_jk)
  \item
    Apply kernel: R\_ij = exp(-D\_ij)
  \item
    Add nugget: R\_ii += noise
  \end{itemize}
\item
  \textbf{Maximum Likelihood:}

  \begin{itemize}
  \tightlist
  \item
    Optimize  via differential evolution
  \item
    Minimize: (n/2)log() + (1/2)log\textbar R\textbar{}
  \item
    Concentrated likelihood ( profiled out)
  \end{itemize}
\item
  \textbf{Prediction:}

  \begin{itemize}
  \tightlist
  \item
    Mean: f(x) =  + (x)Rr
  \item
    Variance: s(x) = {[}1 +  - (x)R(x){]}
  \item
    Uses Cholesky decomposition for efficiency
  \end{itemize}
\end{enumerate}

\subsection{Key Arguments Passed from
SpotOptim}\label{key-arguments-passed-from-spotoptim}

SpotOptim passes these to the surrogate via the standard interface:

\textbf{During fit:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surrogate.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{X}: Training points (n\_initial or accumulated evaluations)
\item
  \texttt{y}: Function values
\end{itemize}

\textbf{During predict:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OperatorTok{=}\NormalTok{ surrogate.predict(x)[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# For acquisition=\textquotesingle{}y\textquotesingle{}}
\NormalTok{mu, sigma }\OperatorTok{=}\NormalTok{ surrogate.predict(x, return\_std}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# For acquisition=\textquotesingle{}ei\textquotesingle{}, \textquotesingle{}pi\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\textbf{Implicit parameters via seed:}

\begin{itemize}
\tightlist
\item
  \texttt{random\_state=seed} (for GaussianProcessRegressor)
\item
  \texttt{seed=seed} (for Kriging)
\end{itemize}

\section{Benefits}\label{benefits}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Self-contained}: No heavy scikit-learn dependency for
  surrogate
\item
  \textbf{Explicit}: Clear hyperparameter bounds and optimization
\item
  \textbf{Educational}: Readable implementation of Kriging/GP
\item
  \textbf{Flexible}: Easy to extend with new kernels or features
\item
  \textbf{Compatible}: Works seamlessly with existing SpotOptim API
\end{enumerate}

\section{Future Enhancements}\label{future-enhancements}

Potential additions:

\begin{itemize}
\tightlist
\item[$\square$]
  Additional kernels (Matern, Exponential, Cubic)
\item[$\square$]
  Anisotropic hyperparameters (separate  per dimension)
\item[$\square$]
  Gradient-enhanced predictions
\item[$\square$]
  Batch predictions for efficiency
\item[$\square$]
  Parallel hyperparameter optimization
\item[$\square$]
  ARD (Automatic Relevance Determination)
\end{itemize}

\section{Conclusion}\label{conclusion}

Implementation of a Kriging surrogate into SpotOptim with:

\begin{itemize}
\tightlist
\item
   Full scikit-learn compatibility
\item
   Comprehensive test coverage (9 new tests)
\item
   Complete documentation
\item
   Example notebook
\item
   Zero breaking changes
\item
   All 25 tests passing
\end{itemize}

\chapter{Learning Rate Mapping for Unified Optimizer
Interface}\label{learning-rate-mapping-for-unified-optimizer-interface}

SpotOptim provides a sophisticated learning rate mapping system through
the \texttt{map\_lr()} function, enabling a unified interface for
learning rates across different PyTorch optimizers. This solves the
challenge that different optimizers operate on vastly different learning
rate scales.

\section{Overview}\label{overview-3}

Different PyTorch optimizers use different default learning rates and
optimal ranges:

\begin{itemize}
\tightlist
\item
  \textbf{Adam}: default 0.001, typical range 0.0001-0.01
\item
  \textbf{SGD}: default 0.01, typical range 0.001-0.1
\item
  \textbf{RMSprop}: default 0.01, typical range 0.001-0.1
\end{itemize}

This makes it difficult to compare optimizer performance fairly or
optimize learning rates across different optimizers. The
\texttt{map\_lr()} function provides a unified scale where
\textbf{\texttt{lr\_unified=1.0} corresponds to each optimizer's PyTorch
default}.

\textbf{Module}: \texttt{spotoptim.utils.mapping}

\textbf{Key Features}:

\begin{itemize}
\tightlist
\item
  Unified learning rate scale across all optimizers
\item
  Fair comparison when evaluating different optimizers
\item
  Simplified hyperparameter optimization
\item
  Based on official PyTorch default learning rates
\item
  Supports 13 major PyTorch optimizers
\end{itemize}

\section{Quick Start}\label{quick-start-2}

\subsection{Basic Usage}\label{basic-usage-3}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}

\CommentTok{\# Get optimizer{-}specific learning rate from unified scale}
\NormalTok{lr\_adam }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"Adam"}\NormalTok{)      }\CommentTok{\# Returns 0.001 (Adam\textquotesingle{}s default)}
\NormalTok{lr\_sgd }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"SGD"}\NormalTok{)        }\CommentTok{\# Returns 0.01 (SGD\textquotesingle{}s default)}
\NormalTok{lr\_rmsprop }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{)  }\CommentTok{\# Returns 0.01 (RMSprop\textquotesingle{}s default)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Unified lr=1.0:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Adam:    }\SpecialCharTok{\{}\NormalTok{lr\_adam}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  SGD:     }\SpecialCharTok{\{}\NormalTok{lr\_sgd}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  RMSprop: }\SpecialCharTok{\{}\NormalTok{lr\_rmsprop}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Scaling Learning Rates}\label{scaling-learning-rates}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}

\CommentTok{\# Scale all learning rates by the same factor}
\NormalTok{unified\_lr }\OperatorTok{=} \FloatTok{0.5}

\NormalTok{lr\_adam }\OperatorTok{=}\NormalTok{ map\_lr(unified\_lr, }\StringTok{"Adam"}\NormalTok{)      }\CommentTok{\# 0.5 * 0.001 = 0.0005}
\NormalTok{lr\_sgd }\OperatorTok{=}\NormalTok{ map\_lr(unified\_lr, }\StringTok{"SGD"}\NormalTok{)        }\CommentTok{\# 0.5 * 0.01 = 0.005}
\NormalTok{lr\_rmsprop }\OperatorTok{=}\NormalTok{ map\_lr(unified\_lr, }\StringTok{"RMSprop"}\NormalTok{)  }\CommentTok{\# 0.5 * 0.01 = 0.005}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Unified lr=}\SpecialCharTok{\{}\NormalTok{unified\_lr}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Adam:    }\SpecialCharTok{\{}\NormalTok{lr\_adam}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  SGD:     }\SpecialCharTok{\{}\NormalTok{lr\_sgd}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  RMSprop: }\SpecialCharTok{\{}\NormalTok{lr\_rmsprop}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Integration with
LinearRegressor}\label{integration-with-linearregressor}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}

\CommentTok{\# Create model with unified learning rate}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{    input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, }
\NormalTok{    output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, }
\NormalTok{    l1}\OperatorTok{=}\DecValTok{32}\NormalTok{, }
\NormalTok{    num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    lr}\OperatorTok{=}\FloatTok{1.0}  \CommentTok{\# Unified learning rate}
\NormalTok{)}

\CommentTok{\# Get optimizer {-} automatically uses mapped learning rate}
\NormalTok{optimizer\_adam }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{)     }\CommentTok{\# Gets 1.0 * 0.001 = 0.001}
\NormalTok{optimizer\_sgd }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"SGD"}\NormalTok{)       }\CommentTok{\# Gets 1.0 * 0.01 = 0.01}

\CommentTok{\# Verify the actual learning rates}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Adam actual lr: }\SpecialCharTok{\{}\NormalTok{optimizer\_adam}\SpecialCharTok{.}\NormalTok{param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}lr\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"SGD actual lr: }\SpecialCharTok{\{}\NormalTok{optimizer\_sgd}\SpecialCharTok{.}\NormalTok{param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}lr\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Function Reference}\label{function-reference-1}

\subsection{\texorpdfstring{\texttt{map\_lr(lr\_unified,\ optimizer\_name,\ use\_default\_scale=True)}}{map\_lr(lr\_unified, optimizer\_name, use\_default\_scale=True)}}\label{map_lrlr_unified-optimizer_name-use_default_scaletrue}

Maps a unified learning rate to an optimizer-specific learning rate.

\textbf{Parameters}:

\begin{itemize}
\tightlist
\item
  \texttt{lr\_unified} (float): Unified learning rate multiplier. A
  value of 1.0 corresponds to the optimizer's default learning rate.
  Typical range: {[}0.001, 100.0{]}.
\item
  \texttt{optimizer\_name} (str): Name of the PyTorch optimizer. Must be
  one of: ``Adadelta'', ``Adagrad'', ``Adam'', ``AdamW'',
  ``SparseAdam'', ``Adamax'', ``ASGD'', ``LBFGS'', ``NAdam'', ``RAdam'',
  ``RMSprop'', ``Rprop'', ``SGD''.
\item
  \texttt{use\_default\_scale} (bool, optional): Whether to scale by the
  optimizer's default learning rate. If \texttt{True} (default),
  \texttt{lr\_unified} is multiplied by the default lr. If
  \texttt{False}, returns \texttt{lr\_unified} directly.
\end{itemize}

\textbf{Returns}:

\begin{itemize}
\tightlist
\item
  \texttt{float}: The optimizer-specific learning rate.
\end{itemize}

\textbf{Raises}:

\begin{itemize}
\tightlist
\item
  \texttt{ValueError}: If \texttt{optimizer\_name} is not supported.
\item
  \texttt{ValueError}: If \texttt{lr\_unified} is not positive.
\end{itemize}

\textbf{Example}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}

\CommentTok{\# Get default learning rates (unified lr = 1.0)}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"Adam"}\NormalTok{)      }\CommentTok{\# 0.001}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"SGD"}\NormalTok{)       }\CommentTok{\# 0.01}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{)   }\CommentTok{\# 0.01}

\CommentTok{\# Scale learning rates}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{0.5}\NormalTok{, }\StringTok{"Adam"}\NormalTok{)      }\CommentTok{\# 0.0005}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{2.0}\NormalTok{, }\StringTok{"SGD"}\NormalTok{)       }\CommentTok{\# 0.02}

\CommentTok{\# Without default scaling}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{0.01}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, use\_default\_scale}\OperatorTok{=}\VariableTok{False}\NormalTok{)  }\CommentTok{\# 0.01 (direct)}
\end{Highlighting}
\end{Shaded}

\section{Supported Optimizers}\label{supported-optimizers}

All major PyTorch optimizers are supported with their default learning
rates:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Optimizer & Default LR & Typical Range & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Adam} & 0.001 & 0.0001-0.01 & Most popular, good default \\
\textbf{AdamW} & 0.001 & 0.0001-0.01 & Adam with weight decay \\
\textbf{Adamax} & 0.002 & 0.0001-0.01 & Adam variant with infinity
norm \\
\textbf{NAdam} & 0.002 & 0.0001-0.01 & Adam with Nesterov momentum \\
\textbf{RAdam} & 0.001 & 0.0001-0.01 & Rectified Adam \\
\textbf{SparseAdam} & 0.001 & 0.0001-0.01 & For sparse gradients \\
\textbf{SGD} & 0.01 & 0.001-0.1 & Classic, needs momentum \\
\textbf{RMSprop} & 0.01 & 0.001-0.1 & Good for RNNs \\
\textbf{Adagrad} & 0.01 & 0.001-0.1 & Adaptive learning rate \\
\textbf{Adadelta} & 1.0 & 0.1-10.0 & Extension of Adagrad \\
\textbf{ASGD} & 0.01 & 0.001-0.1 & Averaged SGD \\
\textbf{LBFGS} & 1.0 & 0.1-10.0 & Second-order optimizer \\
\textbf{Rprop} & 0.01 & 0.001-0.1 & Resilient backpropagation \\
\end{longtable}

\section{Use Cases}\label{use-cases}

\subsection{Comparing Different
Optimizers}\label{comparing-different-optimizers}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}

\CommentTok{\# Load data}
\NormalTok{train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Test different optimizers with unified learning rate}
\NormalTok{unified\_lr }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{optimizers\_to\_test }\OperatorTok{=}\NormalTok{ [}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{, }\StringTok{"AdamW"}\NormalTok{]}
\NormalTok{results }\OperatorTok{=}\NormalTok{ \{\}}

\ControlFlowTok{for}\NormalTok{ opt\_name }\KeywordTok{in}\NormalTok{ optimizers\_to\_test:}
    \CommentTok{\# Reset for fair comparison}
\NormalTok{    torch.manual\_seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{    model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, l1}\OperatorTok{=}\DecValTok{32}\NormalTok{, }
\NormalTok{                           num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{, lr}\OperatorTok{=}\NormalTok{unified\_lr)}
    
    \CommentTok{\# Create optimizer with mapped learning rate}
    \ControlFlowTok{if}\NormalTok{ opt\_name }\OperatorTok{==} \StringTok{"SGD"}\NormalTok{:}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(opt\_name, momentum}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(opt\_name)}
    
\NormalTok{    criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
    
    \CommentTok{\# Train}
\NormalTok{    model.train()}
    \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{50}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{            optimizer.zero\_grad()}
\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{            loss.backward()}
\NormalTok{            optimizer.step()}
    
    \CommentTok{\# Evaluate}
\NormalTok{    model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{    test\_loss }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
        \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{            predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{            test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
    
\NormalTok{    avg\_test\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\NormalTok{    results[opt\_name] }\OperatorTok{=}\NormalTok{ avg\_test\_loss}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{opt\_name}\SpecialCharTok{:10s\}}\SpecialStringTok{: Test MSE = }\SpecialCharTok{\{}\NormalTok{avg\_test\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{ "}
          \SpecialStringTok{f"(actual lr = }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}lr\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{)"}\NormalTok{)}

\CommentTok{\# Find best optimizer}
\NormalTok{best\_opt }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(results, key}\OperatorTok{=}\NormalTok{results.get)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Best optimizer: }\SpecialCharTok{\{}\NormalTok{best\_opt}\SpecialCharTok{\}}\SpecialStringTok{ with MSE = }\SpecialCharTok{\{}\NormalTok{results[best\_opt]}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Hyperparameter Optimization with
SpotOptim}\label{hyperparameter-optimization-with-spotoptim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ train\_and\_evaluate(X):}
    \CommentTok{"""Objective function for hyperparameter optimization."""}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    
    \CommentTok{\# Load data once}
\NormalTok{    train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{        batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{    )}
    
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
        \CommentTok{\# Extract hyperparameters}
\NormalTok{        lr\_unified }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ params[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# Log scale}
\NormalTok{        optimizer\_name }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]     }\CommentTok{\# Factor variable}
\NormalTok{        l1 }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{2}\NormalTok{])           }\CommentTok{\# Integer}
\NormalTok{        num\_layers }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(params[}\DecValTok{3}\NormalTok{])   }\CommentTok{\# Integer}
        
        \CommentTok{\# Create model with unified learning rate}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            l1}\OperatorTok{=}\NormalTok{l1,}
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\NormalTok{num\_layers,}
\NormalTok{            lr}\OperatorTok{=}\NormalTok{lr\_unified  }\CommentTok{\# Automatically mapped per optimizer}
\NormalTok{        )}
        
        \CommentTok{\# Get optimizer (lr already mapped internally)}
        \ControlFlowTok{if}\NormalTok{ optimizer\_name }\OperatorTok{==} \StringTok{"SGD"}\NormalTok{:}
\NormalTok{            optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(optimizer\_name, momentum}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(optimizer\_name)}
        
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \CommentTok{\# Train}
\NormalTok{        model.train()}
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{30}\NormalTok{):}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
        
        \CommentTok{\# Evaluate}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
        
\NormalTok{        avg\_test\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\NormalTok{        results.append(avg\_test\_loss)}
    
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\CommentTok{\# Optimize learning rate, optimizer choice, and architecture}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{train\_and\_evaluate,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[}
\NormalTok{        (}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{),                           }\CommentTok{\# log10(lr\_unified): [0.0001, 1.0]}
\NormalTok{        (}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{, }\StringTok{"AdamW"}\NormalTok{),  }\CommentTok{\# Optimizer choice}
\NormalTok{        (}\DecValTok{16}\NormalTok{, }\DecValTok{128}\NormalTok{),                         }\CommentTok{\# Layer size}
\NormalTok{        (}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)                             }\CommentTok{\# Number of hidden layers}
\NormalTok{    ],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"factor"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"int"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Display results}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Optimization Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best unified lr: }\SpecialCharTok{\{}\DecValTok{10}\OperatorTok{**}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best optimizer: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best layer size: }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best num layers: }\SpecialCharTok{\{}\BuiltInTok{int}\NormalTok{(result.x[}\DecValTok{3}\NormalTok{])}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best test MSE: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Show actual learning rate used}
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}
\NormalTok{actual\_lr }\OperatorTok{=}\NormalTok{ map\_lr(}\DecValTok{10}\OperatorTok{**}\NormalTok{result.x[}\DecValTok{0}\NormalTok{], result.x[}\DecValTok{1}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Actual }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ learning rate: }\SpecialCharTok{\{}\NormalTok{actual\_lr}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Log-Scale Hyperparameter
Search}\label{log-scale-hyperparameter-search}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Common pattern: sample unified lr from log scale}
\NormalTok{log\_lr\_range }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)  }\CommentTok{\# [{-}4, {-}3.56, ..., 0]}
\NormalTok{optimizers }\OperatorTok{=}\NormalTok{ [}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Log{-}scale learning rate search:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\StringTok{\textquotesingle{}log\_lr\textquotesingle{}}\SpecialCharTok{:\textless{}10\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}unified\_lr\textquotesingle{}}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}Adam\textquotesingle{}}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}SGD\textquotesingle{}}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{ }\SpecialCharTok{\{}\StringTok{\textquotesingle{}RMSprop\textquotesingle{}}\SpecialCharTok{:\textless{}12\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}"} \OperatorTok{*} \DecValTok{60}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ log\_lr }\KeywordTok{in}\NormalTok{ log\_lr\_range:}
\NormalTok{    lr\_unified }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ log\_lr}
\NormalTok{    lr\_adam }\OperatorTok{=}\NormalTok{ map\_lr(lr\_unified, }\StringTok{"Adam"}\NormalTok{)}
\NormalTok{    lr\_sgd }\OperatorTok{=}\NormalTok{ map\_lr(lr\_unified, }\StringTok{"SGD"}\NormalTok{)}
\NormalTok{    lr\_rmsprop }\OperatorTok{=}\NormalTok{ map\_lr(lr\_unified, }\StringTok{"RMSprop"}\NormalTok{)}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{log\_lr}\SpecialCharTok{:\textless{}10.2f\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{lr\_unified}\SpecialCharTok{:\textless{}12.6f\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{lr\_adam}\SpecialCharTok{:\textless{}12.8f\}}\SpecialStringTok{ "}
          \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{lr\_sgd}\SpecialCharTok{:\textless{}12.8f\}}\SpecialStringTok{ }\SpecialCharTok{\{}\NormalTok{lr\_rmsprop}\SpecialCharTok{:\textless{}12.8f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Output:

\begin{verbatim}
log_lr     unified_lr   Adam         SGD          RMSprop     
------------------------------------------------------------
-4.00      0.000100     0.00000010   0.00000100   0.00000100  
-3.56      0.000275     0.00000028   0.00000275   0.00000275  
-3.11      0.000759     0.00000076   0.00000759   0.00000759  
-2.67      0.002089     0.00000209   0.00002089   0.00002089  
-2.22      0.005754     0.00000575   0.00005754   0.00005754  
-1.78      0.015849     0.00001585   0.00015849   0.00015849  
-1.33      0.043652     0.00004365   0.00043652   0.00043652  
-0.89      0.120226     0.00012023   0.00120226   0.00120226  
-0.44      0.331131     0.00033113   0.00331131   0.00331131  
0.00       1.000000     0.00100000   0.01000000   0.01000000  
\end{verbatim}

\subsection{Custom Learning Rate
Schedules}\label{custom-learning-rate-schedules}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}

\CommentTok{\# Create model with unified lr}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}

\CommentTok{\# Get initial optimizer}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{)}
\NormalTok{initial\_lr }\OperatorTok{=}\NormalTok{ optimizer.param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}lr\textquotesingle{}}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Initial learning rate: }\SpecialCharTok{\{}\NormalTok{initial\_lr}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Use PyTorch learning rate scheduler}
\NormalTok{scheduler }\OperatorTok{=}\NormalTok{ torch.optim.lr\_scheduler.StepLR(optimizer, step\_size}\OperatorTok{=}\DecValTok{30}\NormalTok{, gamma}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}

\CommentTok{\# Training with scheduler}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{100}\NormalTok{):}
    \CommentTok{\# ... training code ...}
\NormalTok{    scheduler.step()}
    
    \ControlFlowTok{if}\NormalTok{ (epoch }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{\%} \DecValTok{30} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
\NormalTok{        current\_lr }\OperatorTok{=}\NormalTok{ optimizer.param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}lr\textquotesingle{}}\NormalTok{]}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{: lr = }\SpecialCharTok{\{}\NormalTok{current\_lr}\SpecialCharTok{:.8f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Direct Usage Without
LinearRegressor}\label{direct-usage-without-linearregressor}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}

\CommentTok{\# Define your own model}
\KeywordTok{class}\NormalTok{ MyModel(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.fc1 }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{32}\NormalTok{)}
        \VariableTok{self}\NormalTok{.fc2 }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{32}\NormalTok{, }\DecValTok{1}\NormalTok{)}
    
    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(}\VariableTok{self}\NormalTok{.fc1(x))}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.fc2(x)}

\NormalTok{model }\OperatorTok{=}\NormalTok{ MyModel()}

\CommentTok{\# Use map\_lr to get optimizer{-}specific learning rate}
\NormalTok{unified\_lr }\OperatorTok{=} \FloatTok{2.0}
\NormalTok{optimizer\_name }\OperatorTok{=} \StringTok{"Adam"}

\NormalTok{actual\_lr }\OperatorTok{=}\NormalTok{ map\_lr(unified\_lr, optimizer\_name)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.Adam(model.parameters(), lr}\OperatorTok{=}\NormalTok{actual\_lr)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Unified lr: }\SpecialCharTok{\{}\NormalTok{unified\_lr}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Actual }\SpecialCharTok{\{}\NormalTok{optimizer\_name}\SpecialCharTok{\}}\SpecialStringTok{ lr: }\SpecialCharTok{\{}\NormalTok{actual\_lr}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Best Practices}\label{best-practices-4}

\subsection{Choosing Unified Learning
Rate}\label{choosing-unified-learning-rate}

\textbf{For initial experiments:}

\begin{itemize}
\tightlist
\item
  Start with \texttt{lr=1.0} (gives defaults for all optimizers)
\item
  Test with \texttt{lr=0.1}, \texttt{lr=1.0}, \texttt{lr=10.0} to get a
  sense of scale
\end{itemize}

\textbf{For hyperparameter optimization:}

\begin{itemize}
\tightlist
\item
  Use log scale: sample from \texttt{{[}-4,\ 0{]}} or
  \texttt{{[}-3,\ 1{]}}
\item
  Convert with \texttt{lr\_unified\ =\ 10\ **\ log\_lr}
\item
  This gives reasonable ranges for all optimizers
\end{itemize}

\textbf{For fine-tuning:}

\begin{itemize}
\tightlist
\item
  If training is unstable: try smaller \texttt{lr} (e.g., 0.1 or 0.5)
\item
  If training is too slow: try larger \texttt{lr} (e.g., 2.0 or 5.0)
\item
  Monitor loss curves to adjust
\end{itemize}

\subsection{Optimizer Selection
Guidelines}\label{optimizer-selection-guidelines}

\textbf{Adam family (Adam, AdamW, NAdam, RAdam):}

\begin{itemize}
\tightlist
\item
   Good default choice for most tasks
\item
   Adaptive learning rates per parameter
\item
   Works well out of the box
\item
  Use \texttt{lr=1.0} as starting point
\end{itemize}

\textbf{SGD:}

\begin{itemize}
\tightlist
\item
   Good for large datasets
\item
   Often achieves better generalization
\item
   Requires momentum (e.g., 0.9)
\item
  Use \texttt{lr=1.0} with momentum=0.9
\end{itemize}

\textbf{RMSprop:}

\begin{itemize}
\tightlist
\item
   Good for recurrent networks
\item
   Handles non-stationary objectives
\item
  Use \texttt{lr=1.0} as starting point
\end{itemize}

\textbf{Others (Adadelta, Adagrad, etc.):}

\begin{itemize}
\tightlist
\item
  Specialized use cases
\item
  Start with \texttt{lr=1.0} and adjust
\end{itemize}

\subsection{Common Patterns}\label{common-patterns}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pattern 1: Quick optimizer comparison}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ opt }\KeywordTok{in}\NormalTok{ [}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{]:}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(opt)}
    \CommentTok{\# ... train and compare ...}

\CommentTok{\# Pattern 2: Hyperparameter optimization}
\KeywordTok{def}\NormalTok{ objective(X):}
\NormalTok{    lr\_unified }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]  }\CommentTok{\# Log scale}
\NormalTok{    optimizer\_name }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]     }\CommentTok{\# Factor}
    \CommentTok{\# ... use unified lr ...}

\CommentTok{\# Pattern 3: Override model\textquotesingle{}s lr}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{, lr}\OperatorTok{=}\FloatTok{2.0}\NormalTok{)  }\CommentTok{\# Override with 2.0}

\CommentTok{\# Pattern 4: Direct mapping}
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}
\NormalTok{lr\_actual }\OperatorTok{=}\NormalTok{ map\_lr(unified\_lr, optimizer\_name)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.Adam(params, lr}\OperatorTok{=}\NormalTok{lr\_actual)}
\end{Highlighting}
\end{Shaded}

\section{Troubleshooting}\label{troubleshooting-2}

\subsection{Issue: Training is unstable (loss
explodes)}\label{issue-training-is-unstable-loss-explodes}

\textbf{Solution}: Learning rate is too high. Try:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Reduce from 1.0}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Training is too slow (loss decreases very
slowly)}\label{issue-training-is-too-slow-loss-decreases-very-slowly}

\textbf{Solution}: Learning rate is too low. Try:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{5.0}\NormalTok{)  }\CommentTok{\# Increase from 1.0}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Different results across optimizer
runs}\label{issue-different-results-across-optimizer-runs}

\textbf{Solution}: Set random seed for reproducibility:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\NormalTok{torch.manual\_seed(}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Want to use raw learning rate without
mapping}\label{issue-want-to-use-raw-learning-rate-without-mapping}

\textbf{Solution}: Use \texttt{use\_default\_scale=False}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{0.001}\NormalTok{, }\StringTok{"Adam"}\NormalTok{, use\_default\_scale}\OperatorTok{=}\VariableTok{False}\NormalTok{)  }\CommentTok{\# Returns 0.001 directly}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Optimizer not
supported}\label{issue-optimizer-not-supported}

\textbf{Solution}: Check supported optimizers:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ OPTIMIZER\_DEFAULT\_LR}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Supported optimizers:"}\NormalTok{, }\BuiltInTok{list}\NormalTok{(OPTIMIZER\_DEFAULT\_LR.keys()))}
\end{Highlighting}
\end{Shaded}

\section{Technical Details}\label{technical-details-2}

\subsection{How It Works}\label{how-it-works-2}

The mapping is simple but effective:

\begin{verbatim}
actual_lr = lr_unified * default_lr[optimizer_name]
\end{verbatim}

For example:

\begin{itemize}
\tightlist
\item
  \texttt{map\_lr(1.0,\ "Adam")}  \texttt{1.0\ *\ 0.001} =
  \texttt{0.001}
\item
  \texttt{map\_lr(0.5,\ "SGD")}  \texttt{0.5\ *\ 0.01} = \texttt{0.005}
\item
  \texttt{map\_lr(2.0,\ "RMSprop")}  \texttt{2.0\ *\ 0.01} =
  \texttt{0.02}
\end{itemize}

This ensures that the same unified learning rate gives
optimizer-specific learning rates in their typical working ranges.

\subsection{Design Rationale}\label{design-rationale}

\textbf{Why use defaults as scaling factors?}

PyTorch's default learning rates are carefully chosen to work well for
typical use cases. By using them as scaling factors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{lr=1.0} always gives sensible defaults
\item
  Scaling preserves the relative relationships between optimizers
\item
  Each optimizer stays in its optimal range
\item
  Easy to understand and explain
\end{enumerate}

\textbf{Comparison with spotPython's approach:}

spotPython uses \texttt{lr\ =\ lr\_mult\ *\ default\_lr} in
\texttt{optimizer\_handler()}. Our implementation:

\begin{itemize}
\tightlist
\item
   Separates mapping logic (testable, reusable)
\item
   Provides standalone function (\texttt{map\_lr()})
\item
   Comprehensive error handling and validation
\item
   Extensive documentation and examples
\item
   Full integration with \texttt{LinearRegressor}
\end{itemize}

\subsection{Default Learning Rates}\label{default-learning-rates}

All values verified against
\href{https://pytorch.org/docs/stable/optim.html}{PyTorch
documentation}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OPTIMIZER\_DEFAULT\_LR }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"Adadelta"}\NormalTok{: }\FloatTok{1.0}\NormalTok{,}
    \StringTok{"Adagrad"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
    \StringTok{"Adam"}\NormalTok{: }\FloatTok{0.001}\NormalTok{,}
    \StringTok{"AdamW"}\NormalTok{: }\FloatTok{0.001}\NormalTok{,}
    \StringTok{"SparseAdam"}\NormalTok{: }\FloatTok{0.001}\NormalTok{,}
    \StringTok{"Adamax"}\NormalTok{: }\FloatTok{0.002}\NormalTok{,}
    \StringTok{"ASGD"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
    \StringTok{"LBFGS"}\NormalTok{: }\FloatTok{1.0}\NormalTok{,}
    \StringTok{"NAdam"}\NormalTok{: }\FloatTok{0.002}\NormalTok{,}
    \StringTok{"RAdam"}\NormalTok{: }\FloatTok{0.001}\NormalTok{,}
    \StringTok{"RMSprop"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
    \StringTok{"Rprop"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
    \StringTok{"SGD"}\NormalTok{: }\FloatTok{0.01}\NormalTok{,}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{Examples}\label{examples}

\subsection{Complete Example: Optimizer Comparison
Study}\label{complete-example-optimizer-comparison-study}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{"""}
\CommentTok{Complete example: Compare optimizers with unified learning rate interface.}
\CommentTok{"""}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}
\ImportTok{from}\NormalTok{ spotoptim.data }\ImportTok{import}\NormalTok{ get\_diabetes\_dataloaders}
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Set seed for reproducibility}
\NormalTok{torch.manual\_seed(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Load data}
\NormalTok{train\_loader, test\_loader, \_ }\OperatorTok{=}\NormalTok{ get\_diabetes\_dataloaders(}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{, }
\NormalTok{    random\_state}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\CommentTok{\# Test configurations}
\NormalTok{optimizers }\OperatorTok{=}\NormalTok{ [}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{, }\StringTok{"AdamW"}\NormalTok{]}
\NormalTok{unified\_lrs }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{]}

\CommentTok{\# Store results}
\NormalTok{results }\OperatorTok{=}\NormalTok{ \{\}}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Training models with different optimizers and learning rates..."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{ unified\_lr }\KeywordTok{in}\NormalTok{ unified\_lrs:}
\NormalTok{    results[unified\_lr] }\OperatorTok{=}\NormalTok{ \{\}}
    
    \ControlFlowTok{for}\NormalTok{ opt\_name }\KeywordTok{in}\NormalTok{ optimizers:}
        \CommentTok{\# Reset model for fair comparison}
\NormalTok{        torch.manual\_seed(}\DecValTok{42}\NormalTok{)}
        
        \CommentTok{\# Create model with unified lr}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(}
\NormalTok{            input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, }
\NormalTok{            output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, }
\NormalTok{            l1}\OperatorTok{=}\DecValTok{32}\NormalTok{, }
\NormalTok{            num\_hidden\_layers}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{            lr}\OperatorTok{=}\NormalTok{unified\_lr}
\NormalTok{        )}
        
        \CommentTok{\# Get optimizer}
        \ControlFlowTok{if}\NormalTok{ opt\_name }\OperatorTok{==} \StringTok{"SGD"}\NormalTok{:}
\NormalTok{            optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(opt\_name, momentum}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(opt\_name)}
        
\NormalTok{        actual\_lr }\OperatorTok{=}\NormalTok{ optimizer.param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{\textquotesingle{}lr\textquotesingle{}}\NormalTok{]}
\NormalTok{        criterion }\OperatorTok{=}\NormalTok{ nn.MSELoss()}
        
        \CommentTok{\# Track training loss}
\NormalTok{        train\_losses }\OperatorTok{=}\NormalTok{ []}
        
        \CommentTok{\# Train}
\NormalTok{        model.train()}
        \ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{50}\NormalTok{):}
\NormalTok{            epoch\_loss }\OperatorTok{=} \FloatTok{0.0}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ train\_loader:}
\NormalTok{                optimizer.zero\_grad()}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                loss }\OperatorTok{=}\NormalTok{ criterion(predictions, batch\_y)}
\NormalTok{                loss.backward()}
\NormalTok{                optimizer.step()}
\NormalTok{                epoch\_loss }\OperatorTok{+=}\NormalTok{ loss.item()}
            
\NormalTok{            avg\_epoch\_loss }\OperatorTok{=}\NormalTok{ epoch\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(train\_loader)}
\NormalTok{            train\_losses.append(avg\_epoch\_loss)}
        
        \CommentTok{\# Evaluate on test set}
\NormalTok{        model.}\BuiltInTok{eval}\NormalTok{()}
\NormalTok{        test\_loss }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{with}\NormalTok{ torch.no\_grad():}
            \ControlFlowTok{for}\NormalTok{ batch\_X, batch\_y }\KeywordTok{in}\NormalTok{ test\_loader:}
\NormalTok{                predictions }\OperatorTok{=}\NormalTok{ model(batch\_X)}
\NormalTok{                test\_loss }\OperatorTok{+=}\NormalTok{ criterion(predictions, batch\_y).item()}
        
\NormalTok{        avg\_test\_loss }\OperatorTok{=}\NormalTok{ test\_loss }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(test\_loader)}
\NormalTok{        results[unified\_lr][opt\_name] }\OperatorTok{=}\NormalTok{ \{}
            \StringTok{\textquotesingle{}train\_losses\textquotesingle{}}\NormalTok{: train\_losses,}
            \StringTok{\textquotesingle{}test\_loss\textquotesingle{}}\NormalTok{: avg\_test\_loss,}
            \StringTok{\textquotesingle{}actual\_lr\textquotesingle{}}\NormalTok{: actual\_lr}
\NormalTok{        \}}
        
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Unified lr=}\SpecialCharTok{\{}\NormalTok{unified\_lr}\SpecialCharTok{:.1f\}}\SpecialStringTok{, }\SpecialCharTok{\{}\NormalTok{opt\_name}\SpecialCharTok{:10s\}}\SpecialStringTok{: "}
              \SpecialStringTok{f"actual\_lr=}\SpecialCharTok{\{}\NormalTok{actual\_lr}\SpecialCharTok{:.6f\}}\SpecialStringTok{, test\_MSE=}\SpecialCharTok{\{}\NormalTok{avg\_test\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Display summary}
\BuiltInTok{print}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{70}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Summary: Best configurations"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"="} \OperatorTok{*} \DecValTok{70}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ unified\_lr }\KeywordTok{in}\NormalTok{ unified\_lrs:}
\NormalTok{    best\_opt }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(results[unified\_lr].items(), }
\NormalTok{                   key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x[}\DecValTok{1}\NormalTok{][}\StringTok{\textquotesingle{}test\_loss\textquotesingle{}}\NormalTok{])}
\NormalTok{    opt\_name, metrics }\OperatorTok{=}\NormalTok{ best\_opt}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Unified lr=}\SpecialCharTok{\{}\NormalTok{unified\_lr}\SpecialCharTok{:.1f\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{opt\_name}\SpecialCharTok{:10s\}}\SpecialStringTok{ "}
          \SpecialStringTok{f"(test MSE=}\SpecialCharTok{\{}\NormalTok{metrics[}\StringTok{\textquotesingle{}test\_loss\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.4f\}}\SpecialStringTok{, "}
          \SpecialStringTok{f"actual lr=}\SpecialCharTok{\{}\NormalTok{metrics[}\StringTok{\textquotesingle{}actual\_lr\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{)"}\NormalTok{)}

\CommentTok{\# Find overall best}
\NormalTok{best\_overall }\OperatorTok{=} \VariableTok{None}
\NormalTok{best\_overall\_loss }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ unified\_lr }\KeywordTok{in}\NormalTok{ unified\_lrs:}
    \ControlFlowTok{for}\NormalTok{ opt\_name, metrics }\KeywordTok{in}\NormalTok{ results[unified\_lr].items():}
        \ControlFlowTok{if}\NormalTok{ metrics[}\StringTok{\textquotesingle{}test\_loss\textquotesingle{}}\NormalTok{] }\OperatorTok{\textless{}}\NormalTok{ best\_overall\_loss:}
\NormalTok{            best\_overall\_loss }\OperatorTok{=}\NormalTok{ metrics[}\StringTok{\textquotesingle{}test\_loss\textquotesingle{}}\NormalTok{]}
\NormalTok{            best\_overall }\OperatorTok{=}\NormalTok{ (unified\_lr, opt\_name, metrics[}\StringTok{\textquotesingle{}actual\_lr\textquotesingle{}}\NormalTok{])}

\BuiltInTok{print}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Overall best: unified\_lr=}\SpecialCharTok{\{}\NormalTok{best\_overall[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.1f\}}\SpecialStringTok{, "}
      \SpecialStringTok{f"optimizer=}\SpecialCharTok{\{}\NormalTok{best\_overall[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{, "}
      \SpecialStringTok{f"test\_MSE=}\SpecialCharTok{\{}\NormalTok{best\_overall\_loss}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Actual learning rate used: }\SpecialCharTok{\{}\NormalTok{best\_overall[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{See Also}\label{see-also-1}

\begin{itemize}
\tightlist
\item
  \href{../api/linear_regressor.md}{LinearRegressor Documentation} -
  Neural network class with lr parameter
\item
  \href{diabetes_dataset.md}{Diabetes Dataset Utilities} - Data loading
  for examples
\item
  \href{../tutorials/hyperparameter_optimization.md}{Hyperparameter
  Optimization} - Using map\_lr with SpotOptim
\item
  \href{https://pytorch.org/docs/stable/optim.html}{PyTorch Optimizer
  Documentation} - Official PyTorch reference
\end{itemize}

\section{References}\label{references}

\begin{itemize}
\tightlist
\item
  Kingma, D. P., \& Ba, J. (2014). Adam: A method for stochastic
  optimization. arXiv:1412.6980.
\item
  Loshchilov, I., \& Hutter, F. (2017). Decoupled weight decay
  regularization. arXiv:1711.05101.
\item
  PyTorch Team. (2023). PyTorch Optimizer Documentation.
  https://pytorch.org/docs/stable/optim.html
\end{itemize}

\chapter{Unified Learning Rate
Interface}\label{unified-learning-rate-interface}

This module provides a sophisticated unified learning rate interface for
PyTorch optimizers through the \texttt{map\_lr()} function and
integration with \texttt{LinearRegressor}.

\section{Overview}\label{overview-4}

Different PyTorch optimizers operate on vastly different learning rate
scales:

\begin{itemize}
\tightlist
\item
  \textbf{Adam} typically uses lr \textasciitilde{} 0.0001-0.001
\item
  \textbf{SGD} typically uses lr \textasciitilde{} 0.01-0.1
\item
  \textbf{RMSprop} typically uses lr \textasciitilde{} 0.001-0.01
\end{itemize}

This makes it difficult to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare optimizer performance fairly
\item
  Optimize learning rate as a hyperparameter across different optimizers
\item
  Switch between optimizers without retuning learning rates
\end{enumerate}

The \texttt{map\_lr()} function solves this by providing a unified
learning rate scale where \textbf{lr=1.0 corresponds to each optimizer's
PyTorch default}.

\section{Key Features}\label{key-features}

\begin{itemize}
\tightlist
\item
   \textbf{Unified Interface}: Single learning rate parameter works
  across all optimizers
\item
   \textbf{Fair Comparison}: Same unified lr gives optimizer-specific
  optimal ranges
\item
   \textbf{Hyperparameter Optimization}: Optimize one learning rate
  for multiple optimizers
\item
   \textbf{Backward Compatible}: Existing code continues to work
\item
   \textbf{Well-tested}: 36 comprehensive tests covering all use cases
\item
   \textbf{Documented}: Extensive docstrings and examples
\end{itemize}

\section{Usage}\label{usage}

\subsection{Basic Usage with
LinearRegressor}\label{basic-usage-with-linearregressor}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.nn.linear\_regressor }\ImportTok{import}\NormalTok{ LinearRegressor}

\CommentTok{\# Create model with unified lr=1.0 (gives each optimizer its default)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}

\CommentTok{\# Adam gets 0.001 (its default)}
\NormalTok{optimizer\_adam }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{)}

\CommentTok{\# SGD gets 0.01 (its default)}
\NormalTok{optimizer\_sgd }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"SGD"}\NormalTok{)}

\CommentTok{\# RMSprop gets 0.01 (its default)}
\NormalTok{optimizer\_rmsprop }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"RMSprop"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Using Custom Unified Learning
Rate}\label{using-custom-unified-learning-rate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using lr=0.5 scales all optimizers by 0.5}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{optimizer\_adam }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"Adam"}\NormalTok{)     }\CommentTok{\# Gets 0.5 * 0.001 = 0.0005}
\NormalTok{optimizer\_sgd }\OperatorTok{=}\NormalTok{ model.get\_optimizer(}\StringTok{"SGD"}\NormalTok{)       }\CommentTok{\# Gets 0.5 * 0.01 = 0.005}
\end{Highlighting}
\end{Shaded}

\subsection{Direct Use of map\_lr()}\label{direct-use-of-map_lr}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim.utils.mapping }\ImportTok{import}\NormalTok{ map\_lr}

\CommentTok{\# Map unified lr to optimizer{-}specific lr}
\NormalTok{lr\_adam }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"Adam"}\NormalTok{)      }\CommentTok{\# Returns 0.001}
\NormalTok{lr\_sgd }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"SGD"}\NormalTok{)        }\CommentTok{\# Returns 0.01}
\NormalTok{lr\_rmsprop }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{)  }\CommentTok{\# Returns 0.01}

\CommentTok{\# Scale by 2x}
\NormalTok{lr\_adam }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{2.0}\NormalTok{, }\StringTok{"Adam"}\NormalTok{)      }\CommentTok{\# Returns 0.002}
\NormalTok{lr\_sgd }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{2.0}\NormalTok{, }\StringTok{"SGD"}\NormalTok{)        }\CommentTok{\# Returns 0.02}
\end{Highlighting}
\end{Shaded}

\subsection{Hyperparameter
Optimization}\label{hyperparameter-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ train\_model(X):}
\NormalTok{    results }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ params }\KeywordTok{in}\NormalTok{ X:}
\NormalTok{        lr\_unified }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ params[}\DecValTok{0}\NormalTok{]  }\CommentTok{\# Log scale: [{-}4, 0]}
\NormalTok{        optimizer\_name }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]     }\CommentTok{\# Factor: "Adam", "SGD", "RMSprop"}
        
        \CommentTok{\# Create model with unified lr {-} automatically scaled per optimizer}
\NormalTok{        model }\OperatorTok{=}\NormalTok{ LinearRegressor(input\_dim}\OperatorTok{=}\DecValTok{10}\NormalTok{, output\_dim}\OperatorTok{=}\DecValTok{1}\NormalTok{, lr}\OperatorTok{=}\NormalTok{lr\_unified)}
\NormalTok{        optimizer }\OperatorTok{=}\NormalTok{ model.get\_optimizer(optimizer\_name)}
        
        \CommentTok{\# Train and evaluate}
        \CommentTok{\# ... training code ...}
\NormalTok{        results.append(test\_loss)}
    \ControlFlowTok{return}\NormalTok{ np.array(results)}

\CommentTok{\# Optimize unified lr across different optimizers}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{train\_model,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{), (}\StringTok{"Adam"}\NormalTok{, }\StringTok{"SGD"}\NormalTok{, }\StringTok{"RMSprop"}\NormalTok{)],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"factor"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\section{Supported Optimizers}\label{supported-optimizers-1}

All major PyTorch optimizers are supported with their default learning
rates:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Optimizer & Default LR & Typical Range \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Adam & 0.001 & 0.0001-0.01 \\
AdamW & 0.001 & 0.0001-0.01 \\
Adamax & 0.002 & 0.0001-0.01 \\
NAdam & 0.002 & 0.0001-0.01 \\
RAdam & 0.001 & 0.0001-0.01 \\
SGD & 0.01 & 0.001-0.1 \\
RMSprop & 0.01 & 0.001-0.1 \\
Adagrad & 0.01 & 0.001-0.1 \\
Adadelta & 1.0 & 0.1-10.0 \\
ASGD & 0.01 & 0.001-0.1 \\
LBFGS & 1.0 & 0.1-10.0 \\
Rprop & 0.01 & 0.001-0.1 \\
\end{longtable}

\section{API Reference}\label{api-reference}

\subsection{\texorpdfstring{\texttt{map\_lr(lr\_unified,\ optimizer\_name,\ use\_default\_scale=True)}}{map\_lr(lr\_unified, optimizer\_name, use\_default\_scale=True)}}\label{map_lrlr_unified-optimizer_name-use_default_scaletrue-1}

Maps a unified learning rate to an optimizer-specific learning rate.

\textbf{Parameters:}

\begin{itemize}
\tightlist
\item
  \texttt{lr\_unified} (float): Unified learning rate multiplier.
  Typical range: {[}0.001, 100.0{]}
\item
  \texttt{optimizer\_name} (str): Name of the PyTorch optimizer
\item
  \texttt{use\_default\_scale} (bool): Whether to scale by optimizer's
  default (default: True)
\end{itemize}

\textbf{Returns:}

\begin{itemize}
\tightlist
\item
  \texttt{float}: The optimizer-specific learning rate
\end{itemize}

\textbf{Example:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{1.0}\NormalTok{, }\StringTok{"Adam"}\NormalTok{)  }\CommentTok{\# Returns 0.001 (Adam\textquotesingle{}s default)}
\NormalTok{lr }\OperatorTok{=}\NormalTok{ map\_lr(}\FloatTok{0.5}\NormalTok{, }\StringTok{"SGD"}\NormalTok{)   }\CommentTok{\# Returns 0.005 (0.5 * SGD\textquotesingle{}s default)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{LinearRegressor(...,\ lr=1.0)}}{LinearRegressor(..., lr=1.0)}}\label{linearregressor...-lr1.0}

\textbf{Parameter:}

\begin{itemize}
\tightlist
\item
  \texttt{lr} (float): Unified learning rate multiplier. Default: 1.0
\end{itemize}

\textbf{New Behavior in \texttt{get\_optimizer()}:}

\begin{itemize}
\tightlist
\item
  If \texttt{lr} is not specified, uses \texttt{self.lr}
\item
  Automatically maps unified lr to optimizer-specific lr
\item
  Can override model's lr by passing \texttt{lr} parameter
\end{itemize}

\section{Design Rationale}\label{design-rationale-1}

\subsection{Why Unified Learning
Rates?}\label{why-unified-learning-rates}

The approach is based on spotPython's \texttt{optimizer\_handler()} but
improved:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Separation of Concerns}: Mapping logic in separate, testable
  module
\item
  \textbf{Flexibility}: Can be used independently or integrated with
  models
\item
  \textbf{Transparency}: Clear mapping based on PyTorch defaults
\item
  \textbf{Extensibility}: Easy to add new optimizers
\item
  \textbf{Type Safety}: Comprehensive error handling and validation
\end{enumerate}

\subsection{Comparison with
spotPython}\label{comparison-with-spotpython}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & spotPython & spotoptim \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Approach & \texttt{lr\_mult\ *\ default\_lr} &
\texttt{map\_lr(lr\_unified,\ optimizer)} \\
Module & \texttt{optimizer\_handler()} & \texttt{map\_lr()} +
integration \\
Testing & Minimal & 36 comprehensive tests \\
Documentation & Basic & Extensive with examples \\
Reusability & Coupled & Standalone function \\
Error Handling & Basic & Comprehensive validation \\
\end{longtable}

\subsection{Log-scale Optimization}\label{log-scale-optimization}

For hyperparameter optimization, use log-scale for unified lr:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample from log10 scale [{-}4, 0]}
\NormalTok{log\_lr }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{2.5}  \CommentTok{\# Sampled value}
\NormalTok{lr\_unified }\OperatorTok{=} \DecValTok{10} \OperatorTok{**}\NormalTok{ log\_lr  }\CommentTok{\# 0.00316}

\CommentTok{\# Map to optimizer{-}specific}
\NormalTok{lr\_adam }\OperatorTok{=}\NormalTok{ map\_lr(lr\_unified, }\StringTok{"Adam"}\NormalTok{)  }\CommentTok{\# 0.00316 * 0.001 = 0.00000316}
\NormalTok{lr\_sgd }\OperatorTok{=}\NormalTok{ map\_lr(lr\_unified, }\StringTok{"SGD"}\NormalTok{)    }\CommentTok{\# 0.00316 * 0.01 = 0.0000316}
\end{Highlighting}
\end{Shaded}

This gives a reasonable search range across all optimizers.

\section{Examples}\label{examples-1}

See \texttt{examples/unified\_learning\_rate\_demo.py} for comprehensive
examples including: 1. Basic unified interface usage 2. Custom unified
learning rates 3. Training with different optimizers 4. Direct use of
map\_lr() 5. Log-scale hyperparameter optimization 6. Complete
hyperparameter optimization scenario

\section{References}\label{references-1}

\begin{itemize}
\tightlist
\item
  \href{https://pytorch.org/docs/stable/optim.html}{PyTorch Optimizer
  Documentation}
\item
  spotPython's \texttt{optimizer\_handler()} function (inspiration)
\item
  \href{https://arxiv.org/abs/2003.05689}{Hyperparameter Optimization
  Best Practices}
\end{itemize}

\section{Contributing}\label{contributing}

When adding new optimizers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Add default lr to \texttt{OPTIMIZER\_DEFAULT\_LR} dict in
  \texttt{mapping.py}
\item
  Verify the default against PyTorch documentation
\item
  Add tests in \texttt{test\_mapping.py}
\item
  Update this README
\end{enumerate}

\section{License}\label{license}

Same as spotoptim package (see main LICENSE file).

\chapter{Multi-Objective Optimization Support in
SpotOptim}\label{multi-objective-optimization-support-in-spotoptim}

\section{Overview}\label{overview-5}

SpotOptim supports multi-objective optimization functions with automatic
detection and flexible scalarization strategies. This implementation
follows the same approach as the Spot class from spotPython.

\section{What Was Implemented}\label{what-was-implemented}

\subsection{1. Core Functionality}\label{core-functionality}

\textbf{Parameter:}

\begin{itemize}
\tightlist
\item
  \texttt{fun\_mo2so} (callable, optional): Function to convert
  multi-objective values to single-objective

  \begin{itemize}
  \tightlist
  \item
    Takes array of shape \texttt{(n\_samples,\ n\_objectives)}
  \item
    Returns array of shape \texttt{(n\_samples,)}
  \item
    If \texttt{None}, uses first objective (default behavior)
  \end{itemize}
\end{itemize}

\textbf{Attribute:}

\begin{itemize}
\tightlist
\item
  \texttt{y\_mo} (ndarray or None): Stores all multi-objective function
  values

  \begin{itemize}
  \tightlist
  \item
    Shape: \texttt{(n\_samples,\ n\_objectives)} for multi-objective
    problems
  \item
    \texttt{None} for single-objective problems
  \end{itemize}
\end{itemize}

\textbf{Methods:}

\begin{itemize}
\tightlist
\item
  \texttt{\_get\_shape(y)}: Get shape of objective function output
\item
  \texttt{\_store\_mo(y\_mo)}: Store multi-objective values with
  automatic appending
\item
  \texttt{\_mo2so(y\_mo)}: Convert multi-objective to single-objective
  values
\end{itemize}

The method \texttt{\_evaluate\_function(X)} automatically detects
multi-objective functions. It calls \texttt{\_mo2so()} to convert
multi-objective to single-objective. It also stores the original
multi-objective values in \texttt{y\_mo}. And it returns
single-objective values for optimization.

\section{Usage Examples}\label{usage-examples-1}

\subsection{Example 1: Default Behavior (Use First
Objective)}\label{example-1-default-behavior-use-first-objective}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ bi\_objective(X):}
    \CommentTok{"""Two conflicting objectives."""}
\NormalTok{    obj1 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)          }\CommentTok{\# Minimize at origin}
\NormalTok{    obj2 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((X }\OperatorTok{{-}} \DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)    }\CommentTok{\# Minimize at (2, 2)}
    \ControlFlowTok{return}\NormalTok{ np.column\_stack([obj1, obj2])}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{bi\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best x: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)                    }\CommentTok{\# Near [0, 0]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best f(x): }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)               }\CommentTok{\# Minimizes obj1}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"MO values stored: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{y\_mo}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# (30, 2)}
\end{Highlighting}
\end{Shaded}

\subsection{Example 2: Weighted Sum
Scalarization}\label{example-2-weighted-sum-scalarization}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ weighted\_sum(y\_mo):}
    \CommentTok{"""Equal weighting of objectives."""}
    \ControlFlowTok{return} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ y\_mo[:, }\DecValTok{0}\NormalTok{] }\OperatorTok{+} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ y\_mo[:, }\DecValTok{1}\NormalTok{]}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{bi\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    fun\_mo2so}\OperatorTok{=}\NormalTok{weighted\_sum,  }\CommentTok{\# Custom conversion}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Compromise solution: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# Near [1, 1]}
\end{Highlighting}
\end{Shaded}

\subsection{Example 3: Min-Max
Scalarization}\label{example-3-min-max-scalarization}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ min\_max(y\_mo):}
    \CommentTok{"""Minimize the maximum objective."""}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(y\_mo, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{bi\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    fun\_mo2so}\OperatorTok{=}\NormalTok{min\_max,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\CommentTok{\# Finds solution with balanced objective values}
\end{Highlighting}
\end{Shaded}

\subsection{Example 4: Three or More
Objectives}\label{example-4-three-or-more-objectives}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ three\_objectives(X):}
    \CommentTok{"""Three different norms."""}
\NormalTok{    obj1 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)           }\CommentTok{\# L2 norm}
\NormalTok{    obj2 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)      }\CommentTok{\# L1 norm}
\NormalTok{    obj3 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)      }\CommentTok{\# L{-}infinity norm}
    \ControlFlowTok{return}\NormalTok{ np.column\_stack([obj1, obj2, obj3])}

\KeywordTok{def}\NormalTok{ custom\_scalarization(y\_mo):}
    \CommentTok{"""Weighted combination."""}
    \ControlFlowTok{return} \FloatTok{0.4} \OperatorTok{*}\NormalTok{ y\_mo[:, }\DecValTok{0}\NormalTok{] }\OperatorTok{+} \FloatTok{0.3} \OperatorTok{*}\NormalTok{ y\_mo[:, }\DecValTok{1}\NormalTok{] }\OperatorTok{+} \FloatTok{0.3} \OperatorTok{*}\NormalTok{ y\_mo[:, }\DecValTok{2}\NormalTok{]}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{three\_objectives,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{35}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    fun\_mo2so}\OperatorTok{=}\NormalTok{custom\_scalarization,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{Example 5: With Noise
Handling}\label{example-5-with-noise-handling}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ noisy\_bi\_objective(X):}
    \CommentTok{"""Noisy multi{-}objective function."""}
\NormalTok{    noise1 }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, X.shape[}\DecValTok{0}\NormalTok{])}
\NormalTok{    noise2 }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{, X.shape[}\DecValTok{0}\NormalTok{])}
    
\NormalTok{    obj1 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ noise1}
\NormalTok{    obj2 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((X }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\NormalTok{ noise2}
    \ControlFlowTok{return}\NormalTok{ np.column\_stack([obj1, obj2])}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{noisy\_bi\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{40}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    repeats\_initial}\OperatorTok{=}\DecValTok{3}\NormalTok{,      }\CommentTok{\# Handle noise}
\NormalTok{    repeats\_surrogate}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\CommentTok{\# Works seamlessly with noise handling}
\end{Highlighting}
\end{Shaded}

\section{Common Scalarization
Strategies}\label{common-scalarization-strategies}

\subsection{1. Weighted Sum}\label{weighted-sum}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ weighted\_sum(y\_mo, weights}\OperatorTok{=}\NormalTok{[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{]):}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(w }\OperatorTok{*}\NormalTok{ y\_mo[:, i] }\ControlFlowTok{for}\NormalTok{ i, w }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(weights))}
\end{Highlighting}
\end{Shaded}

\textbf{Use when:} Objectives have similar scales and you want linear
trade-offs

\subsection{2. Weighted Sum with
Normalization}\label{weighted-sum-with-normalization}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ normalized\_weighted\_sum(y\_mo, weights}\OperatorTok{=}\NormalTok{[}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{]):}
    \CommentTok{\# Normalize each objective to [0, 1]}
\NormalTok{    y\_norm }\OperatorTok{=}\NormalTok{ (y\_mo }\OperatorTok{{-}}\NormalTok{ y\_mo.}\BuiltInTok{min}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)) }\OperatorTok{/}\NormalTok{ (y\_mo.}\BuiltInTok{max}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{) }\OperatorTok{{-}}\NormalTok{ y\_mo.}\BuiltInTok{min}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{) }\OperatorTok{+} \FloatTok{1e{-}10}\NormalTok{)}
    \ControlFlowTok{return} \BuiltInTok{sum}\NormalTok{(w }\OperatorTok{*}\NormalTok{ y\_norm[:, i] }\ControlFlowTok{for}\NormalTok{ i, w }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(weights))}
\end{Highlighting}
\end{Shaded}

\textbf{Use when:} Objectives have very different scales

\subsection{3. Min-Max (Chebyshev)}\label{min-max-chebyshev}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ min\_max(y\_mo):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(y\_mo, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Use when:} You want balanced performance across all objectives

\subsection{4. Target Achievement}\label{target-achievement}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ target\_achievement(y\_mo, targets}\OperatorTok{=}\NormalTok{[}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{]):}
    \CommentTok{\# Minimize deviation from targets}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((y\_mo }\OperatorTok{{-}}\NormalTok{ targets)}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Use when:} You have specific target values for each objective

\subsection{5. Product}\label{product}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ product(y\_mo):}
    \ControlFlowTok{return}\NormalTok{ np.prod(y\_mo }\OperatorTok{+} \FloatTok{1e{-}10}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# Add small value to avoid zero}
\end{Highlighting}
\end{Shaded}

\textbf{Use when:} All objectives should be minimized together

\section{Integration with Other
Features}\label{integration-with-other-features}

Multi-objective support works seamlessly with:

 \textbf{Noise Handling} - Use \texttt{repeats\_initial} and
\texttt{repeats\_surrogate}\\
 \textbf{OCBA} - Use \texttt{ocba\_delta} for intelligent
re-evaluation\\
 \textbf{TensorBoard Logging} - Logs converted single-objective
values\\
 \textbf{Dimension Reduction} - Fixed dimensions work normally\\
 \textbf{Custom Variable Names} - \texttt{var\_name} parameter
supported

\section{Implementation Details}\label{implementation-details-1}

\subsection{Automatic Detection}\label{automatic-detection}

SpotOptim automatically detects multi-objective functions:

\begin{itemize}
\tightlist
\item
  If function returns 2D array (n\_samples, n\_objectives), it's
  multi-objective
\item
  If function returns 1D array (n\_samples,), it's single-objective
\end{itemize}

\subsection{Data Flow}\label{data-flow}

\begin{verbatim}
User Function  y_mo (raw)  _mo2so()  y_ (single-objective)
                    
               y_mo (stored)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Function returns multi-objective values
\item
  \texttt{\_store\_mo()} saves them in \texttt{y\_mo} attribute
\item
  \texttt{\_mo2so()} converts to single-objective using
  \texttt{fun\_mo2so} or default
\item
  Surrogate model optimizes the single-objective values
\item
  All original multi-objective values remain accessible in
  \texttt{y\_mo}
\end{enumerate}

\subsection{Backward Compatibility}\label{backward-compatibility}

 Fully backward compatible:

\begin{itemize}
\tightlist
\item
  Single-objective functions work unchanged
\item
  \texttt{fun\_mo2so} defaults to \texttt{None}
\item
  \texttt{y\_mo} is \texttt{None} for single-objective problems
\item
  No breaking changes to existing code
\end{itemize}

\section{Limitations and Notes}\label{limitations-and-notes}

\subsection{What This Is}\label{what-this-is}

\begin{itemize}
\tightlist
\item
   Scalarization approach to multi-objective optimization
\item
   Single solution found per optimization run
\item
   Different scalarizations  different Pareto solutions
\item
   Suitable for preference-based multi-objective optimization
\end{itemize}

\subsection{What This Is Not}\label{what-this-is-not}

\begin{itemize}
\tightlist
\item
   Not a true multi-objective optimizer (doesn't find Pareto front)
\item
   Doesn't generate multiple solutions in one run
\item
   Not suitable for discovering entire Pareto front
\end{itemize}

\subsection{For True Multi-Objective
Optimization}\label{for-true-multi-objective-optimization}

For finding the complete Pareto front, consider specialized tools:

\begin{itemize}
\tightlist
\item
  \textbf{pymoo}: Comprehensive multi-objective optimization framework
\item
  \textbf{platypus}: Multi-objective optimization library
\item
  \textbf{NSGA-II, MOEA/D}: Dedicated multi-objective algorithms
\end{itemize}

\section{Demo Script}\label{demo-script}

Run the comprehensive demo (the demos files are located in the
\texttt{examples} folder):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ demo\_multiobjective.py}
\end{Highlighting}
\end{Shaded}

This demonstrates:

\begin{itemize}
\tightlist
\item
  Default behavior (first objective)
\item
  Weighted sum scalarization
\item
  Min-max scalarization
\item
  Noisy multi-objective optimization
\item
  Three-objective optimization
\end{itemize}

\section{Summary}\label{summary-4}

SpotOptim provides flexible multi-objective optimization support
through:

\begin{itemize}
\tightlist
\item
  Automatic detection of multi-objective functions
\item
  Customizable scalarization strategies via \texttt{fun\_mo2so}
\item
  Complete storage of multi-objective values in \texttt{y\_mo}
\item
  Full integration with existing features (noise, OCBA, TensorBoard,
  etc.)
\item
  100\% backward compatible with existing code
\end{itemize}

This implementation mirrors the approach used in spotPython's Spot
class, providing consistency across the ecosystem.

\chapter{Surrogate Model
Visualization}\label{surrogate-model-visualization}

This document describes the \texttt{plot\_surrogate()} method added to
the \texttt{SpotOptim} class, which provides visualization capabilities
similar to the \texttt{plotkd()} function in the spotpython package.

\section{Overview}\label{overview-6}

The \texttt{plot\_surrogate()} method creates a comprehensive 4-panel
visualization of the fitted surrogate model, showing both predictions
and uncertainty estimates across two selected dimensions.

\section{Features}\label{features}

\begin{itemize}
\tightlist
\item
  \textbf{3D Surface Plots}: Visualize the surrogate's predictions and
  uncertainty as 3D surfaces
\item
  \textbf{Contour Plots}: View 2D contours with overlaid evaluation
  points
\item
  \textbf{Multi-dimensional Support}: Visualize any two dimensions of
  higher-dimensional problems
\item
  \textbf{Customizable Appearance}: Control colors, resolution,
  transparency, and more
\end{itemize}

\section{Usage}\label{usage-1}

\subsection{Basic Usage}\label{basic-usage-4}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\CommentTok{\# Define objective function}
\KeywordTok{def}\NormalTok{ sphere(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Run optimization}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{sphere, bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)], max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Visualize the surrogate model}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, show}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{With Custom Parameters}\label{with-custom-parameters}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer.plot\_surrogate(}
\NormalTok{    i}\OperatorTok{=}\DecValTok{0}\NormalTok{,                          }\CommentTok{\# First dimension to plot}
\NormalTok{    j}\OperatorTok{=}\DecValTok{1}\NormalTok{,                          }\CommentTok{\# Second dimension to plot}
\NormalTok{    var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{],        }\CommentTok{\# Variable names for axes}
\NormalTok{    add\_points}\OperatorTok{=}\VariableTok{True}\NormalTok{,              }\CommentTok{\# Show evaluated points}
\NormalTok{    cmap}\OperatorTok{=}\StringTok{\textquotesingle{}viridis\textquotesingle{}}\NormalTok{,               }\CommentTok{\# Colormap}
\NormalTok{    alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{,                    }\CommentTok{\# Surface transparency}
\NormalTok{    num}\OperatorTok{=}\DecValTok{100}\NormalTok{,                      }\CommentTok{\# Grid resolution}
\NormalTok{    contour\_levels}\OperatorTok{=}\DecValTok{25}\NormalTok{,            }\CommentTok{\# Number of contour levels}
\NormalTok{    grid\_visible}\OperatorTok{=}\VariableTok{True}\NormalTok{,            }\CommentTok{\# Show grid on contours}
\NormalTok{    figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{10}\NormalTok{),             }\CommentTok{\# Figure size}
\NormalTok{    show}\OperatorTok{=}\VariableTok{True}                     \CommentTok{\# Display immediately}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Higher-Dimensional
Problems}\label{higher-dimensional-problems}

For problems with more than 2 dimensions, \texttt{plot\_surrogate()}
creates a 2D slice by fixing all other dimensions at their mean values:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 4D optimization problem}
\KeywordTok{def}\NormalTok{ sphere\_4d(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)] }\OperatorTok{*} \DecValTok{4}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{sphere\_4d, bounds}\OperatorTok{=}\NormalTok{bounds, max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Visualize dimensions 0 and 2 (dimensions 1 and 3 fixed at mean)}
\NormalTok{optimizer.plot\_surrogate(}
\NormalTok{    i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x3\textquotesingle{}}\NormalTok{]}
\NormalTok{)}

\CommentTok{\# Visualize different dimension pair}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{1}\NormalTok{, j}\OperatorTok{=}\DecValTok{3}\NormalTok{, var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x3\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\section{Plot Interpretation}\label{plot-interpretation}

The visualization consists of 4 panels:

\subsection{Top Left: Prediction
Surface}\label{top-left-prediction-surface}

\begin{itemize}
\tightlist
\item
  Shows the surrogate model's predicted function values as a 3D surface
\item
  Helps understand the model's belief about the objective function
  landscape
\item
  Lower values (blue in default colormap) indicate predicted minima
\end{itemize}

\subsection{Top Right: Prediction Uncertainty
Surface}\label{top-right-prediction-uncertainty-surface}

\begin{itemize}
\tightlist
\item
  Shows the standard deviation of predictions as a 3D surface
\item
  Indicates where the model is uncertain and might benefit from more
  samples
\item
  Lower values (blue) indicate high confidence, higher values (red)
  indicate uncertainty
\end{itemize}

\subsection{Bottom Left: Prediction Contour with
Points}\label{bottom-left-prediction-contour-with-points}

\begin{itemize}
\tightlist
\item
  2D contour plot of predictions
\item
  Red dots show the actual points evaluated during optimization
\item
  Useful for understanding the exploration-exploitation trade-off
\end{itemize}

\subsection{Bottom Right: Uncertainty Contour with
Points}\label{bottom-right-uncertainty-contour-with-points}

\begin{itemize}
\tightlist
\item
  2D contour plot of prediction uncertainty
\item
  Shows how uncertainty decreases around evaluated points
\item
  Helps identify unexplored regions
\end{itemize}

\section{Parameters}\label{parameters}

\subsection{Dimension Selection}\label{dimension-selection}

\begin{itemize}
\tightlist
\item
  \texttt{i} (int, default=0): Index of first dimension to plot
\item
  \texttt{j} (int, default=1): Index of second dimension to plot
\end{itemize}

\subsection{Appearance}\label{appearance}

\begin{itemize}
\tightlist
\item
  \texttt{var\_name} (list of str, optional): Names for each dimension
\item
  \texttt{cmap} (str, default=`jet'): Matplotlib colormap name
\item
  \texttt{alpha} (float, default=0.8): Surface transparency
  (0=transparent, 1=opaque)
\item
  \texttt{figsize} (tuple, default=(12, 10)): Figure size in inches
  (width, height)
\end{itemize}

\subsection{Grid and Resolution}\label{grid-and-resolution}

\begin{itemize}
\tightlist
\item
  \texttt{num} (int, default=100): Number of grid points per dimension
\item
  \texttt{contour\_levels} (int, default=30): Number of contour levels
\item
  \texttt{grid\_visible} (bool, default=True): Show grid lines on
  contour plots
\end{itemize}

\subsection{Color Scaling}\label{color-scaling}

\begin{itemize}
\tightlist
\item
  \texttt{vmin} (float, optional): Minimum value for color scale
\item
  \texttt{vmax} (float, optional): Maximum value for color scale
\end{itemize}

\subsection{Display}\label{display}

\begin{itemize}
\tightlist
\item
  \texttt{show} (bool, default=True): Display plot immediately
\item
  \texttt{add\_points} (bool, default=True): Overlay evaluated points on
  contours
\end{itemize}

\section{Examples}\label{examples-2}

\subsection{Example 1: 2D Rosenbrock
Function}\label{example-1-2d-rosenbrock-function}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ rosenbrock(X):}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.atleast\_2d(X)}
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rosenbrock,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Visualize with custom colormap}
\NormalTok{optimizer.plot\_surrogate(}
\NormalTok{    var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{],}
\NormalTok{    cmap}\OperatorTok{=}\StringTok{\textquotesingle{}coolwarm\textquotesingle{}}\NormalTok{,}
\NormalTok{    add\_points}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Example 2: Using Kriging
Surrogate}\label{example-2-using-kriging-surrogate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim, Kriging}

\KeywordTok{def}\NormalTok{ sphere(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    surrogate}\OperatorTok{=}\NormalTok{Kriging(seed}\OperatorTok{=}\DecValTok{42}\NormalTok{),  }\CommentTok{\# Use Kriging instead of GP}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{20}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# The plotting works the same with any surrogate}
\NormalTok{optimizer.plot\_surrogate(var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{Example 3: Comparing Different Dimension
Pairs}\label{example-3-comparing-different-dimension-pairs}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 3D problem {-} visualize all dimension pairs}
\KeywordTok{def}\NormalTok{ sphere\_3d(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere\_3d,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)] }\OperatorTok{*} \DecValTok{3}\NormalTok{,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{25}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Dimensions 0 vs 1}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{, var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Dimensions 0 vs 2}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Dimensions 1 vs 2}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{1}\NormalTok{, j}\OperatorTok{=}\DecValTok{2}\NormalTok{, var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}x0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}x2\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\section{Tips and Best Practices}\label{tips-and-best-practices}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Run Optimization First}: Always call \texttt{optimize()}
  before \texttt{plot\_surrogate()}
\item
  \textbf{Choose Dimensions Wisely}: For high-dimensional problems, plot
  dimensions that you suspect are most important or interactive
\item
  \textbf{Adjust Resolution}: Use lower \texttt{num} values (e.g., 50)
  for faster plotting, higher values (e.g., 200) for smoother surfaces
\item
  \textbf{Color Scales}: Set \texttt{vmin} and \texttt{vmax} explicitly
  when comparing multiple plots to ensure consistent color scales
\item
  \textbf{Uncertainty Analysis}: High uncertainty areas (bright colors
  in uncertainty plots) are good candidates for additional sampling
\item
  \textbf{Exploration vs Exploitation}: Red dots clustered in
  low-prediction areas show exploitation; spread-out dots show
  exploration
\end{enumerate}

\section{Comparison with spotpython's
plotkd()}\label{comparison-with-spotpythons-plotkd}

The \texttt{plot\_surrogate()} method is inspired by spotpython's
\texttt{plotkd()} function but adapted for SpotOptim's simplified
interface:

\subsection{Similarities}\label{similarities}

\begin{itemize}
\tightlist
\item
  Same 4-panel layout (2 surfaces + 2 contours)
\item
  Visualizes predictions and uncertainty
\item
  Supports dimension selection and customization
\end{itemize}

\subsection{Differences}\label{differences}

\begin{itemize}
\tightlist
\item
  \textbf{Integration}: Method of SpotOptim class (no separate function
  needed)
\item
  \textbf{Simpler}: Fewer parameters, more sensible defaults
\item
  \textbf{Automatic}: Uses optimizer's bounds and data automatically
\item
  \textbf{Type Handling}: Automatically applies variable type
  constraints (int/float/factor)
\end{itemize}

\section{Error Handling}\label{error-handling}

The method validates inputs and provides clear error messages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Before optimization runs}
\NormalTok{optimizer.plot\_surrogate()  }\CommentTok{\# ValueError: No optimization data available}

\CommentTok{\# Invalid dimension indices}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{5}\NormalTok{, j}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# ValueError: i must be less than n\_dim}

\CommentTok{\# Same dimension twice}
\NormalTok{optimizer.plot\_surrogate(i}\OperatorTok{=}\DecValTok{0}\NormalTok{, j}\OperatorTok{=}\DecValTok{0}\NormalTok{)  }\CommentTok{\# ValueError: i and j must be different}
\end{Highlighting}
\end{Shaded}

\section{See Also}\label{see-also-2}

\begin{itemize}
\tightlist
\item
  \texttt{notebooks/demos.ipynb}: Example 4 demonstrates
  \texttt{plot\_surrogate()}
\item
  \texttt{examples/plot\_surrogate\_demo.py}: Standalone example script
\item
  \texttt{tests/test\_plot\_surrogate.py}: Comprehensive test suite
\end{itemize}

\chapter{Point Selection
Implementation}\label{point-selection-implementation}

\section{Overview}\label{overview-7}

This feature automatically selects a subset of evaluated points for
surrogate model training when the total number of points exceeds a
specified threshold.

It is implemented as a point selection mechanism for SpotOptim that
mirrors the functionality in spotpython's \texttt{Spot} class.

\section{Implementation Details}\label{implementation-details-2}

\subsection{Parameters}\label{parameters-1}

Added to \texttt{SpotOptim.\_\_init\_\_}:

\begin{itemize}
\tightlist
\item
  \texttt{max\_surrogate\_points} (int, optional): Maximum number of
  points to use for surrogate fitting
\item
  \texttt{selection\_method} (str, default=`distant'): Method for
  selecting points (`distant' or `best')
\end{itemize}

\subsection{Methods}\label{methods}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\texttt{\_select\_distant\_points(X,\ y,\ k)}}

  \begin{itemize}
  \tightlist
  \item
    Uses K-means clustering to find k clusters
  \item
    Selects the point closest to each cluster center
  \item
    Ensures space-filling properties for surrogate training
  \item
    Mimics \texttt{spotpython.utils.aggregate.select\_distant\_points}
  \end{itemize}
\item
  \textbf{\texttt{\_select\_best\_cluster(X,\ y,\ k)}}

  \begin{itemize}
  \tightlist
  \item
    Uses K-means clustering to find k clusters
  \item
    Computes mean objective value for each cluster
  \item
    Selects all points from the cluster with the best (lowest) mean
    value
  \item
    Mimics \texttt{spotpython.utils.aggregate.select\_best\_cluster}
  \end{itemize}
\item
  \textbf{\texttt{\_selection\_dispatcher(X,\ y)}}

  \begin{itemize}
  \tightlist
  \item
    Dispatcher method that routes to the appropriate selection function
  \item
    Returns all points if \texttt{max\_surrogate\_points} is None
  \item
    Mimics \texttt{spotpython.spot.spot.Spot.selection\_dispatcher}
  \end{itemize}
\end{enumerate}

The method \texttt{\_fit\_surrogate(X,\ y)} checks if
\texttt{X.shape{[}0{]}\ \textgreater{}\ self.max\_surrogate\_points}. If
true, it calls \texttt{\_selection\_dispatcher} to get a subset. Then,
it fits the surrogate only on the selected points. This implementation
matches the logic in \texttt{spotpython.spot.spot.Spot.fit\_surrogate}

\section{Key Differences from
spotpython}\label{key-differences-from-spotpython}

While the implementation follows spotpython's design, there is a
difference: \texttt{spotoptim} uses a simplified clustering, it uses
sklearn's KMeans directly instead of a custom implementation.

\section{Example Usage}\label{example-usage}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\CommentTok{\# Without point selection (default behavior)}
\NormalTok{optimizer1 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{expensive\_function,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}
\NormalTok{)}

\CommentTok{\# With point selection using distant method}
\NormalTok{optimizer2 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{expensive\_function,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    max\_surrogate\_points}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    selection\_method}\OperatorTok{=}\StringTok{\textquotesingle{}distant\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# With point selection using best cluster method}
\NormalTok{optimizer3 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{expensive\_function,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    max\_surrogate\_points}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    selection\_method}\OperatorTok{=}\StringTok{\textquotesingle{}best\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Benefits}\label{benefits-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Scalability}: Enables efficient optimization with many
  function evaluations
\item
  \textbf{Computational efficiency}: Reduces surrogate training time for
  large datasets
\item
  \textbf{Maintained accuracy}: Careful point selection preserves model
  quality
\item
  \textbf{Flexibility}: Two selection methods for different optimization
  scenarios
\end{enumerate}

\section{Comparison with spotpython}\label{comparison-with-spotpython-1}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & spotpython & SpotOptim \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Point selection via clustering &  &  \\
`distant' method &  &  \\
`best' method &  &  \\
Selection dispatcher &  &  \\
Nystrm approximation &  &  \\
Modular design &  (utils.aggregate) &  (class methods) \\
\end{longtable}

\section{References}\label{references-2}

\begin{itemize}
\tightlist
\item
  spotpython implementation: \texttt{src/spotpython/spot/spot.py} lines
  1646-1778
\item
  spotpython utilities: \texttt{src/spotpython/utils/aggregate.py} lines
  262-336
\end{itemize}

\chapter{Save and Load in SpotOptim}\label{save-and-load-in-spotoptim}

SpotOptim provides comprehensive save and load functionality for
serializing optimization configurations and results. This enables
distributed workflows where experiments are defined locally, executed
remotely, and analyzed back on the local machine.

\section{Key Concepts}\label{key-concepts}

\subsection{Experiments vs Results}\label{experiments-vs-results}

SpotOptim distinguishes between two types of saved data:

\begin{itemize}
\tightlist
\item
  \textbf{Experiment} (\texttt{*\_exp.pkl}): Configuration only,
  excluding the objective function and results. Used to transfer
  optimization setup to remote machines.
\item
  \textbf{Result} (\texttt{*\_res.pkl}): Complete optimization state
  including configuration, all evaluations, and results. Used to save
  and analyze completed optimizations.
\end{itemize}

\subsection{What Gets Saved}\label{what-gets-saved}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Component & Experiment & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Configuration (bounds, parameters) &  &  \\
Objective function &  &  \\
Evaluations (X, y) &  &  \\
Best solution &  &  \\
Surrogate model & Excluded* &  \\
TensorBoard writer &  &  \\
\end{longtable}

*Surrogate model is excluded from experiments and automatically
recreated when loaded.

\section{Quick Start}\label{quick-start-3}

\subsection{Basic Save and Load}\label{basic-save-and-load}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ sphere(X):}
    \CommentTok{"""Simple sphere function"""}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Create and configure optimizer}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\CommentTok{\# Run optimization}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Save complete results}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"sphere\_opt"}\NormalTok{)}
\CommentTok{\# Creates: sphere\_opt\_res.pkl}

\CommentTok{\# Later: load and analyze results}
\NormalTok{loaded\_opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(}\StringTok{"sphere\_opt\_res.pkl"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Loaded best value: }\SpecialCharTok{\{}\NormalTok{loaded\_opt}\SpecialCharTok{.}\NormalTok{best\_y\_}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total evaluations: }\SpecialCharTok{\{}\NormalTok{loaded\_opt}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Distributed Workflow}\label{distributed-workflow}

The save/load functionality enables a powerful workflow for distributed
optimization:

\subsection{Step 1: Define Experiment
Locally}\label{step-1-define-experiment-locally}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\CommentTok{\# Define configuration locally (no need to run optimization yet)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{200}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\CommentTok{\# Save experiment configuration}
\NormalTok{optimizer.save\_experiment(prefix}\OperatorTok{=}\StringTok{"remote\_job\_001"}\NormalTok{)}
\CommentTok{\# Creates: remote\_job\_001\_exp.pkl}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Experiment saved. Transfer remote\_job\_001\_exp.pkl to remote machine."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Step 2: Execute on Remote
Machine}\label{step-2-execute-on-remote-machine}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\# Define objective function on remote machine}
\KeywordTok{def}\NormalTok{ expensive\_function(X):}
    \CommentTok{"""Expensive simulation or computation"""}
    \CommentTok{\# Your expensive computation here}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{+} \FloatTok{0.1} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.sin(}\DecValTok{10} \OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Load experiment configuration}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim.load\_experiment(}\StringTok{"remote\_job\_001\_exp.pkl"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Experiment loaded successfully"}\NormalTok{)}

\CommentTok{\# Attach objective function (must be done after loading)}
\NormalTok{optimizer.fun }\OperatorTok{=}\NormalTok{ expensive\_function}

\CommentTok{\# Run optimization}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Optimization complete. Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Save results}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"remote\_job\_001"}\NormalTok{)}
\CommentTok{\# Creates: remote\_job\_001\_res.pkl}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Results saved. Transfer remote\_job\_001\_res.pkl back to local machine."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Step 3: Analyze Results
Locally}\label{step-3-analyze-results-locally}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Load results from remote execution}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(}\StringTok{"remote\_job\_001\_res.pkl"}\NormalTok{)}

\CommentTok{\# Access all optimization data}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value found: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{best\_y\_}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best point: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{best\_x\_}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total evaluations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Number of iterations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{n\_iter\_}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Analyze convergence}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plt.plot(optimizer.y\_, }\StringTok{\textquotesingle{}o{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Evaluations\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(optimizer.y\_)), }
\NormalTok{         [optimizer.y\_[:i}\OperatorTok{+}\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(optimizer.y\_))],}
         \StringTok{\textquotesingle{}r{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Best so far\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Iteration\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Objective Value\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Optimization Progress\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.show()}

\CommentTok{\# Access all evaluated points}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{All evaluated points shape: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{X\_}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"All objective values shape: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{y\_}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Advanced Usage}\label{advanced-usage-2}

\subsection{Custom Filenames and
Paths}\label{custom-filenames-and-paths}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ objective(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\CommentTok{\# Save with custom filename}
\NormalTok{optimizer.save\_experiment(}
\NormalTok{    filename}\OperatorTok{=}\StringTok{"custom\_name.pkl"}\NormalTok{,}
\NormalTok{    verbosity}\OperatorTok{=}\DecValTok{1}
\NormalTok{)}

\CommentTok{\# Save to specific directory}
\NormalTok{os.makedirs(}\StringTok{"experiments/batch\_001"}\NormalTok{, exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{optimizer.save\_experiment(}
\NormalTok{    prefix}\OperatorTok{=}\StringTok{"exp\_001"}\NormalTok{,}
\NormalTok{    path}\OperatorTok{=}\StringTok{"experiments/batch\_001"}\NormalTok{,}
\NormalTok{    verbosity}\OperatorTok{=}\DecValTok{1}
\NormalTok{)}
\CommentTok{\# Creates: experiments/batch\_001/exp\_001\_exp.pkl}
\end{Highlighting}
\end{Shaded}

\subsection{Overwrite Protection}\label{overwrite-protection}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ sphere(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{sphere, bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)], max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# First save}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"my\_result"}\NormalTok{)}

\CommentTok{\# Try to save again {-} raises FileExistsError by default}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"my\_result"}\NormalTok{)}
\ControlFlowTok{except} \PreprocessorTok{FileExistsError} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"File already exists: }\SpecialCharTok{\{}\NormalTok{e}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Explicitly allow overwriting}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"my\_result"}\NormalTok{, overwrite}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"File overwritten successfully"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Loading and Continuing
Optimization}\label{loading-and-continuing-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ objective(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Initial optimization}
\NormalTok{opt1 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{result1 }\OperatorTok{=}\NormalTok{ opt1.optimize()}
\NormalTok{opt1.save\_result(prefix}\OperatorTok{=}\StringTok{"checkpoint"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Initial optimization: }\SpecialCharTok{\{}\NormalTok{result1}\SpecialCharTok{.}\NormalTok{nfev}\SpecialCharTok{\}}\SpecialStringTok{ evaluations, best=}\SpecialCharTok{\{}\NormalTok{result1}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Load and continue}
\NormalTok{opt2 }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(}\StringTok{"checkpoint\_res.pkl"}\NormalTok{)}
\NormalTok{opt2.fun }\OperatorTok{=}\NormalTok{ objective  }\CommentTok{\# Re{-}attach function}
\NormalTok{opt2.max\_iter }\OperatorTok{=} \DecValTok{50}  \CommentTok{\# Increase budget}

\CommentTok{\# Continue optimization}
\NormalTok{result2 }\OperatorTok{=}\NormalTok{ opt2.optimize()}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"After continuation: }\SpecialCharTok{\{}\NormalTok{result2}\SpecialCharTok{.}\NormalTok{nfev}\SpecialCharTok{\}}\SpecialStringTok{ evaluations, best=}\SpecialCharTok{\{}\NormalTok{result2}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Working with Noisy
Functions}\label{working-with-noisy-functions}

Save and load preserves noise statistics for reproducible analysis:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ noisy\_objective(X):}
    \CommentTok{"""Objective with measurement noise"""}
\NormalTok{    true\_value }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, X.shape[}\DecValTok{0}\NormalTok{])}
    \ControlFlowTok{return}\NormalTok{ true\_value }\OperatorTok{+}\NormalTok{ noise}

\CommentTok{\# Optimize noisy function with repeated evaluations}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{noisy\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{40}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    repeats\_initial}\OperatorTok{=}\DecValTok{3}\NormalTok{,    }\CommentTok{\# Repeat initial points}
\NormalTok{    repeats\_surrogate}\OperatorTok{=}\DecValTok{2}\NormalTok{,  }\CommentTok{\# Repeat surrogate points}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Save results (includes noise statistics)}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"noisy\_opt"}\NormalTok{)}

\CommentTok{\# Load and analyze noise statistics}
\NormalTok{loaded\_opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(}\StringTok{"noisy\_opt\_res.pkl"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Noise handling enabled: }\SpecialCharTok{\{}\NormalTok{loaded\_opt}\SpecialCharTok{.}\NormalTok{noise}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best mean value: }\SpecialCharTok{\{}\NormalTok{loaded\_opt}\SpecialCharTok{.}\NormalTok{best\_y\_}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

\ControlFlowTok{if}\NormalTok{ loaded\_opt.mean\_y }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Mean values available: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(loaded\_opt.mean\_y)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Variance values available: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(loaded\_opt.var\_y)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Working with Different Variable
Types}\label{working-with-different-variable-types}

Save and load preserves variable type information:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ mixed\_objective(X):}
    \CommentTok{"""Objective with mixed variable types"""}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Create optimizer with mixed variable types}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{mixed\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{[}\StringTok{"num"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{, }\StringTok{"num"}\NormalTok{],}
\NormalTok{    var\_name}\OperatorTok{=}\NormalTok{[}\StringTok{"continuous"}\NormalTok{, }\StringTok{"integer"}\NormalTok{, }\StringTok{"categorical"}\NormalTok{, }\StringTok{"another\_cont"}\NormalTok{],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Save results}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"mixed\_vars"}\NormalTok{)}

\CommentTok{\# Load results}
\NormalTok{loaded\_opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(}\StringTok{"mixed\_vars\_res.pkl"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Variable types preserved:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  var\_type: }\SpecialCharTok{\{}\NormalTok{loaded\_opt}\SpecialCharTok{.}\NormalTok{var\_type}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  var\_name: }\SpecialCharTok{\{}\NormalTok{loaded\_opt}\SpecialCharTok{.}\NormalTok{var\_name}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Verify integer variables are still integers}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Integer variable (dim 1) values:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(loaded\_opt.X\_[:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{])  }\CommentTok{\# Should be integers}
\end{Highlighting}
\end{Shaded}

\section{Best Practices}\label{best-practices-5}

\subsection{1. Always Re-attach the Objective
Function}\label{always-re-attach-the-objective-function}

After loading an experiment, you \textbf{must} re-attach the objective
function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load experiment}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim.load\_experiment(}\StringTok{"experiment\_exp.pkl"}\NormalTok{)}

\CommentTok{\# REQUIRED: Re{-}attach function}
\NormalTok{optimizer.fun }\OperatorTok{=}\NormalTok{ your\_objective\_function}

\CommentTok{\# Now you can optimize}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{2. Use Meaningful Prefixes}\label{use-meaningful-prefixes}

Organize your experiments with descriptive prefixes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good practice: descriptive prefixes}
\NormalTok{optimizer.save\_experiment(prefix}\OperatorTok{=}\StringTok{"sphere\_d10\_seed42"}\NormalTok{)}
\NormalTok{optimizer.save\_experiment(prefix}\OperatorTok{=}\StringTok{"rosenbrock\_n100\_lhs"}\NormalTok{)}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"final\_run\_2024\_11\_15"}\NormalTok{)}

\CommentTok{\# Avoid: generic names}
\NormalTok{optimizer.save\_experiment(prefix}\OperatorTok{=}\StringTok{"exp1"}\NormalTok{)  }\CommentTok{\# Not descriptive}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"result"}\NormalTok{)     }\CommentTok{\# Hard to track}
\end{Highlighting}
\end{Shaded}

\subsection{3. Save Experiments Before Remote
Execution}\label{save-experiments-before-remote-execution}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define locally}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(bounds}\OperatorTok{=}\NormalTok{bounds, max\_iter}\OperatorTok{=}\DecValTok{500}\NormalTok{, seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{optimizer.save\_experiment(prefix}\OperatorTok{=}\StringTok{"remote\_job"}\NormalTok{)}

\CommentTok{\# Transfer file to remote machine}
\CommentTok{\# Execute remotely}
\CommentTok{\# Transfer results back}
\CommentTok{\# Analyze locally}
\end{Highlighting}
\end{Shaded}

\subsection{4. Version Your Experiments}\label{version-your-experiments}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ datetime}

\CommentTok{\# Add timestamp to prefix}
\NormalTok{timestamp }\OperatorTok{=}\NormalTok{ datetime.datetime.now().strftime(}\StringTok{"\%Y\%m}\SpecialCharTok{\%d}\StringTok{\_\%H\%M\%S"}\NormalTok{)}
\NormalTok{prefix }\OperatorTok{=} \SpecialStringTok{f"experiment\_}\SpecialCharTok{\{}\NormalTok{timestamp}\SpecialCharTok{\}}\SpecialStringTok{"}

\NormalTok{optimizer.save\_experiment(prefix}\OperatorTok{=}\NormalTok{prefix)}
\CommentTok{\# Creates: experiment\_20241115\_143022\_exp.pkl}
\end{Highlighting}
\end{Shaded}

\subsection{5. Handle File Paths
Robustly}\label{handle-file-paths-robustly}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}

\CommentTok{\# Create directory structure}
\NormalTok{exp\_dir }\OperatorTok{=} \StringTok{"experiments/batch\_001"}
\NormalTok{os.makedirs(exp\_dir, exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Save with full path}
\NormalTok{optimizer.save\_experiment(}
\NormalTok{    prefix}\OperatorTok{=}\StringTok{"exp\_001"}\NormalTok{,}
\NormalTok{    path}\OperatorTok{=}\NormalTok{exp\_dir}
\NormalTok{)}

\CommentTok{\# Load with full path}
\NormalTok{exp\_file }\OperatorTok{=}\NormalTok{ os.path.join(exp\_dir, }\StringTok{"exp\_001\_exp.pkl"}\NormalTok{)}
\NormalTok{loaded\_opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_experiment(exp\_file)}
\end{Highlighting}
\end{Shaded}

\section{Complete Example: Multi-Machine
Workflow}\label{complete-example-multi-machine-workflow}

Here's a complete example demonstrating the entire workflow:

\subsection{Local Machine (Setup)}\label{local-machine-setup}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# setup\_experiment.py}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ os}

\CommentTok{\# Create experiments directory}
\NormalTok{os.makedirs(}\StringTok{"experiments"}\NormalTok{, exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Define multiple experiments}
\NormalTok{experiments }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{}\StringTok{"seed"}\NormalTok{: }\DecValTok{42}\NormalTok{, }\StringTok{"max\_iter"}\NormalTok{: }\DecValTok{100}\NormalTok{, }\StringTok{"prefix"}\NormalTok{: }\StringTok{"exp\_seed42"}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"seed"}\NormalTok{: }\DecValTok{123}\NormalTok{, }\StringTok{"max\_iter"}\NormalTok{: }\DecValTok{100}\NormalTok{, }\StringTok{"prefix"}\NormalTok{: }\StringTok{"exp\_seed123"}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"seed"}\NormalTok{: }\DecValTok{999}\NormalTok{, }\StringTok{"max\_iter"}\NormalTok{: }\DecValTok{100}\NormalTok{, }\StringTok{"prefix"}\NormalTok{: }\StringTok{"exp\_seed999"}\NormalTok{\},}
\NormalTok{]}

\ControlFlowTok{for}\NormalTok{ exp\_config }\KeywordTok{in}\NormalTok{ experiments:}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{        bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)],}
\NormalTok{        max\_iter}\OperatorTok{=}\NormalTok{exp\_config[}\StringTok{"max\_iter"}\NormalTok{],}
\NormalTok{        n\_initial}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{        seed}\OperatorTok{=}\NormalTok{exp\_config[}\StringTok{"seed"}\NormalTok{],}
\NormalTok{        verbose}\OperatorTok{=}\VariableTok{False}
\NormalTok{    )}
    
\NormalTok{    optimizer.save\_experiment(}
\NormalTok{        prefix}\OperatorTok{=}\NormalTok{exp\_config[}\StringTok{"prefix"}\NormalTok{],}
\NormalTok{        path}\OperatorTok{=}\StringTok{"experiments"}
\NormalTok{    )}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Created: experiments/}\SpecialCharTok{\{}\NormalTok{exp\_config[}\StringTok{\textquotesingle{}prefix\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{\_exp.pkl"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{All experiments created. Transfer \textquotesingle{}experiments\textquotesingle{} folder to remote machine."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Remote Machine (Execution)}\label{remote-machine-execution}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# run\_experiments.py}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ glob}

\KeywordTok{def}\NormalTok{ complex\_objective(X):}
    \CommentTok{"""Complex multimodal objective function"""}
\NormalTok{    term1 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    term2 }\OperatorTok{=} \DecValTok{10} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.cos(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    term3 }\OperatorTok{=} \FloatTok{0.1} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.sin(}\DecValTok{5} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ term1 }\OperatorTok{{-}}\NormalTok{ term2 }\OperatorTok{+}\NormalTok{ term3}

\CommentTok{\# Find all experiment files}
\NormalTok{exp\_files }\OperatorTok{=}\NormalTok{ glob.glob(}\StringTok{"experiments/*\_exp.pkl"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Found }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(exp\_files)}\SpecialCharTok{\}}\SpecialStringTok{ experiments to run"}\NormalTok{)}

\CommentTok{\# Run each experiment}
\ControlFlowTok{for}\NormalTok{ exp\_file }\KeywordTok{in}\NormalTok{ exp\_files:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Processing: }\SpecialCharTok{\{}\NormalTok{exp\_file}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    
    \CommentTok{\# Load experiment}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim.load\_experiment(exp\_file)}
    
    \CommentTok{\# Attach objective}
\NormalTok{    optimizer.fun }\OperatorTok{=}\NormalTok{ complex\_objective}
    
    \CommentTok{\# Run optimization}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
    
    \CommentTok{\# Save result (same prefix, different suffix)}
\NormalTok{    prefix }\OperatorTok{=}\NormalTok{ os.path.basename(exp\_file).replace(}\StringTok{"\_exp.pkl"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\NormalTok{    optimizer.save\_result(}
\NormalTok{        prefix}\OperatorTok{=}\NormalTok{prefix,}
\NormalTok{        path}\OperatorTok{=}\StringTok{"experiments"}
\NormalTok{    )}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Saved: experiments/}\SpecialCharTok{\{}\NormalTok{prefix}\SpecialCharTok{\}}\SpecialStringTok{\_res.pkl"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{All experiments completed. Transfer results back to local machine."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Local Machine (Analysis)}\label{local-machine-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# analyze\_results.py}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ glob}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Find all result files}
\NormalTok{result\_files }\OperatorTok{=}\NormalTok{ glob.glob(}\StringTok{"experiments/*\_res.pkl"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Found }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(result\_files)}\SpecialCharTok{\}}\SpecialStringTok{ results to analyze"}\NormalTok{)}

\CommentTok{\# Load and compare results}
\NormalTok{results }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ res\_file }\KeywordTok{in}\NormalTok{ result\_files:}
\NormalTok{    opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(res\_file)}
\NormalTok{    results.append(\{}
        \StringTok{"file"}\NormalTok{: res\_file,}
        \StringTok{"best\_value"}\NormalTok{: opt.best\_y\_,}
        \StringTok{"best\_point"}\NormalTok{: opt.best\_x\_,}
        \StringTok{"n\_evals"}\NormalTok{: opt.counter,}
        \StringTok{"seed"}\NormalTok{: opt.seed}
\NormalTok{    \})}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{res\_file}\SpecialCharTok{\}}\SpecialStringTok{: best=}\SpecialCharTok{\{}\NormalTok{opt}\SpecialCharTok{.}\NormalTok{best\_y\_}\SpecialCharTok{:.6f\}}\SpecialStringTok{, evals=}\SpecialCharTok{\{}\NormalTok{opt}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Find best overall result}
\NormalTok{best }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(results, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x[}\StringTok{"best\_value"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Best result:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  File: }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}file\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Value: }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}best\_value\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Point: }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}best\_point\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Seed: }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}seed\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Plot convergence comparison}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ res\_file }\KeywordTok{in}\NormalTok{ result\_files:}
\NormalTok{    opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(res\_file)}
\NormalTok{    seed }\OperatorTok{=}\NormalTok{ opt.seed}
\NormalTok{    cummin }\OperatorTok{=}\NormalTok{ [opt.y\_[:i}\OperatorTok{+}\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(opt.y\_))]}
\NormalTok{    plt.plot(cummin, label}\OperatorTok{=}\SpecialStringTok{f"Seed }\SpecialCharTok{\{}\NormalTok{seed}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}

\NormalTok{plt.xlabel(}\StringTok{"Iteration"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Best Value Found"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Optimization Progress Comparison"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.savefig(}\StringTok{"experiments/convergence\_comparison.png"}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{150}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Convergence plot saved to: experiments/convergence\_comparison.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Technical Details}\label{technical-details-3}

\subsection{Serialization Method}\label{serialization-method}

SpotOptim uses Python's built-in \texttt{pickle} module for
serialization. This provides:

\begin{itemize}
\tightlist
\item
  \textbf{Standard library}: No additional dependencies required
\item
  \textbf{Compatibility}: Works with numpy arrays, sklearn models, scipy
  functions
\item
  \textbf{Performance}: Efficient serialization of large datasets
\end{itemize}

\subsection{Component
Reinitialization}\label{component-reinitialization}

When loading experiments, certain components are automatically
recreated:

\begin{itemize}
\tightlist
\item
  \textbf{Surrogate model}: Gaussian Process with default kernel
\item
  \textbf{LHS sampler}: Latin Hypercube Sampler with original seed
\end{itemize}

This ensures loaded experiments can continue optimization without manual
configuration.

\subsection{Excluded Components}\label{excluded-components}

Some components cannot be pickled and are automatically excluded:

\begin{itemize}
\tightlist
\item
  \textbf{Objective function} (\texttt{fun}): Lambda functions and local
  functions cannot be reliably pickled
\item
  \textbf{TensorBoard writer} (\texttt{tb\_writer}): File handles cannot
  be serialized
\item
  \textbf{Surrogate model} (experiments only): Recreated on load for
  experiments
\end{itemize}

\subsection{File Format}\label{file-format}

Files are saved using pickle's highest protocol:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(filename, }\StringTok{"wb"}\NormalTok{) }\ImportTok{as}\NormalTok{ handle:}
\NormalTok{    pickle.dump(optimizer\_state, handle, protocol}\OperatorTok{=}\NormalTok{pickle.HIGHEST\_PROTOCOL)}
\end{Highlighting}
\end{Shaded}

\section{Troubleshooting}\label{troubleshooting-3}

\subsection{Issue: ``AttributeError: `SpotOptim' object has no attribute
`fun'\,''}\label{issue-attributeerror-spotoptim-object-has-no-attribute-fun}

\textbf{Cause}: Objective function not re-attached after loading
experiment.

\textbf{Solution}: Always re-attach the function after loading:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_experiment(}\StringTok{"exp.pkl"}\NormalTok{)}
\NormalTok{opt.fun }\OperatorTok{=}\NormalTok{ your\_objective\_function  }\CommentTok{\# Add this line}
\NormalTok{result }\OperatorTok{=}\NormalTok{ opt.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: ``FileNotFoundError: Experiment file not
found''}\label{issue-filenotfounderror-experiment-file-not-found}

\textbf{Cause}: Incorrect file path or file doesn't exist.

\textbf{Solution}: Check file path and ensure file exists:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}

\NormalTok{filename }\OperatorTok{=} \StringTok{"experiment\_exp.pkl"}
\ControlFlowTok{if}\NormalTok{ os.path.exists(filename):}
\NormalTok{    opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_experiment(filename)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"File not found: }\SpecialCharTok{\{}\NormalTok{filename}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: ``FileExistsError: File already
exists''}\label{issue-fileexistserror-file-already-exists}

\textbf{Cause}: Attempting to save over an existing file without
\texttt{overwrite=True}.

\textbf{Solution}: Either use a different prefix or enable overwriting:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Option 1: Use different prefix}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"my\_result\_v2"}\NormalTok{)}

\CommentTok{\# Option 2: Enable overwriting}
\NormalTok{optimizer.save\_result(prefix}\OperatorTok{=}\StringTok{"my\_result"}\NormalTok{, overwrite}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Issue: Results differ after
loading}\label{issue-results-differ-after-loading}

\textbf{Cause}: Random state not preserved or function behavior changed.

\textbf{Solution}: Ensure you're using the same seed and function
definition:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# When saving}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(..., seed}\OperatorTok{=}\DecValTok{42}\NormalTok{)  }\CommentTok{\# Use fixed seed}

\CommentTok{\# When loading and continuing}
\NormalTok{loaded\_opt }\OperatorTok{=}\NormalTok{ SpotOptim.load\_result(}\StringTok{"result\_res.pkl"}\NormalTok{)}
\NormalTok{loaded\_opt.fun }\OperatorTok{=}\NormalTok{ same\_objective\_function  }\CommentTok{\# Same function definition}
\end{Highlighting}
\end{Shaded}

\section{See Also}\label{see-also-3}

\begin{itemize}
\tightlist
\item
  \href{reproducibility.md}{Reproducibility Manual}: Learn about using
  seeds for reproducible results
\item
  \href{tensorboard.md}{TensorBoard Manual}: Monitor optimization
  progress in real-time
\end{itemize}

\chapter{Success Rate Tracking in
SpotOptim}\label{success-rate-tracking-in-spotoptim}

SpotOptim tracks the \textbf{success rate} of the optimization process,
which measures how often the optimizer finds improvements over recent
evaluations. This metric helps you understand whether the optimization
is making progress or has stalled.

\section{What is Success Rate?}\label{what-is-success-rate}

The success rate is a \textbf{rolling metric} that tracks the percentage
of recent evaluations that improved upon the best value found so far.
It's calculated over a sliding window of the last 100 evaluations.

\textbf{Key Points:} - A ``success'' occurs when a new evaluation finds
a value \textbf{better (smaller)} than the best found so far - The rate
is computed over the last \textbf{100 evaluations} (window size) -
Values range from \textbf{0.0} (no recent improvements) to \textbf{1.0}
(all recent evaluations improved) - Helps identify when optimization is
stalling and may need adjustment

\section{First Example}\label{first-example}

\begin{itemize}
\tightlist
\item
  Start \texttt{TensorBoard} to visualize success rate in real-time:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{tensorboard} \AttributeTok{{-}{-}logdir}\OperatorTok{=}\NormalTok{runs}
\end{Highlighting}
\end{Shaded}

The execute the following code:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ rosenbrock(X):}
    \CommentTok{"""Rosenbrock function {-} challenging optimization problem"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\CommentTok{\# Run optimization with periodic success rate checks}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rosenbrock,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{200}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_clean}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Analyze final success rate}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Optimization Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total evaluations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Final success rate: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{success\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Interpret the result}
\ControlFlowTok{if}\NormalTok{ optimizer.success\_rate }\OperatorTok{\textgreater{}} \FloatTok{0.5}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" High success rate: Optimization is still making good progress"}\NormalTok{)}
\ControlFlowTok{elif}\NormalTok{ optimizer.success\_rate }\OperatorTok{\textgreater{}} \FloatTok{0.2}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" Medium success rate: Approaching convergence"}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" Low success rate: Optimization has likely converged"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Optimization Results:
Best value: 0.000000
Total evaluations: 200
Final success rate: 4.00%
 Low success rate: Optimization has likely converged
\end{verbatim}

\section{Second Example}\label{second-example}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ sphere(X):}
    \CommentTok{"""Simple sphere function: f(x) = sum(x\^{}2)"""}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Create optimizer}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\CommentTok{\# Run optimization}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Check success rate}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Final success rate: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{success\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total evaluations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Accessing Success Rate}\label{accessing-success-rate}

The success rate is stored in the \texttt{success\_rate} attribute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{objective, bounds}\OperatorTok{=}\NormalTok{bounds, max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Access success rate}
\NormalTok{current\_rate }\OperatorTok{=}\NormalTok{ optimizer.success\_rate}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Success rate: }\SpecialCharTok{\{}\NormalTok{current\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Also available via getter method}
\NormalTok{rate }\OperatorTok{=}\NormalTok{ optimizer.\_get\_success\_rate()}
\end{Highlighting}
\end{Shaded}

\section{Interpreting Success Rate}\label{interpreting-success-rate}

\subsection{High Success Rate (\textgreater{}
0.5)}\label{high-success-rate-0.5}

\begin{verbatim}
Success Rate: 75%
\end{verbatim}

\textbf{Interpretation:} The optimizer is finding improvements
frequently. This typically indicates: - The optimization is in an
exploratory phase - The surrogate model is effectively guiding the
search - There's still room for improvement in the search space

\textbf{Action:} Continue optimization - progress is good!

\subsection{Medium Success Rate (0.2 -
0.5)}\label{medium-success-rate-0.2---0.5}

\begin{verbatim}
Success Rate: 35%
\end{verbatim}

\textbf{Interpretation:} The optimizer occasionally finds improvements.
This suggests: - The search is becoming more refined - The optimizer is
balancing exploration and exploitation - Approaching a local or global
optimum

\textbf{Action:} Monitor progress and consider stopping criteria.

\subsection{Low Success Rate (\textless{}
0.2)}\label{low-success-rate-0.2}

\begin{verbatim}
Success Rate: 8%
\end{verbatim}

\textbf{Interpretation:} Few recent evaluations improve the best value.
This may indicate: - The optimization has converged to a (local) optimum
- The search is stuck in a plateau region - The budget may be exhausted
in terms of meaningful progress

\textbf{Action:} Consider stopping optimization or adjusting parameters.

\section{TensorBoard Visualization}\label{tensorboard-visualization}

When TensorBoard logging is enabled, success rate is automatically
logged and can be visualized in real-time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,  }\CommentTok{\# Enable logging}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\textbf{View in TensorBoard:}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{tensorboard} \AttributeTok{{-}{-}logdir}\OperatorTok{=}\NormalTok{runs}
\end{Highlighting}
\end{Shaded}

In the TensorBoard interface, look for: - \textbf{SCALARS} tab 
\texttt{success\_rate}: Rolling success rate over iterations - Compare
multiple runs side-by-side - Identify when optimization stalls

\section{Example: Monitoring Optimization
Progress}\label{example-monitoring-optimization-progress}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ rosenbrock(X):}
    \CommentTok{"""Rosenbrock function {-} challenging optimization problem"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{100} \OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\OperatorTok{**}\DecValTok{2}

\CommentTok{\# Run optimization with periodic success rate checks}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{rosenbrock,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\CommentTok{\# Analyze final success rate}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Optimization Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total evaluations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Final success rate: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{success\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Interpret the result}
\ControlFlowTok{if}\NormalTok{ optimizer.success\_rate }\OperatorTok{\textgreater{}} \FloatTok{0.5}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" High success rate: Optimization is still making good progress"}\NormalTok{)}
\ControlFlowTok{elif}\NormalTok{ optimizer.success\_rate }\OperatorTok{\textgreater{}} \FloatTok{0.2}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" Medium success rate: Approaching convergence"}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{" Low success rate: Optimization has likely converged"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Example: Comparing Multiple
Runs}\label{example-comparing-multiple-runs}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ ackley(X):}
    \CommentTok{"""Ackley function {-} multimodal test function"""}
\NormalTok{    a }\OperatorTok{=} \DecValTok{20}
\NormalTok{    b }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{    c }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.shape[}\DecValTok{1}\NormalTok{]}
    
\NormalTok{    sum\_sq }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    sum\_cos }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.cos(c }\OperatorTok{*}\NormalTok{ X), axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
    
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{a }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{b }\OperatorTok{*}\NormalTok{ np.sqrt(sum\_sq }\OperatorTok{/}\NormalTok{ n)) }\OperatorTok{{-}}\NormalTok{ np.exp(sum\_cos }\OperatorTok{/}\NormalTok{ n) }\OperatorTok{+}\NormalTok{ a }\OperatorTok{+}\NormalTok{ np.e}

\CommentTok{\# Run with different configurations}
\NormalTok{configs }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    \{}\StringTok{"n\_initial"}\NormalTok{: }\DecValTok{10}\NormalTok{, }\StringTok{"max\_iter"}\NormalTok{: }\DecValTok{50}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"Small initial"}\NormalTok{\},}
\NormalTok{    \{}\StringTok{"n\_initial"}\NormalTok{: }\DecValTok{30}\NormalTok{, }\StringTok{"max\_iter"}\NormalTok{: }\DecValTok{50}\NormalTok{, }\StringTok{"name"}\NormalTok{: }\StringTok{"Large initial"}\NormalTok{\},}
\NormalTok{]}

\NormalTok{results }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ config }\KeywordTok{in}\NormalTok{ configs:}
\NormalTok{    optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{        fun}\OperatorTok{=}\NormalTok{ackley,}
\NormalTok{        bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{        n\_initial}\OperatorTok{=}\NormalTok{config[}\StringTok{"n\_initial"}\NormalTok{],}
\NormalTok{        max\_iter}\OperatorTok{=}\NormalTok{config[}\StringTok{"max\_iter"}\NormalTok{],}
\NormalTok{        seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{        verbose}\OperatorTok{=}\VariableTok{False}
\NormalTok{    )}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
    
\NormalTok{    results.append(\{}
        \StringTok{"name"}\NormalTok{: config[}\StringTok{"name"}\NormalTok{],}
        \StringTok{"best\_value"}\NormalTok{: result.fun,}
        \StringTok{"success\_rate"}\NormalTok{: optimizer.success\_rate,}
        \StringTok{"n\_evals"}\NormalTok{: optimizer.counter}
\NormalTok{    \})}
    
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialCharTok{\{}\NormalTok{config[}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Best value: }\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Success rate: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{success\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Evaluations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Find best configuration}
\NormalTok{best }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(results, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x[}\StringTok{"best\_value"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Best configuration: }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Achieved: f(x) = }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}best\_value\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  Final success rate: }\SpecialCharTok{\{}\NormalTok{best[}\StringTok{\textquotesingle{}success\_rate\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Success Rate with Noisy
Functions}\label{success-rate-with-noisy-functions}

For noisy functions (when \texttt{repeats\_initial\ \textgreater{}\ 1}
or \texttt{repeats\_surrogate\ \textgreater{}\ 1}), the success rate
tracks improvements in the \textbf{raw} y values, not the aggregated
means:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\KeywordTok{def}\NormalTok{ noisy\_sphere(X):}
    \CommentTok{"""Sphere function with Gaussian noise"""}
\NormalTok{    base }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, size}\OperatorTok{=}\NormalTok{base.shape)}
    \ControlFlowTok{return}\NormalTok{ base }\OperatorTok{+}\NormalTok{ noise}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{noisy\_sphere,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    repeats\_initial}\OperatorTok{=}\DecValTok{3}\NormalTok{,    }\CommentTok{\# 3 evaluations per initial point}
\NormalTok{    repeats\_surrogate}\OperatorTok{=}\DecValTok{2}\NormalTok{,  }\CommentTok{\# 2 evaluations per new point}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}n}\SpecialStringTok{Noisy Optimization Results:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best raw value: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{min\_y}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Best mean value: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{min\_mean\_y}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Success rate: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{success\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total evaluations: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{counter}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Unique design points: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{mean\_X}\SpecialCharTok{.}\NormalTok{shape[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Note:} With noisy functions, the success rate may be lower
because: - Noise can mask true improvements - Multiple evaluations of
the same point contribute to the window - Focus on the mean values
(\texttt{min\_mean\_y}) for better assessment

\section{Advanced: Custom Window
Size}\label{advanced-custom-window-size}

The success rate is calculated over a window of 100 evaluations by
default. This is controlled by the \texttt{window\_size} attribute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{10}
\NormalTok{)}

\CommentTok{\# Check default window size}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Window size: }\SpecialCharTok{\{}\NormalTok{optimizer}\SpecialCharTok{.}\NormalTok{window\_size}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# 100}

\CommentTok{\# The window size is set during initialization}
\CommentTok{\# To use a different window, you would need to modify it}
\CommentTok{\# before running optimization (not typically recommended)}
\end{Highlighting}
\end{Shaded}

\section{Best Practices}\label{best-practices-6}

\subsection{1. Monitor During Long Runs}\label{monitor-during-long-runs}

For expensive optimization runs, periodically check success rate:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Could be implemented with callbacks in future versions}
\CommentTok{\# For now, success rate is updated automatically and logged to TensorBoard}
\end{Highlighting}
\end{Shaded}

\subsection{2. Combine with TensorBoard}\label{combine-with-tensorboard}

Always enable TensorBoard logging for visual monitoring:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{expensive\_function,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{1000}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,  }\CommentTok{\# Track success\_rate visually}
\NormalTok{    tensorboard\_path}\OperatorTok{=}\StringTok{"runs/long\_optimization"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{3. Use as Stopping
Criterion}\label{use-as-stopping-criterion}

Consider stopping when success rate drops very low:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Manual stopping check (conceptual)}
\ControlFlowTok{if}\NormalTok{ optimizer.success\_rate }\OperatorTok{\textless{}} \FloatTok{0.05} \KeywordTok{and}\NormalTok{ optimizer.counter }\OperatorTok{\textgreater{}} \DecValTok{50}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Success rate very low {-} optimization has likely converged"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{4. Compare Different
Strategies}\label{compare-different-strategies}

Use success rate to compare optimization strategies:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strategies }\OperatorTok{=}\NormalTok{ [}\StringTok{"ei"}\NormalTok{, }\StringTok{"pi"}\NormalTok{, }\StringTok{"y"}\NormalTok{]  }\CommentTok{\# Different acquisition functions}
\ControlFlowTok{for}\NormalTok{ acq }\KeywordTok{in}\NormalTok{ strategies:}
\NormalTok{    opt }\OperatorTok{=}\NormalTok{ SpotOptim(fun}\OperatorTok{=}\NormalTok{obj, bounds}\OperatorTok{=}\NormalTok{bnds, acquisition}\OperatorTok{=}\NormalTok{acq, max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ opt.optimize()}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{acq}\SpecialCharTok{\}}\SpecialStringTok{: success\_rate=}\SpecialCharTok{\{}\NormalTok{opt}\SpecialCharTok{.}\NormalTok{success\_rate}\SpecialCharTok{:.2\%\}}\SpecialStringTok{, best=}\SpecialCharTok{\{}\NormalTok{result}\SpecialCharTok{.}\NormalTok{fun}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Technical Details}\label{technical-details-4}

\subsection{How Success is Counted}\label{how-success-is-counted}

A new evaluation \texttt{y\_new} is considered a success if:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_new }\OperatorTok{\textless{}}\NormalTok{ best\_y\_so\_far}
\end{Highlighting}
\end{Shaded}

where \texttt{best\_y\_so\_far} is the minimum value found in all
previous evaluations.

\subsection{Rolling Window
Calculation}\label{rolling-window-calculation}

The success rate is computed as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{success\_rate }\OperatorTok{=}\NormalTok{ (number of successes }\KeywordTok{in}\NormalTok{ last }\DecValTok{100}\NormalTok{ evals) }\OperatorTok{/}\NormalTok{ (window size)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Window size defaults to 100
\item
  If fewer than 100 evaluations have been performed, the window size is
  the number of evaluations
\item
  The window slides forward with each new evaluation
\end{itemize}

\subsection{Update Frequency}\label{update-frequency}

The success rate is updated after: 1. Initial design evaluation 2. Each
iteration's new point evaluation(s) 3. OCBA re-evaluations (if
applicable)

\section{Summary}\label{summary-5}

\begin{itemize}
\tightlist
\item
  \textbf{Success rate} measures the percentage of recent evaluations
  that improve the best value
\item
  Calculated over a rolling window of the last \textbf{100 evaluations}
\item
  Values range from \textbf{0.0} to \textbf{1.0}
\item
  High rates (\textgreater0.5) indicate active progress
\item
  Low rates (\textless0.2) suggest convergence
\item
  Automatically logged to \textbf{TensorBoard} when logging is enabled
\item
  Available via \texttt{optimizer.success\_rate} attribute after
  optimization
\end{itemize}

Use success rate to: -  Monitor optimization progress in real-time - 
Identify when to stop optimization -  Compare different optimization
strategies -  Assess optimization difficulty for different problems

\chapter{TensorBoard Log Cleaning
Feature}\label{tensorboard-log-cleaning-feature}

\section{Summary}\label{summary-6}

Automatic cleaning of old TensorBoard log directories with the
\texttt{tensorboard\_clean} parameter.

\section{Usage}\label{usage-2}

\subsection{Basic Usage}\label{basic-usage-5}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\CommentTok{\# Remove old logs and create new log directory}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_clean}\OperatorTok{=}\VariableTok{True}\NormalTok{,  }\CommentTok{\# Removes all subdirectories in \textquotesingle{}runs\textquotesingle{}}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{Use Cases}\label{use-cases-1}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\texttt{tensorboard\_log} & \texttt{tensorboard\_clean} & Behavior \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{True} & \texttt{True} & Clean old logs, create new log
directory \\
\texttt{True} & \texttt{False} & Preserve old logs, create new log
directory \\
\texttt{False} & \texttt{True} & Clean old logs, no new logging \\
\texttt{False} & \texttt{False} & No logging, no cleaning (default) \\
\end{longtable}

\section{Implementation Details}\label{implementation-details-3}

\subsection{Cleaning Method}\label{cleaning-method}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ \_clean\_tensorboard\_logs(}\VariableTok{self}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \VariableTok{None}\NormalTok{:}
    \CommentTok{"""Clean old TensorBoard log directories from the runs folder."""}
    \ControlFlowTok{if} \VariableTok{self}\NormalTok{.tensorboard\_clean:}
\NormalTok{        runs\_dir }\OperatorTok{=} \StringTok{"runs"}
        \ControlFlowTok{if}\NormalTok{ os.path.exists(runs\_dir) }\KeywordTok{and}\NormalTok{ os.path.isdir(runs\_dir):}
            \CommentTok{\# Get all subdirectories in runs}
\NormalTok{            subdirs }\OperatorTok{=}\NormalTok{ [}
\NormalTok{                os.path.join(runs\_dir, d)}
                \ControlFlowTok{for}\NormalTok{ d }\KeywordTok{in}\NormalTok{ os.listdir(runs\_dir)}
                \ControlFlowTok{if}\NormalTok{ os.path.isdir(os.path.join(runs\_dir, d))}
\NormalTok{            ]}
            
            \CommentTok{\# Remove each subdirectory}
            \ControlFlowTok{for}\NormalTok{ subdir }\KeywordTok{in}\NormalTok{ subdirs:}
                \ControlFlowTok{try}\NormalTok{:}
\NormalTok{                    shutil.rmtree(subdir)}
                    \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbose:}
                        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Removed old TensorBoard logs: }\SpecialCharTok{\{}\NormalTok{subdir}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
                \ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
                    \ControlFlowTok{if} \VariableTok{self}\NormalTok{.verbose:}
                        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Warning: Could not remove }\SpecialCharTok{\{}\NormalTok{subdir}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{e}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Execution Flow}\label{execution-flow}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  User creates \texttt{SpotOptim} instance with
  \texttt{tensorboard\_clean=True}
\item
  During initialization, \texttt{\_clean\_tensorboard\_logs()} is called
\item
  Method checks if `runs' directory exists
\item
  Removes all subdirectories (but preserves files)
\item
  If \texttt{tensorboard\_log=True}, a new log directory is created
\item
  Optimization proceeds normally
\end{enumerate}

\section{Safety Features}\label{safety-features}

\begin{itemize}
\tightlist
\item
  Only removes \textbf{directories}, not files in `runs' folder
\item
  Handles missing `runs' directory gracefully
\item
  Error handling for permission issues
\item
  Verbose output shows what's being removed
\item
  Default is \texttt{False} to prevent accidental deletion
\end{itemize}

\section{Warning}\label{warning}

 \textbf{IMPORTANT}: Setting \texttt{tensorboard\_clean=True}
permanently deletes all subdirectories in the `runs' folder. Make sure
to save important logs elsewhere before enabling this feature.

\chapter{TensorBoard Logging in
SpotOptim}\label{tensorboard-logging-in-spotoptim}

SpotOptim supports TensorBoard logging for monitoring optimization
progress in real-time.

\section{Quick Start}\label{quick-start-4}

\subsection{1. Enable TensorBoard
Logging}\label{enable-tensorboard-logging}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\KeywordTok{def}\NormalTok{ objective(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    n\_initial}\OperatorTok{=}\DecValTok{15}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,  }\CommentTok{\# Enable logging}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{2. View Logs in TensorBoard}\label{view-logs-in-tensorboard}

In a separate terminal, run:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{tensorboard} \AttributeTok{{-}{-}logdir}\OperatorTok{=}\NormalTok{runs}
\end{Highlighting}
\end{Shaded}

Then open your browser to \url{http://localhost:6006}

\section{Cleaning Old Logs}\label{cleaning-old-logs}

You can automatically remove old TensorBoard logs before starting a new
optimization:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_clean}\OperatorTok{=}\VariableTok{True}\NormalTok{,  }\CommentTok{\# Remove old logs from \textquotesingle{}runs\textquotesingle{} directory}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Warning:} This permanently deletes all subdirectories in the
\texttt{runs} folder. Make sure to save important logs elsewhere before
enabling this feature.

\subsection{Use Cases}\label{use-cases-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Clean Start} - Remove old logs and create new one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{, tensorboard\_clean}\OperatorTok{=}\VariableTok{True}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Preserve History} - Keep old logs and add new one (default):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{, tensorboard\_clean}\OperatorTok{=}\VariableTok{False}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Just Clean} - Remove old logs without new logging:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard\_log}\OperatorTok{=}\VariableTok{False}\NormalTok{, tensorboard\_clean}\OperatorTok{=}\VariableTok{True}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Custom Log Directory}\label{custom-log-directory}

Specify a custom path for TensorBoard logs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_path}\OperatorTok{=}\StringTok{"my\_experiments/run\_001"}\NormalTok{,}
\NormalTok{    ...}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{What Gets Logged}\label{what-gets-logged}

\subsection{Scalar Metrics}\label{scalar-metrics}

\textbf{For Deterministic Functions:}

\begin{itemize}
\tightlist
\item
  \texttt{y\_values/min}: Best (minimum) y value found so far
\item
  \texttt{y\_values/last}: Most recently evaluated y value
\item
  \texttt{X\_best/x0,\ X\_best/x1,\ ...}: Coordinates of the best point
\end{itemize}

\textbf{For Noisy Functions (repeats \textgreater{} 1):}

\begin{itemize}
\tightlist
\item
  \texttt{y\_values/min}: Best single evaluation
\item
  \texttt{y\_values/mean\_best}: Best mean y value
\item
  \texttt{y\_values/last}: Most recent evaluation
\item
  \texttt{y\_variance\_at\_best}: Variance at the best mean point
\item
  \texttt{X\_mean\_best/x0,\ X\_mean\_best/x1,\ ...}: Coordinates of
  best mean point
\end{itemize}

\subsection{Hyperparameters}\label{hyperparameters}

Each function evaluation is logged with:

\begin{itemize}
\tightlist
\item
  Input coordinates (x0, x1, x2, \ldots)
\item
  Function value (hp\_metric)
\end{itemize}

This allows you to explore the relationship between hyperparameters and
objective values in the HPARAMS tab.

\section{Examples}\label{examples-3}

\subsection{Basic Usage}\label{basic-usage-6}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ X: np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{),}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{30}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    verbose}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{Noisy Optimization}\label{noisy-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ noisy\_objective(X):}
\NormalTok{    base }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(X}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    noise }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, size}\OperatorTok{=}\NormalTok{base.shape)}
    \ControlFlowTok{return}\NormalTok{ base }\OperatorTok{+}\NormalTok{ noise}

\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{noisy\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    repeats\_initial}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{    repeats\_surrogate}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    tensorboard\_path}\OperatorTok{=}\StringTok{"runs/noisy\_exp"}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\subsection{With OCBA}\label{with-ocba}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{noisy\_objective,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{[(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)],}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    repeats\_initial}\OperatorTok{=}\DecValTok{2}\NormalTok{,}
\NormalTok{    ocba\_delta}\OperatorTok{=}\DecValTok{3}\NormalTok{,  }\CommentTok{\# Re{-}evaluate 3 promising points per iteration}
\NormalTok{    tensorboard\_log}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ optimizer.optimize()}
\end{Highlighting}
\end{Shaded}

\section{Comparing Multiple Runs}\label{comparing-multiple-runs}

Run multiple optimizations with different settings:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run 1: Standard}
\NormalTok{opt1 }\OperatorTok{=}\NormalTok{ SpotOptim(..., tensorboard\_path}\OperatorTok{=}\StringTok{"runs/standard"}\NormalTok{)}
\NormalTok{opt1.optimize()}

\CommentTok{\# Run 2: With OCBA}
\NormalTok{opt2 }\OperatorTok{=}\NormalTok{ SpotOptim(..., ocba\_delta}\OperatorTok{=}\DecValTok{3}\NormalTok{, tensorboard\_path}\OperatorTok{=}\StringTok{"runs/with\_ocba"}\NormalTok{)}
\NormalTok{opt2.optimize()}

\CommentTok{\# Run 3: More initial points}
\NormalTok{opt3 }\OperatorTok{=}\NormalTok{ SpotOptim(..., n\_initial}\OperatorTok{=}\DecValTok{20}\NormalTok{, tensorboard\_path}\OperatorTok{=}\StringTok{"runs/more\_initial"}\NormalTok{)}
\NormalTok{opt3.optimize()}
\end{Highlighting}
\end{Shaded}

Then view all runs together:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{tensorboard} \AttributeTok{{-}{-}logdir}\OperatorTok{=}\NormalTok{runs}
\end{Highlighting}
\end{Shaded}

\section{TensorBoard Features}\label{tensorboard-features}

\subsection{SCALARS Tab}\label{scalars-tab}

\begin{itemize}
\tightlist
\item
  View convergence curves
\item
  Compare optimization progress across runs
\item
  Track how metrics change over iterations
\end{itemize}

\subsection{HPARAMS Tab}\label{hparams-tab}

\begin{itemize}
\tightlist
\item
  Explore hyperparameter space
\item
  See which parameter combinations work best
\item
  Identify patterns in successful configurations
\end{itemize}

\subsection{Text Tab}\label{text-tab}

\begin{itemize}
\tightlist
\item
  View configuration details
\item
  Check run metadata
\end{itemize}

\section{Tips}\label{tips}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Organize Experiments}: Use descriptive tensorboard\_path
  names:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorboard\_path}\OperatorTok{=}\SpecialStringTok{f"runs/exp\_}\SpecialCharTok{\{}\NormalTok{date}\SpecialCharTok{\}}\SpecialStringTok{\_}\SpecialCharTok{\{}\NormalTok{config\_name}\SpecialCharTok{\}}\SpecialStringTok{"}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Compare Algorithms}: Run multiple optimization strategies and
  compare:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Different acquisition functions}
\ControlFlowTok{for}\NormalTok{ acq }\KeywordTok{in}\NormalTok{ [}\StringTok{\textquotesingle{}ei\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}pi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{]:}
\NormalTok{    opt }\OperatorTok{=}\NormalTok{ SpotOptim(..., acquisition}\OperatorTok{=}\NormalTok{acq, tensorboard\_path}\OperatorTok{=}\SpecialStringTok{f"runs/acq\_}\SpecialCharTok{\{}\NormalTok{acq}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    opt.optimize()}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Clean Up Old Runs}: Use \texttt{tensorboard\_clean=True} for
  automatic cleanup, or manually:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm} \AttributeTok{{-}rf}\NormalTok{ runs/old\_experiment}
\end{Highlighting}
\end{Shaded}
\item
  \textbf{Port Conflicts}: If port 6006 is busy, use a different port:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{tensorboard} \AttributeTok{{-}{-}logdir}\OperatorTok{=}\NormalTok{runs }\AttributeTok{{-}{-}port}\OperatorTok{=}\NormalTok{6007}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

\section{Demo Scripts}\label{demo-scripts}

Run the comprehensive TensorBoard demo:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ demo\_tensorboard.py}
\end{Highlighting}
\end{Shaded}

This demonstrates:

\begin{itemize}
\tightlist
\item
  Deterministic optimization (Rosenbrock function)
\item
  Noisy optimization with repeated evaluations
\item
  OCBA for intelligent re-evaluation
\end{itemize}

Run the log cleaning demo:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ demo\_tensorboard\_clean.py}
\end{Highlighting}
\end{Shaded}

This demonstrates:

\begin{itemize}
\tightlist
\item
  Creating multiple log directories
\item
  Preserving old logs (default behavior)
\item
  Cleaning old logs automatically
\item
  Cleaning without creating new logs
\end{itemize}

This demonstrates:

\begin{itemize}
\tightlist
\item
  Deterministic optimization (Rosenbrock function)
\item
  Noisy optimization with repeated evaluations
\item
  OCBA for intelligent re-evaluation
\end{itemize}

\section{Troubleshooting}\label{troubleshooting-4}

\textbf{Q: TensorBoard shows ``No dashboards are active''} A: Make sure
you've run an optimization with \texttt{tensorboard\_log=True} first.

\textbf{Q: Can't see my latest run} A: Refresh TensorBoard (click the
reload button in the upper right).

\textbf{Q: How do I stop TensorBoard?} A: Press Ctrl+C in the terminal
where TensorBoard is running.

\textbf{Q: Logs taking up too much space?} A: Use
\texttt{tensorboard\_clean=True} to automatically remove old logs, or
manually delete old run directories.

\textbf{Q: How do I remove all old logs at once?} A: Set
\texttt{tensorboard\_clean=True} when creating your optimizer. This will
remove all subdirectories in the \texttt{runs} folder.

\section{Related Parameters}\label{related-parameters}

\begin{itemize}
\tightlist
\item
  \texttt{tensorboard\_log} (bool): Enable/disable logging (default:
  False)
\item
  \texttt{tensorboard\_path} (str): Custom log directory (default:
  auto-generated with timestamp)
\item
  \texttt{tensorboard\_clean} (bool): Remove old logs from `runs'
  directory before starting (default: False)
\item
  \texttt{verbose} (bool): Print progress to console (default: False)
\item
  \texttt{var\_name} (list): Custom names for variables (used in
  TensorBoard labels)
\end{itemize}

\section{Performance Notes}\label{performance-notes}

TensorBoard logging has minimal overhead:

\begin{itemize}
\tightlist
\item
  \textless{} 1\% slowdown for typical optimizations
\item
  Event files are efficiently buffered and written
\item
  Writer is properly closed after optimization completes
\end{itemize}

\chapter{Variable Type (var\_type)
Implementation}\label{variable-type-var_type-implementation}

\section{Overview}\label{overview-8}

This document describes the \texttt{var\_type} implementation in
SpotOptim, which allows users to specify different data types for
optimization variables.

\section{Supported Variable Types}\label{supported-variable-types}

SpotOptim supports three main data types:

\subsection{\texorpdfstring{1.
\textbf{`float'}}{1. `float'}}\label{float}

\begin{itemize}
\tightlist
\item
  \textbf{Purpose}: Continuous optimization with Python floats
\item
  \textbf{Behavior}: No rounding applied, values remain continuous
\item
  \textbf{Use case}: Standard continuous optimization variables
\item
  \textbf{Example}: Temperature (23.5C), Distance (1.234m)
\end{itemize}

\subsection{\texorpdfstring{2. \textbf{`int'}}{2. `int'}}\label{int}

\begin{itemize}
\tightlist
\item
  \textbf{Purpose}: Discrete integer optimization
\item
  \textbf{Behavior}: Float values are automatically rounded to integers
\item
  \textbf{Use case}: Count variables, discrete parameters
\item
  \textbf{Example}: Number of layers (5), Population size (100)
\end{itemize}

\subsection{\texorpdfstring{3.
\textbf{`factor'}}{3. `factor'}}\label{factor}

\begin{itemize}
\tightlist
\item
  \textbf{Purpose}: Unordered categorical data
\item
  \textbf{Behavior}: Internally mapped to integer values (0, 1, 2,
  \ldots)
\item
  \textbf{Use case}: Categorical choices like colors, algorithms, modes
\item
  \textbf{Example}: Color (``red''0, ``green''1, ``blue''2)
\item
  \textbf{Note}: The actual string-to-int mapping is external to
  SpotOptim; the optimizer works with the integer representation
\end{itemize}

\section{Implementation Details}\label{implementation-details-4}

\subsection{\texorpdfstring{Where \texttt{var\_type} is
Used}{Where var\_type is Used}}\label{where-var_type-is-used}

The \texttt{var\_type} parameter is properly propagated throughout the
optimization process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initialization} (\texttt{\_\_init\_\_}):

  \begin{itemize}
  \tightlist
  \item
    Stored as \texttt{self.var\_type}
  \item
    Default: \texttt{{[}"float"{]}\ *\ n\_dim} if not specified
  \end{itemize}
\item
  \textbf{Initial Design Generation}
  (\texttt{\_generate\_initial\_design}):

  \begin{itemize}
  \tightlist
  \item
    Applies type constraints via \texttt{\_repair\_non\_numeric()}
  \item
    Ensures initial points respect variable types
  \end{itemize}
\item
  \textbf{New Point Suggestion} (\texttt{\_suggest\_next\_point}):

  \begin{itemize}
  \tightlist
  \item
    Applies type constraints to acquisition function optimization
    results
  \item
    Ensures suggested points respect variable types
  \end{itemize}
\item
  \textbf{User-Provided Initial Design} (\texttt{optimize}):

  \begin{itemize}
  \tightlist
  \item
    Applies type constraints to X0 if provided
  \item
    Ensures consistency regardless of input source
  \end{itemize}
\item
  \textbf{Mesh Grid Generation} (\texttt{\_generate\_mesh\_grid}):

  \begin{itemize}
  \tightlist
  \item
    Used for plotting, respects variable types
  \item
    Ensures visualization shows correct discrete/continuous behavior
  \end{itemize}
\end{enumerate}

\subsection{\texorpdfstring{Core Method:
\texttt{\_repair\_non\_numeric()}}{Core Method: \_repair\_non\_numeric()}}\label{core-method-_repair_non_numeric}

This method enforces variable type constraints:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ \_repair\_non\_numeric(}\VariableTok{self}\NormalTok{, X: np.ndarray, var\_type: List[}\BuiltInTok{str}\NormalTok{]) }\OperatorTok{{-}\textgreater{}}\NormalTok{ np.ndarray:}
    \CommentTok{"""Round non{-}continuous values to integers."""}
\NormalTok{    mask }\OperatorTok{=}\NormalTok{ np.isin(var\_type, [}\StringTok{"float"}\NormalTok{], invert}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X[:, mask] }\OperatorTok{=}\NormalTok{ np.around(X[:, mask])}
    \ControlFlowTok{return}\NormalTok{ X}
\end{Highlighting}
\end{Shaded}

\textbf{Logic:}

\begin{itemize}
\tightlist
\item
  Variables with type
  \texttt{\textquotesingle{}float\textquotesingle{}}: No change
  (continuous)
\item
  Variables with type \texttt{\textquotesingle{}int\textquotesingle{}}
  or \texttt{\textquotesingle{}factor\textquotesingle{}}: Rounded to
  integers
\end{itemize}

\section{Usage Examples}\label{usage-examples-2}

\section{5. Example Usage}\label{example-usage-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ spotoptim }\ImportTok{import}\NormalTok{ SpotOptim}

\CommentTok{\# Example 1: All float variables (default)}
\NormalTok{opt1 }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: np.}\BuiltInTok{sum}\NormalTok{(x}\OperatorTok{**}\DecValTok{2}\NormalTok{),}
\NormalTok{    lower}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]),}
\NormalTok{    upper}\OperatorTok{=}\NormalTok{np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{])}
    \CommentTok{\# var\_type defaults to ["float", "float", "float"]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Example 2: Pure Integer
Optimization}\label{example-2-pure-integer-optimization}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ discrete\_func(X):}
    \ControlFlowTok{return}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(X)}\OperatorTok{**}\DecValTok{2}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)]}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ [}\StringTok{"int"}\NormalTok{, }\StringTok{"int"}\NormalTok{]}

\NormalTok{opt }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{discrete\_func,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{var\_type,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ opt.optimize()}
\CommentTok{\# result.x will have integer values like [1.0, {-}2.0]}
\end{Highlighting}
\end{Shaded}

\subsection{Example 3: Categorical (Factor)
Variables}\label{example-3-categorical-factor-variables}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ categorical\_func(X):}
    \CommentTok{\# Assume X[:, 0] represents 3 categories: 0, 1, 2}
    \CommentTok{\# Category 0 is best}
    \ControlFlowTok{return}\NormalTok{ (X[:, }\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\NormalTok{ (X[:, }\DecValTok{1}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{)}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{), (}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{)]  }\CommentTok{\# 3 and 4 categories respectively}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ [}\StringTok{"factor"}\NormalTok{, }\StringTok{"factor"}\NormalTok{]}

\NormalTok{opt }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{categorical\_func,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{var\_type,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ opt.optimize()}
\CommentTok{\# result.x will be integers like [0.0, 1.0] representing categories}
\end{Highlighting}
\end{Shaded}

\subsection{Example 4: Mixed Variable
Types}\label{example-4-mixed-variable-types}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mixed\_func(X):}
    \CommentTok{\# X[:, 0]: continuous temperature}
    \CommentTok{\# X[:, 1]: discrete number of iterations}
    \CommentTok{\# X[:, 2]: categorical algorithm choice (0, 1, 2)}
    \ControlFlowTok{return}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{]}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{]}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ X[:, }\DecValTok{2}\NormalTok{]}\OperatorTok{**}\DecValTok{2}

\NormalTok{bounds }\OperatorTok{=}\NormalTok{ [(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), (}\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{), (}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{)]}
\NormalTok{var\_type }\OperatorTok{=}\NormalTok{ [}\StringTok{"float"}\NormalTok{, }\StringTok{"int"}\NormalTok{, }\StringTok{"factor"}\NormalTok{]}

\NormalTok{opt }\OperatorTok{=}\NormalTok{ SpotOptim(}
\NormalTok{    fun}\OperatorTok{=}\NormalTok{mixed\_func,}
\NormalTok{    bounds}\OperatorTok{=}\NormalTok{bounds,}
\NormalTok{    var\_type}\OperatorTok{=}\NormalTok{var\_type,}
\NormalTok{    max\_iter}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{    seed}\OperatorTok{=}\DecValTok{42}
\NormalTok{)}

\NormalTok{result }\OperatorTok{=}\NormalTok{ opt.optimize()}
\CommentTok{\# result.x[0]: continuous float like 0.123}
\CommentTok{\# result.x[1]: integer like 5.0}
\CommentTok{\# result.x[2]: integer category like 0.0}
\end{Highlighting}
\end{Shaded}

\section{Key Findings}\label{key-findings}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Type Persistence}: Variable types are correctly maintained
  throughout the entire optimization process, from initial design
  through all iterations.
\item
  \textbf{Automatic Enforcement}: The \texttt{\_repair\_non\_numeric()}
  method is called at all critical points, ensuring type constraints are
  never violated.
\item
  \textbf{Three Explicit Types}: Only
  \texttt{\textquotesingle{}float\textquotesingle{}},
  \texttt{\textquotesingle{}int\textquotesingle{}}, and
  \texttt{\textquotesingle{}factor\textquotesingle{}} are supported. The
  legacy \texttt{\textquotesingle{}num\textquotesingle{}} type has been
  removed for clarity.
\item
  \textbf{User-Provided Data}: Type constraints are applied even to
  user-provided initial designs, ensuring consistency.
\item
  \textbf{Plotting Compatibility}: The plotting functionality respects
  variable types, ensuring correct visualization of discrete
  vs.~continuous variables.
\end{enumerate}

\section{Recommendations}\label{recommendations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Always specify var\_type explicitly} for clarity, especially
  in mixed-type problems
\item
  \textbf{Use appropriate bounds} for factor variables (e.g.,
  \texttt{(0,\ n\_categories-1)})
\item
  \textbf{External mapping} for string categories: Maintain your own
  mapping dictionary outside SpotOptim (e.g.,
  \texttt{\{"red":\ 0,\ "green":\ 1,\ "blue":\ 2\}})
\item
  \textbf{Validation}: The current implementation doesn't validate
  var\_type length matches bounds length - users should ensure this
  manually
\end{enumerate}

\section{Future Enhancements
(Optional)}\label{future-enhancements-optional}

Potential improvements that could be added:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Validation}: Add validation in \texttt{\_\_init\_\_} to check
  \texttt{len(var\_type)\ ==\ len(bounds)}
\item
  \textbf{String Categories}: Add built-in support for automatic
  string-to-int mapping
\item
  \textbf{Ordered Categories}: Support ordered categorical variables
  (ordinal data)
\item
  \textbf{Type Checking}: Validate that var\_type values are one of the
  allowed strings
\item
  \textbf{Bounds Checking}: Warn if factor bounds are not integer ranges
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{References}\label{references-3}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}


\backmatter


\end{document}
