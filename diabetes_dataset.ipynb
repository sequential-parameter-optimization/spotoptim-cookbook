{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Diabetes Dataset Utilities\n",
        "sidebar_position: 5\n",
        "eval: true\n",
        "---\n",
        "\n",
        "\n",
        "SpotOptim provides convenient utilities for working with the sklearn diabetes dataset, including PyTorch `Dataset` and `DataLoader` implementations. These utilities simplify data loading, preprocessing, and model training for regression tasks.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The diabetes dataset contains 10 baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements) for 442 diabetes patients. The target is a quantitative measure of disease progression one year after baseline.\n",
        "\n",
        "**Module**: `spotoptim.data.diabetes`\n",
        "\n",
        "**Key Components**:\n",
        "\n",
        "- `DiabetesDataset`: PyTorch Dataset class\n",
        "- `get_diabetes_dataloaders()`: Convenience function for complete data pipeline\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "### Basic Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from sklearn.datasets import load_diabetes\n",
        "from spotoptim.data.diabetes import DiabetesDataset\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target.reshape(-1, 1)\n",
        "\n",
        "# Now create the dataset\n",
        "dataset = DiabetesDataset(X, y, transform=None, target_transform=None)\n",
        "# Load data with default settings\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders()\n",
        "\n",
        "# Iterate through batches\n",
        "for batch_X, batch_y in train_loader:\n",
        "    print(f\"Batch features: {batch_X.shape}\")  # (32, 10)\n",
        "    print(f\"Batch targets: {batch_y.shape}\")   # (32, 1)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "# Load data\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    test_size=0.2,\n",
        "    batch_size=32,\n",
        "    scale_features=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create model\n",
        "model = LinearRegressor(\n",
        "    input_dim=10,\n",
        "    output_dim=1,\n",
        "    l1=64,\n",
        "    num_hidden_layers=2,\n",
        "    activation=\"ReLU\"\n",
        ")\n",
        "\n",
        "# Setup training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Forward pass\n",
        "        predictions = model(batch_X)\n",
        "        loss = criterion(predictions, batch_y)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        predictions = model(batch_X)\n",
        "        loss = criterion(predictions, batch_y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f\"Test MSE: {avg_test_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function Reference\n",
        "\n",
        "### get_diabetes_dataloaders()\n",
        "\n",
        "Loads the sklearn diabetes dataset and returns configured PyTorch DataLoaders.\n",
        "\n",
        "**Signature:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_diabetes_dataloaders(\n",
        "    test_size=0.2,\n",
        "    batch_size=32,\n",
        "    shuffle_train=True,\n",
        "    shuffle_test=False,\n",
        "    random_state=42,\n",
        "    scale_features=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Parameters:**\n",
        "\n",
        "| Parameter | Type | Default | Description |\n",
        "|-----------|------|---------|-------------|\n",
        "| `test_size` | float | 0.2 | Proportion of dataset for testing (0.0 to 1.0) |\n",
        "| `batch_size` | int | 32 | Number of samples per batch |\n",
        "| `shuffle_train` | bool | True | Whether to shuffle training data |\n",
        "| `shuffle_test` | bool | False | Whether to shuffle test data |\n",
        "| `random_state` | int | 42 | Random seed for train/test split |\n",
        "| `scale_features` | bool | True | Whether to standardize features |\n",
        "| `num_workers` | int | 0 | Number of subprocesses for data loading |\n",
        "| `pin_memory` | bool | False | Whether to pin memory (useful for GPU) |\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- `train_loader` (DataLoader): Training data loader\n",
        "- `test_loader` (DataLoader): Test data loader\n",
        "- `scaler` (StandardScaler or None): Fitted scaler if `scale_features=True`, else None\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# Custom configuration\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    test_size=0.3,\n",
        "    batch_size=64,\n",
        "    shuffle_train=True,\n",
        "    scale_features=True,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"Scaler mean: {scaler.mean_[:3]}\")  # First 3 features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DiabetesDataset Class\n",
        "\n",
        "PyTorch Dataset implementation for the diabetes dataset.\n",
        "\n",
        "**Signature:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DiabetesDataset(X, y, transform=None, target_transform=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Parameters:**\n",
        "\n",
        "- `X` (np.ndarray): Feature matrix of shape (n_samples, n_features)\n",
        "- `y` (np.ndarray): Target values of shape (n_samples,) or (n_samples, 1)\n",
        "- `transform` (callable, optional): Transform to apply to features\n",
        "- `target_transform` (callable, optional): Transform to apply to targets\n",
        "\n",
        "**Attributes:**\n",
        "\n",
        "- `X` (torch.Tensor): Feature tensor (n_samples, n_features)\n",
        "- `y` (torch.Tensor): Target tensor (n_samples, 1)\n",
        "- `n_features` (int): Number of features (10 for diabetes)\n",
        "- `n_samples` (int): Number of samples\n",
        "\n",
        "**Methods:**\n",
        "\n",
        "- `__len__()`: Returns number of samples\n",
        "- `__getitem__(idx)`: Returns tuple (features, target) for given index\n",
        "\n",
        "### Manual Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import DiabetesDataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load raw data\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DiabetesDataset(X_train, y_train)\n",
        "test_dataset = DiabetesDataset(X_test, y_test)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Inspect dataset\n",
        "print(f\"Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Features shape: {train_dataset.X.shape}\")\n",
        "print(f\"Targets shape: {train_dataset.y.shape}\")\n",
        "\n",
        "# Get a sample\n",
        "features, target = train_dataset[0]\n",
        "print(f\"Sample features: {features.shape}\")  # (10,)\n",
        "print(f\"Sample target: {target.shape}\")      # (1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage\n",
        "\n",
        "### Custom Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import DiabetesDataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "import torch\n",
        "\n",
        "# Define custom transforms\n",
        "def add_noise(x):\n",
        "    \"\"\"Add Gaussian noise to features.\"\"\"\n",
        "    return x + torch.randn_like(x) * 0.01\n",
        "\n",
        "def log_transform(y):\n",
        "    \"\"\"Apply log transform to target.\"\"\"\n",
        "    return torch.log1p(y)\n",
        "\n",
        "# Load data\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Create dataset with transforms\n",
        "dataset = DiabetesDataset(\n",
        "    X, y,\n",
        "    transform=add_noise,\n",
        "    target_transform=log_transform\n",
        ")\n",
        "\n",
        "# Transforms are applied when accessing items\n",
        "features, target = dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Different Train/Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# 70/30 split\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")  # ~310\n",
        "print(f\"Test samples: {len(test_loader.dataset)}\")       # ~132\n",
        "\n",
        "# 90/10 split\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")  # ~398\n",
        "print(f\"Test samples: {len(test_loader.dataset)}\")       # ~44"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Without Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# Load without scaling (useful for tree-based models)\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    scale_features=False\n",
        ")\n",
        "\n",
        "print(f\"Scaler: {scaler}\")  # None\n",
        "\n",
        "# Data is in original scale\n",
        "for batch_X, batch_y in train_loader:\n",
        "    print(f\"Mean: {batch_X.mean(dim=0)[:3]}\")  # Non-zero values\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Larger Batch Sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# Larger batches for faster training (if memory allows)\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    batch_size=128\n",
        ")\n",
        "print(f\"Batches per epoch: {len(train_loader)}\")  # Fewer batches\n",
        "\n",
        "# Smaller batches for more gradient updates\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    batch_size=8\n",
        ")\n",
        "print(f\"Batches per epoch: {len(train_loader)}\")  # More batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPU Training with Pin Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# Enable pin_memory for faster GPU transfer\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    batch_size=32,\n",
        "    pin_memory=True  # Set to True when using GPU\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Training loop with GPU\n",
        "for batch_X, batch_y in train_loader:\n",
        "    # Data is already pinned, faster transfer to GPU\n",
        "    batch_X = batch_X.to(device, non_blocking=True)\n",
        "    batch_y = batch_y.to(device, non_blocking=True)\n",
        "    \n",
        "    # ... training code ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Training Example\n",
        "\n",
        "Here's a complete example showing data loading, model training, and evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "def train_diabetes_model():\n",
        "    \"\"\"Train a neural network on the diabetes dataset.\"\"\"\n",
        "    \n",
        "    # Load data\n",
        "    train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "        test_size=0.2,\n",
        "        batch_size=32,\n",
        "        scale_features=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Create model\n",
        "    model = LinearRegressor(\n",
        "        input_dim=10,\n",
        "        output_dim=1,\n",
        "        l1=128,\n",
        "        num_hidden_layers=3,\n",
        "        activation=\"ReLU\"\n",
        "    )\n",
        "    \n",
        "    # Setup training\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = model.get_optimizer(\"Adam\", lr=0.001, weight_decay=1e-5)\n",
        "    \n",
        "    # Training configuration\n",
        "    num_epochs = 200\n",
        "    best_test_loss = float('inf')\n",
        "    \n",
        "    print(\"Starting training...\")\n",
        "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"Test samples: {len(test_loader.dataset)}\")\n",
        "    print(f\"Batches per epoch: {len(train_loader)}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        \n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Forward pass\n",
        "            predictions = model(batch_X)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        \n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                test_loss += loss.item()\n",
        "        \n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "        \n",
        "        # Track best model\n",
        "        if avg_test_loss < best_test_loss:\n",
        "            best_test_loss = avg_test_loss\n",
        "            # Could save model here: torch.save(model.state_dict(), 'best_model.pt')\n",
        "        \n",
        "        # Print progress\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{num_epochs}: \"\n",
        "                  f\"Train Loss = {avg_train_loss:.4f}, \"\n",
        "                  f\"Test Loss = {avg_test_loss:.4f}\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Training complete!\")\n",
        "    print(f\"Best test loss: {best_test_loss:.4f}\")\n",
        "    \n",
        "    return model, best_test_loss\n",
        "\n",
        "# Run training\n",
        "if __name__ == \"__main__\":\n",
        "    model, best_loss = train_diabetes_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration with SpotOptim\n",
        "\n",
        "Use the diabetes dataset for hyperparameter optimization with SpotOptim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "def evaluate_model(X):\n",
        "    \"\"\"Objective function for SpotOptim.\n",
        "    \n",
        "    Args:\n",
        "        X: Array of hyperparameters [lr, l1, num_hidden_layers]\n",
        "        \n",
        "    Returns:\n",
        "        Array of validation losses\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        lr, l1, num_hidden_layers = params\n",
        "        lr = 10 ** lr  # Log scale for learning rate\n",
        "        l1 = int(l1)\n",
        "        num_hidden_layers = int(num_hidden_layers)\n",
        "        \n",
        "        # Load data\n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
        "            test_size=0.2,\n",
        "            batch_size=32,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        # Create model\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=l1,\n",
        "            num_hidden_layers=num_hidden_layers,\n",
        "            activation=\"ReLU\"\n",
        "        )\n",
        "        \n",
        "        # Train briefly\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n",
        "        \n",
        "        num_epochs = 50\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                test_loss += loss.item()\n",
        "        \n",
        "        results.append(test_loss / len(test_loader))\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Optimize hyperparameters\n",
        "optimizer = SpotOptim(\n",
        "    fun=evaluate_model,\n",
        "    bounds=[\n",
        "        (-4, -2),   # log10(lr): 0.0001 to 0.01\n",
        "        (16, 128),  # l1: number of neurons\n",
        "        (0, 4)      # num_hidden_layers\n",
        "    ],\n",
        "    var_type=[\"num\", \"int\", \"int\"],\n",
        "    max_iter=30,\n",
        "    n_initial=10,\n",
        "    seed=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "print(f\"Best hyperparameters found:\")\n",
        "print(f\"  Learning rate: {10**result.x[0]:.6f}\")\n",
        "print(f\"  Hidden neurons (l1): {int(result.x[1])}\")\n",
        "print(f\"  Hidden layers: {int(result.x[2])}\")\n",
        "print(f\"  Best MSE: {result.fun:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### 1. Always Use Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Good: Features are standardized\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    scale_features=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural networks typically perform better with normalized inputs.\n",
        "\n",
        "### 2. Set Random Seeds for Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reproducible train/test splits\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Also set PyTorch seed\n",
        "import torch\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Don't Shuffle Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Good: Test data in consistent order\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    shuffle_train=True,   # Shuffle training data\n",
        "    shuffle_test=False    # Don't shuffle test data\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ensures consistent evaluation metrics across runs.\n",
        "\n",
        "### 4. Choose Appropriate Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Small dataset (442 samples) - moderate batch size works well\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    batch_size=32  # Good balance for this dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Too large: Fewer gradient updates per epoch  \n",
        "Too small: Noisy gradients, slower training\n",
        "\n",
        "### 5. Save the Scaler for Production"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "\n",
        "# Train with scaling\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    scale_features=True\n",
        ")\n",
        "\n",
        "# Save scaler for production use\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Later: Load and use on new data\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    loaded_scaler = pickle.load(f)\n",
        "\n",
        "# Create some example new data (same shape as diabetes features)\n",
        "new_data = np.random.randn(5, 10)  # 5 samples, 10 features\n",
        "new_data_scaled = loaded_scaler.transform(new_data)\n",
        "\n",
        "print(f\"Original data shape: {new_data.shape}\")\n",
        "print(f\"Scaled data shape: {new_data_scaled.shape}\")\n",
        "print(f\"Scaled data mean: {new_data_scaled.mean(axis=0)[:3]}\")  # Should be close to 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Issue: Out of Memory\n",
        "\n",
        "**Solution**: Reduce batch size or disable pin_memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    batch_size=16,      # Smaller batches\n",
        "    pin_memory=False    # Disable if not using GPU\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Issue: Different Data Ranges\n",
        "\n",
        "**Symptom**: Model not converging, loss is NaN\n",
        "\n",
        "**Solution**: Ensure feature scaling is enabled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    scale_features=True  # Must be True for neural networks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Issue: Non-Reproducible Results\n",
        "\n",
        "**Solution**: Set all random seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set all seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    random_state=42,\n",
        "    shuffle_train=False  # Disable shuffle for full reproducibility\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Issue: Slow Data Loading\n",
        "\n",
        "**Solution**: Use multiple workers (if not on Windows)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_loader, test_loader, scaler = get_diabetes_dataloaders(\n",
        "    num_workers=4,      # Use 4 subprocesses\n",
        "    pin_memory=True     # Enable for GPU\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: On Windows, set `num_workers=0` to avoid multiprocessing issues.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The diabetes dataset utilities in SpotOptim provide:\n",
        "\n",
        "- **Easy data loading**: One function call gets complete data pipeline\n",
        "- **PyTorch integration**: Native Dataset and DataLoader support\n",
        "- **Preprocessing included**: Automatic feature scaling and train/test splitting\n",
        "- **Flexible configuration**: Control batch size, splitting, scaling, and more\n",
        "- **Production ready**: Save scalers and ensure reproducibility\n",
        "\n",
        "For more examples, see:\n",
        "- `examples/diabetes_dataset_example.py`\n",
        "- `notebooks/demos.ipynb`\n",
        "- Test suite: `tests/test_diabetes_dataset.py`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}