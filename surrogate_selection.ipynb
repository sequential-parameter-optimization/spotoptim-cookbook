{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---\n",
        "\n",
        "# Surrogate Model Selection in SpotOptim {#sec-surrogate-selection}\n",
        "\n",
        "::: {.callout-note}\n",
        "### Note\n",
        "* This section demonstrates how to select and configure different surrogate models in SpotOptim\n",
        "* We compare various surrogate options:\n",
        "\n",
        "  - **Gaussian Process** with different kernels (Matern, RBF, Rational Quadratic)\n",
        "  - **SpotOptim Kriging** model\n",
        "  - **Random Forest** regressor\n",
        "  - **XGBoost** regressor\n",
        "  - **Support Vector Regression** (SVR)\n",
        "  - **Gradient Boosting** regressor\n",
        "* All methods are evaluated on the Aircraft Wing Weight Example (AWWE) function\n",
        "* We visualize the fitted surrogates and compare optimization performance\n",
        ":::\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Surrogate models are the heart of Bayesian optimization. SpotOptim supports any scikit-learn compatible regressor, allowing you to choose the surrogate that best fits your problem characteristics:\n",
        "\n",
        "- **Gaussian Processes**: Provide uncertainty estimates, smooth interpolation, good for continuous functions. Support Expected Improvement (EI) acquisition.\n",
        "- **Kriging**: Similar to GP but with customizable correlation functions. Supports EI acquisition.\n",
        "- **Random Forests**: Robust to noise, handle discontinuities. Don't provide uncertainty, so use `acquisition='y'` (greedy).\n",
        "- **XGBoost**: Excellent for high-dimensional problems, fast training and prediction. Use `acquisition='y'`.\n",
        "- **SVR**: Good for high-dimensional problems with smooth structure. Use `acquisition='y'`.\n",
        "- **Gradient Boosting**: Strong performance on structured problems. Use `acquisition='y'`.\n",
        "\n",
        "::: {.callout-important}\n",
        "### Acquisition Functions and Uncertainty\n",
        "Models that provide uncertainty estimates (Gaussian Process, Kriging) work with all acquisition functions: 'ei' (Expected Improvement), 'pi' (Probability of Improvement), and 'y' (greedy).\n",
        "\n",
        "Tree-based and other models (Random Forest, XGBoost, SVR, Gradient Boosting) don't provide uncertainty estimates by default, so they should use `acquisition='y'` for greedy optimization. SpotOptim automatically handles this gracefully.\n",
        ":::\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup-surrogate-selection\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.surrogate import Kriging\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn models\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import (\n",
        "    Matern, RBF, ConstantKernel, WhiteKernel, RationalQuadratic\n",
        ")\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* XGBoost (if available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The AWWE Objective Function\n",
        "\n",
        "We use the Aircraft Wing Weight Example function, which models the weight of an unpainted light aircraft wing. The function accepts inputs in the unit cube $[0,1]^9$ and returns the wing weight in pounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: awwe-function-surrogate-selection\n",
        "def wingwt(x):\n",
        "    \"\"\"\n",
        "    Aircraft Wing Weight function.\n",
        "    \n",
        "    Args:\n",
        "        x: array-like of 9 values in [0,1]\n",
        "           [Sw, Wfw, A, L, q, l, Rtc, Nz, Wdg]\n",
        "    \n",
        "    Returns:\n",
        "        Wing weight (scalar)\n",
        "    \"\"\"\n",
        "    # Ensure x is a 2D array for batch evaluation\n",
        "    x = np.atleast_2d(x)\n",
        "    \n",
        "    # Transform from unit cube to natural scales\n",
        "    Sw = x[:, 0] * (200 - 150) + 150      # Wing area (ft²)\n",
        "    Wfw = x[:, 1] * (300 - 220) + 220     # Fuel weight (lb)\n",
        "    A = x[:, 2] * (10 - 6) + 6            # Aspect ratio\n",
        "    L = (x[:, 3] * (10 - (-10)) - 10) * np.pi/180  # Sweep angle (rad)\n",
        "    q = x[:, 4] * (45 - 16) + 16          # Dynamic pressure (lb/ft²)\n",
        "    l = x[:, 5] * (1 - 0.5) + 0.5         # Taper ratio\n",
        "    Rtc = x[:, 6] * (0.18 - 0.08) + 0.08  # Root thickness/chord\n",
        "    Nz = x[:, 7] * (6 - 2.5) + 2.5        # Ultimate load factor\n",
        "    Wdg = x[:, 8] * (2500 - 1700) + 1700  # Design gross weight (lb)\n",
        "    \n",
        "    # Calculate weight on natural scale\n",
        "    W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A/np.cos(L)**2)**0.6 * q**0.006 \n",
        "    W = W * l**0.04 * (100*Rtc/np.cos(L))**(-0.3) * (Nz*Wdg)**(0.49)\n",
        "    \n",
        "    return W.ravel()\n",
        "\n",
        "# Problem setup\n",
        "bounds = [(0, 1)] * 9\n",
        "param_names = ['Sw', 'Wfw', 'A', 'L', 'q', 'l', 'Rtc', 'Nz', 'Wdg']\n",
        "max_iter = 30\n",
        "n_initial = 10\n",
        "seed = 42\n",
        "\n",
        "print(f\"Problem dimension: {len(bounds)}\")\n",
        "print(f\"Optimization budget: {max_iter} evaluations\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Default Surrogate: Gaussian Process with Matern Kernel\n",
        "\n",
        "SpotOptim's default surrogate is a Gaussian Process with a Matern kernel (ν=2.5), which provides twice-differentiable sample paths and good performance for most optimization problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-default\n",
        "print(\"=\" * 80)\n",
        "print(\"1. DEFAULT: Gaussian Process with Matern ν=2.5 Kernel\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Default GP (no surrogate specified)\n",
        "optimizer_default = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='ei',\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_default = optimizer_default.optimize()\n",
        "time_default = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_default.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_default.nfev}\")\n",
        "print(f\"  Time: {time_default:.2f}s\")\n",
        "print(f\"  Success: {result_default.success}\")\n",
        "\n",
        "# Store for comparison\n",
        "results_comparison = [{\n",
        "    'Surrogate': 'GP Matern ν=2.5 (Default)',\n",
        "    'Best Weight': result_default.fun,\n",
        "    'Evaluations': result_default.nfev,\n",
        "    'Time (s)': time_default,\n",
        "    'Success': result_default.success\n",
        "}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Default Surrogate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-default-viz\n",
        "# Plot convergence\n",
        "optimizer_default.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot most important hyperparameters\n",
        "optimizer_default.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('Default GP Matern ν=2.5: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Gaussian Process with RBF (Radial Basis Function) Kernel\n",
        "\n",
        "The RBF kernel (also called squared exponential) produces infinitely differentiable sample paths, resulting in very smooth predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-rbf\n",
        "print(\"=\" * 80)\n",
        "print(\"2. Gaussian Process with RBF Kernel\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure GP with RBF kernel\n",
        "kernel_rbf = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(\n",
        "    length_scale=1.0, \n",
        "    length_scale_bounds=(1e-2, 1e2)\n",
        ")\n",
        "\n",
        "gp_rbf = GaussianProcessRegressor(\n",
        "    kernel=kernel_rbf,\n",
        "    n_restarts_optimizer=10,\n",
        "    normalize_y=True,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "optimizer_rbf = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=gp_rbf,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='ei',\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_rbf = optimizer_rbf.optimize()\n",
        "time_rbf = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_rbf.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_rbf.nfev}\")\n",
        "print(f\"  Time: {time_rbf:.2f}s\")\n",
        "print(f\"  Success: {result_rbf.success}\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'GP RBF',\n",
        "    'Best Weight': result_rbf.fun,\n",
        "    'Evaluations': result_rbf.nfev,\n",
        "    'Time (s)': time_rbf,\n",
        "    'Success': result_rbf.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: RBF Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-rbf-viz\n",
        "optimizer_rbf.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_rbf.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('GP RBF Kernel: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gaussian Process with Matern ν=1.5 Kernel\n",
        "\n",
        "The Matern ν=1.5 kernel produces once-differentiable sample paths, allowing for more flexible (less smooth) fits than ν=2.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-matern15\n",
        "print(\"=\" * 80)\n",
        "print(\"3. Gaussian Process with Matern ν=1.5 Kernel\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure GP with Matern nu=1.5\n",
        "kernel_matern15 = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(\n",
        "    length_scale=1.0, \n",
        "    length_scale_bounds=(1e-2, 1e2),\n",
        "    nu=1.5\n",
        ")\n",
        "\n",
        "gp_matern15 = GaussianProcessRegressor(\n",
        "    kernel=kernel_matern15,\n",
        "    n_restarts_optimizer=10,\n",
        "    normalize_y=True,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "optimizer_matern15 = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=gp_matern15,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='ei',\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_matern15 = optimizer_matern15.optimize()\n",
        "time_matern15 = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_matern15.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_matern15.nfev}\")\n",
        "print(f\"  Time: {time_matern15:.2f}s\")\n",
        "print(f\"  Success: {result_matern15.success}\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'GP Matern ν=1.5',\n",
        "    'Best Weight': result_matern15.fun,\n",
        "    'Evaluations': result_matern15.nfev,\n",
        "    'Time (s)': time_matern15,\n",
        "    'Success': result_matern15.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Matern ν=1.5 Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-matern15-viz\n",
        "optimizer_matern15.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_matern15.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('GP Matern ν=1.5 Kernel: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gaussian Process with Rational Quadratic Kernel\n",
        "\n",
        "The Rational Quadratic kernel is a scale mixture of RBF kernels with different length scales, providing more flexibility than a single RBF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-rq\n",
        "print(\"=\" * 80)\n",
        "print(\"4. Gaussian Process with Rational Quadratic Kernel\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure GP with Rational Quadratic kernel\n",
        "kernel_rq = ConstantKernel(1.0, (1e-3, 1e3)) * RationalQuadratic(\n",
        "    length_scale=1.0,\n",
        "    alpha=1.0,\n",
        "    length_scale_bounds=(1e-2, 1e2),\n",
        "    alpha_bounds=(1e-2, 1e2)\n",
        ")\n",
        "\n",
        "gp_rq = GaussianProcessRegressor(\n",
        "    kernel=kernel_rq,\n",
        "    n_restarts_optimizer=10,\n",
        "    normalize_y=True,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "optimizer_rq = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=gp_rq,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='ei',\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_rq = optimizer_rq.optimize()\n",
        "time_rq = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_rq.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_rq.nfev}\")\n",
        "print(f\"  Time: {time_rq:.2f}s\")\n",
        "print(f\"  Success: {result_rq.success}\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'GP Rational Quadratic',\n",
        "    'Best Weight': result_rq.fun,\n",
        "    'Evaluations': result_rq.nfev,\n",
        "    'Time (s)': time_rq,\n",
        "    'Success': result_rq.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Rational Quadratic Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-rq-viz\n",
        "optimizer_rq.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_rq.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('GP Rational Quadratic Kernel: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. SpotOptim Kriging Model\n",
        "\n",
        "SpotOptim includes its own Kriging implementation optimized for sequential design. It uses Gaussian correlation function and optimizes hyperparameters via differential evolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-kriging\n",
        "print(\"=\" * 80)\n",
        "print(\"5. SpotOptim Kriging Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure Kriging model\n",
        "kriging_model = Kriging(\n",
        "    noise=1e-10,          # Regularization parameter\n",
        "    kernel='gauss',       # Gaussian/RBF kernel\n",
        "    n_theta=None,         # Auto: use number of dimensions\n",
        "    min_theta=-3.0,       # Min log10(theta) bound\n",
        "    max_theta=2.0,        # Max log10(theta) bound\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "optimizer_kriging = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=kriging_model,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='ei',\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_kriging = optimizer_kriging.optimize()\n",
        "time_kriging = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_kriging.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_kriging.nfev}\")\n",
        "print(f\"  Time: {time_kriging:.2f}s\")\n",
        "print(f\"  Success: {result_kriging.success}\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'SpotOptim Kriging',\n",
        "    'Best Weight': result_kriging.fun,\n",
        "    'Evaluations': result_kriging.nfev,\n",
        "    'Time (s)': time_kriging,\n",
        "    'Success': result_kriging.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Kriging Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-kriging-viz\n",
        "optimizer_kriging.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_kriging.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('SpotOptim Kriging: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Random Forest Regressor\n",
        "\n",
        "Random Forests are ensemble methods that handle noise well and can model discontinuities. They don't naturally provide uncertainty estimates, so the acquisition function uses predictions only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-rf\n",
        "print(\"=\" * 80)\n",
        "print(\"6. Random Forest Regressor\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=seed,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "optimizer_rf = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=rf_model,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='y',  # Use 'y' (greedy) since RF doesn't provide std\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_rf = optimizer_rf.optimize()\n",
        "time_rf = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_rf.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_rf.nfev}\")\n",
        "print(f\"  Time: {time_rf:.2f}s\")\n",
        "print(f\"  Success: {result_rf.success}\")\n",
        "print(f\"  Note: Using acquisition='y' (greedy) since RF doesn't provide uncertainty\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'Random Forest',\n",
        "    'Best Weight': result_rf.fun,\n",
        "    'Evaluations': result_rf.nfev,\n",
        "    'Time (s)': time_rf,\n",
        "    'Success': result_rf.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-rf-viz\n",
        "optimizer_rf.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_rf.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('Random Forest: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. XGBoost Regressor\n",
        "\n",
        "XGBoost is a gradient boosting implementation known for excellent performance on structured data and fast training/prediction times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-xgboost\n",
        "if XGBOOST_AVAILABLE:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"7. XGBoost Regressor\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Configure XGBoost\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=seed,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    optimizer_xgb = SpotOptim(\n",
        "        fun=wingwt,\n",
        "        bounds=bounds,\n",
        "        surrogate=xgb_model,\n",
        "        max_iter=max_iter,\n",
        "        n_initial=n_initial,\n",
        "        var_name=param_names,\n",
        "        acquisition='y',  # Use 'y' (greedy) since XGBoost doesn't provide std\n",
        "        seed=seed,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    result_xgb = optimizer_xgb.optimize()\n",
        "    time_xgb = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Best weight: {result_xgb.fun:.4f} lb\")\n",
        "    print(f\"  Function evaluations: {result_xgb.nfev}\")\n",
        "    print(f\"  Time: {time_xgb:.2f}s\")\n",
        "    print(f\"  Success: {result_xgb.success}\")\n",
        "    print(f\"  Note: Using acquisition='y' (greedy) since XGBoost doesn't provide uncertainty\")\n",
        "    \n",
        "    results_comparison.append({\n",
        "        'Surrogate': 'XGBoost',\n",
        "        'Best Weight': result_xgb.fun,\n",
        "        'Evaluations': result_xgb.nfev,\n",
        "        'Time (s)': time_xgb,\n",
        "        'Success': result_xgb.success\n",
        "    })\n",
        "    \n",
        "    # Visualization\n",
        "    optimizer_xgb.plot_progress(log_y=False, figsize=(10, 5))\n",
        "    plt.title('XGBoost: Convergence')\n",
        "    plt.show()\n",
        "    \n",
        "    optimizer_xgb.plot_important_hyperparameter_contour(max_imp=3)\n",
        "    plt.suptitle('XGBoost: Most Important Parameters', y=1.02)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"7. XGBoost Regressor - SKIPPED (not installed)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Install XGBoost with: pip install xgboost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Support Vector Regression (SVR)\n",
        "\n",
        "SVR with RBF kernel can model complex non-linear relationships. It's particularly good for high-dimensional problems with smooth structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-svr\n",
        "print(\"=\" * 80)\n",
        "print(\"8. Support Vector Regression (SVR)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure SVR\n",
        "svr_model = SVR(\n",
        "    kernel='rbf',\n",
        "    C=100.0,\n",
        "    epsilon=0.1,\n",
        "    gamma='scale'\n",
        ")\n",
        "\n",
        "optimizer_svr = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=svr_model,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='y',  # Use 'y' (greedy) since SVR doesn't provide std by default\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_svr = optimizer_svr.optimize()\n",
        "time_svr = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_svr.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_svr.nfev}\")\n",
        "print(f\"  Time: {time_svr:.2f}s\")\n",
        "print(f\"  Success: {result_svr.success}\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'SVR (RBF)',\n",
        "    'Best Weight': result_svr.fun,\n",
        "    'Evaluations': result_svr.nfev,\n",
        "    'Time (s)': time_svr,\n",
        "    'Success': result_svr.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-svr-viz\n",
        "optimizer_svr.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_svr.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('Support Vector Regression: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Gradient Boosting Regressor\n",
        "\n",
        "Gradient Boosting from scikit-learn is another ensemble method that builds trees sequentially, with each tree correcting errors of the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-gb\n",
        "print(\"=\" * 80)\n",
        "print(\"9. Gradient Boosting Regressor\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure Gradient Boosting\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "optimizer_gb = SpotOptim(\n",
        "    fun=wingwt,\n",
        "    bounds=bounds,\n",
        "    surrogate=gb_model,\n",
        "    max_iter=max_iter,\n",
        "    n_initial=n_initial,\n",
        "    var_name=param_names,\n",
        "    acquisition='y',  # Use 'y' (greedy) since GB doesn't provide std\n",
        "    seed=seed,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "result_gb = optimizer_gb.optimize()\n",
        "time_gb = time.time() - start_time\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Best weight: {result_gb.fun:.4f} lb\")\n",
        "print(f\"  Function evaluations: {result_gb.nfev}\")\n",
        "print(f\"  Time: {time_gb:.2f}s\")\n",
        "print(f\"  Success: {result_gb.success}\")\n",
        "\n",
        "results_comparison.append({\n",
        "    'Surrogate': 'Gradient Boosting',\n",
        "    'Best Weight': result_gb.fun,\n",
        "    'Evaluations': result_gb.nfev,\n",
        "    'Time (s)': time_gb,\n",
        "    'Success': result_gb.success\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-gb-viz\n",
        "optimizer_gb.plot_progress(log_y=False, figsize=(10, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer_gb.plot_important_hyperparameter_contour(max_imp=3)\n",
        "plt.suptitle('Gradient Boosting: Most Important Parameters', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Comparison\n",
        "\n",
        "Now let's compare all surrogate models side-by-side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-comparison\n",
        "# Create comparison DataFrame\n",
        "df_comparison = pd.DataFrame(results_comparison)\n",
        "\n",
        "# Calculate improvement from best\n",
        "best_weight = df_comparison['Best Weight'].min()\n",
        "df_comparison['Gap to Best (%)'] = (\n",
        "    (df_comparison['Best Weight'] - best_weight) / best_weight * 100\n",
        ")\n",
        "\n",
        "# Sort by best weight\n",
        "df_comparison = df_comparison.sort_values('Best Weight')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SURROGATE MODEL COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "print(df_comparison.to_string(index=False))\n",
        "print(\"=\" * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-comparison-viz\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Best weight comparison\n",
        "ax1 = axes[0, 0]\n",
        "colors = ['green' if i == 0 else 'steelblue' for i in range(len(df_comparison))]\n",
        "ax1.barh(df_comparison['Surrogate'], df_comparison['Best Weight'], color=colors)\n",
        "ax1.set_xlabel('Best Weight (lb)')\n",
        "ax1.set_title('Best Weight Found by Each Surrogate')\n",
        "ax1.axvline(x=best_weight, color='red', linestyle='--', linewidth=2, label='Best Overall')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 2: Computational time\n",
        "ax2 = axes[0, 1]\n",
        "ax2.barh(df_comparison['Surrogate'], df_comparison['Time (s)'], color='coral')\n",
        "ax2.set_xlabel('Time (seconds)')\n",
        "ax2.set_title('Computational Time')\n",
        "ax2.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 3: Gap to best\n",
        "ax3 = axes[1, 0]\n",
        "colors_gap = ['green' if gap < 0.1 else 'orange' if gap < 1.0 else 'red' \n",
        "              for gap in df_comparison['Gap to Best (%)']]\n",
        "ax3.barh(df_comparison['Surrogate'], df_comparison['Gap to Best (%)'], color=colors_gap)\n",
        "ax3.set_xlabel('Gap to Best Solution (%)')\n",
        "ax3.set_title('Solution Quality (Lower is Better)')\n",
        "ax3.axvline(x=1.0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 4: Efficiency (weight reduction per second)\n",
        "ax4 = axes[1, 1]\n",
        "baseline_weight = wingwt(np.array([[0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38]]))[0]\n",
        "df_comparison['Efficiency'] = (baseline_weight - df_comparison['Best Weight']) / df_comparison['Time (s)']\n",
        "ax4.barh(df_comparison['Surrogate'], df_comparison['Efficiency'], color='mediumseagreen')\n",
        "ax4.set_xlabel('Weight Reduction per Second (lb/s)')\n",
        "ax4.set_title('Optimization Efficiency')\n",
        "ax4.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convergence Comparison\n",
        "\n",
        "Let's compare how different surrogates converge over iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-convergence\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Collect convergence data from all optimizers\n",
        "optimizers = [\n",
        "    (optimizer_default, 'GP Matern ν=2.5 (Default)', 'blue'),\n",
        "    (optimizer_rbf, 'GP RBF', 'green'),\n",
        "    (optimizer_matern15, 'GP Matern ν=1.5', 'red'),\n",
        "    (optimizer_rq, 'GP Rational Quadratic', 'purple'),\n",
        "    (optimizer_kriging, 'SpotOptim Kriging', 'orange'),\n",
        "    (optimizer_rf, 'Random Forest', 'brown'),\n",
        "    (optimizer_svr, 'SVR', 'pink'),\n",
        "    (optimizer_gb, 'Gradient Boosting', 'gray')\n",
        "]\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    optimizers.append((optimizer_xgb, 'XGBoost', 'cyan'))\n",
        "\n",
        "for opt, label, color in optimizers:\n",
        "    y_history = opt.y_\n",
        "    best_so_far = np.minimum.accumulate(y_history)\n",
        "    ax.plot(range(len(best_so_far)), best_so_far, linewidth=2, label=label, color=color)\n",
        "\n",
        "ax.set_xlabel('Function Evaluation', fontsize=12)\n",
        "ax.set_ylabel('Best Wing Weight Found (lb)', fontsize=12)\n",
        "ax.set_title('Convergence Comparison: All Surrogates', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(0, max_iter)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-insights\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Find best surrogate\n",
        "best_surrogate = df_comparison.iloc[0]['Surrogate']\n",
        "best_value = df_comparison.iloc[0]['Best Weight']\n",
        "best_time = df_comparison.iloc[0]['Time (s)']\n",
        "\n",
        "print(f\"\\n1. BEST OVERALL PERFORMANCE:\")\n",
        "print(f\"   Surrogate: {best_surrogate}\")\n",
        "print(f\"   Best Weight: {best_value:.4f} lb\")\n",
        "print(f\"   Computation Time: {best_time:.2f}s\")\n",
        "\n",
        "# Find fastest\n",
        "fastest_idx = df_comparison['Time (s)'].idxmin()\n",
        "fastest_surrogate = df_comparison.loc[fastest_idx, 'Surrogate']\n",
        "fastest_time = df_comparison.loc[fastest_idx, 'Time (s)']\n",
        "\n",
        "print(f\"\\n2. FASTEST OPTIMIZATION:\")\n",
        "print(f\"   Surrogate: {fastest_surrogate}\")\n",
        "print(f\"   Time: {fastest_time:.2f}s\")\n",
        "print(f\"   Best Weight: {df_comparison.loc[fastest_idx, 'Best Weight']:.4f} lb\")\n",
        "\n",
        "# Find most efficient\n",
        "most_efficient_idx = df_comparison['Efficiency'].idxmax()\n",
        "most_efficient = df_comparison.loc[most_efficient_idx, 'Surrogate']\n",
        "\n",
        "print(f\"\\n3. MOST EFFICIENT (weight reduction per second):\")\n",
        "print(f\"   Surrogate: {most_efficient}\")\n",
        "print(f\"   Efficiency: {df_comparison.loc[most_efficient_idx, 'Efficiency']:.4f} lb/s\")\n",
        "\n",
        "print(f\"\\n4. RECOMMENDATIONS BY PROBLEM TYPE:\")\n",
        "print(f\"   - Smooth, continuous functions: Gaussian Process with RBF or Matern ν=2.5\")\n",
        "print(f\"   - Functions with noise: Random Forest or Gradient Boosting\")\n",
        "print(f\"   - High-dimensional problems (>20D): XGBoost or Random Forest\")\n",
        "print(f\"   - Limited budget (<50 evals): Gaussian Process with Expected Improvement\")\n",
        "print(f\"   - Fast evaluation needed: XGBoost or Random Forest\")\n",
        "print(f\"   - Need uncertainty estimates: Gaussian Process or Kriging\")\n",
        "print(f\"   - Non-smooth/discontinuous: Random Forest or Gradient Boosting\")\n",
        "\n",
        "print(f\"\\n5. KERNEL COMPARISON (Gaussian Process):\")\n",
        "gp_results = df_comparison[df_comparison['Surrogate'].str.contains('GP')]\n",
        "print(gp_results[['Surrogate', 'Best Weight', 'Time (s)']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: surrogate-selection-summary-stats\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "summary_stats = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Best Weight Found',\n",
        "        'Worst Weight Found',\n",
        "        'Average Weight',\n",
        "        'Std Dev Weight',\n",
        "        'Fastest Time',\n",
        "        'Slowest Time',\n",
        "        'Average Time',\n",
        "    ],\n",
        "    'Value': [\n",
        "        f\"{df_comparison['Best Weight'].min():.4f} lb\",\n",
        "        f\"{df_comparison['Best Weight'].max():.4f} lb\",\n",
        "        f\"{df_comparison['Best Weight'].mean():.4f} lb\",\n",
        "        f\"{df_comparison['Best Weight'].std():.4f} lb\",\n",
        "        f\"{df_comparison['Time (s)'].min():.2f} s\",\n",
        "        f\"{df_comparison['Time (s)'].max():.2f} s\",\n",
        "        f\"{df_comparison['Time (s)'].mean():.2f} s\",\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(summary_stats.to_string(index=False))\n",
        "print(\"=\" * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This comprehensive comparison demonstrates that:\n",
        "\n",
        "1. **Gaussian Processes** with appropriate kernels (Matern, RBF) provide excellent performance for smooth optimization problems and naturally support Expected Improvement acquisition\n",
        "2. **SpotOptim Kriging** offers a lightweight alternative to sklearn's GP with comparable performance\n",
        "3. **Random Forest** and **XGBoost** are robust alternatives that handle noise and discontinuities well, though they require greedy acquisition\n",
        "4. **SVR** and **Gradient Boosting** offer middle-ground solutions with good scalability\n",
        "5. The choice of surrogate should be based on:\n",
        "   - Function smoothness\n",
        "   - Computational budget\n",
        "   - Need for uncertainty quantification\n",
        "   - Problem dimensionality\n",
        "   - Noise characteristics\n",
        "\n",
        "For the AWWE problem, Gaussian Process surrogates generally performed best due to the function's smooth structure, but tree-based methods (RF, XGBoost, GB) can be preferable for more complex, noisy, or high-dimensional problems.\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "* The Jupyter-Notebook of this lecture is available on GitHub in the [Hyperparameter-Tuning-Cookbook Repository](https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook)\n",
        "\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}