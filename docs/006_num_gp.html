<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Kriging (Gaussian Process Regression) – Sequential Parameter Optimization Cookbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./006_matrices.html" rel="next">
<link href="./006_num_rbf.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-58b8d76c6f3e5a567bac4a37e40b55a6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="8&nbsp; Kriging (Gaussian Process Regression) – Sequential Parameter Optimization Cookbook">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="006_num_gp_files/figure-html/fig-pval12-output-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./001_surrogate.html">Numerical Methods</a></li><li class="breadcrumb-item"><a href="./006_num_gp.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Sequential Parameter Optimization Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotpython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Sequential-Parameter-Optimization-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002_awwe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Numerical Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simulation and Surrogate Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling Plans</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_constructing_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Constructing a Surrogate</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_num_rsm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Response Surface Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_poly.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Polynomial Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_rbf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Radial Basis Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_num_gp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_matrices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Matrices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006_infill.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Infill Criteria</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Sequential Parameter Optimization Toolbox (SPOT)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./spot_step_by_step.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">SpotOptim Step-by-Step Optimization Process</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./spotoptim_examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">SpotOptim Internal Methods Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./019_spotoptim_sk_matern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Benchmarking SpotOptim with Sklearn Kriging (Matern Kernel) on 6D Rosenbrock and 10D Michalewicz Functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kriging_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Kriging Surrogate Models in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Reproducibility in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./awwe_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Optimizing the Aircraft Wing Weight Example</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./surrogate_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Surrogate Model Selection in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pinns_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Physics-Informed Neural Networks (PINNs) Demo 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pinns_2_hyperparameter_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning for Physics-Informed Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acquisition_failure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Acquisition Failure Handling in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diabetes_dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Diabetes Dataset Utilities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./factor_variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Factor Variables for Categorical Hyperparameters</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Variable Transformations for Search Space Scaling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kriging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Kriging Surrogate Integration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learning_rate_mapping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Learning Rate Mapping for Unified Optimizer Interface</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unified_learning_rate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Unified Learning Rate Interface in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multiobjective.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Multi-Objective Optimization Support in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./plot_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Surrogate Model Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./point_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Point Selection Implementation in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./save_load.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Save and Load in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./success_rate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Success Rate Tracking in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tensorboard_clean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">TensorBoard Log Cleaning Feature in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tensorboard.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">TensorBoard Logging in SpotOptim</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./var_type.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Variable Type (var_type) Implementation in SpotOptim</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-gaussian-rbf-to-kriging-basis-functions" id="toc-from-gaussian-rbf-to-kriging-basis-functions" class="nav-link active" data-scroll-target="#from-gaussian-rbf-to-kriging-basis-functions"><span class="header-section-number">8.1</span> From Gaussian RBF to Kriging Basis Functions</a></li>
  <li><a href="#building-the-kriging-model" id="toc-building-the-kriging-model" class="nav-link" data-scroll-target="#building-the-kriging-model"><span class="header-section-number">8.2</span> Building the Kriging Model</a></li>
  <li><a href="#mle-to-estimate-theta-and-p" id="toc-mle-to-estimate-theta-and-p" class="nav-link" data-scroll-target="#mle-to-estimate-theta-and-p"><span class="header-section-number">8.3</span> MLE to estimate <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></a>
  <ul class="collapse">
  <li><a href="#the-log-likelihood" id="toc-the-log-likelihood" class="nav-link" data-scroll-target="#the-log-likelihood"><span class="header-section-number">8.3.1</span> The Log-Likelihood</a></li>
  <li><a href="#differentiation-with-respect-to-mu" id="toc-differentiation-with-respect-to-mu" class="nav-link" data-scroll-target="#differentiation-with-respect-to-mu"><span class="header-section-number">8.3.2</span> Differentiation with Respect to <span class="math inline">\(\mu\)</span></a></li>
  <li><a href="#differentiation-with-respect-to-sigma" id="toc-differentiation-with-respect-to-sigma" class="nav-link" data-scroll-target="#differentiation-with-respect-to-sigma"><span class="header-section-number">8.3.3</span> Differentiation with Respect to <span class="math inline">\(\sigma\)</span></a></li>
  <li><a href="#results-of-the-optimizations" id="toc-results-of-the-optimizations" class="nav-link" data-scroll-target="#results-of-the-optimizations"><span class="header-section-number">8.3.4</span> Results of the Optimizations</a></li>
  <li><a href="#the-concentrated-log-likelihood-function" id="toc-the-concentrated-log-likelihood-function" class="nav-link" data-scroll-target="#the-concentrated-log-likelihood-function"><span class="header-section-number">8.3.5</span> The Concentrated Log-Likelihood Function</a></li>
  <li><a href="#optimizing-the-parameters-vectheta-and-vecp" id="toc-optimizing-the-parameters-vectheta-and-vecp" class="nav-link" data-scroll-target="#optimizing-the-parameters-vectheta-and-vecp"><span class="header-section-number">8.3.6</span> Optimizing the Parameters <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span></a></li>
  <li><a href="#correlation-and-covariance-matrices-revisited" id="toc-correlation-and-covariance-matrices-revisited" class="nav-link" data-scroll-target="#correlation-and-covariance-matrices-revisited"><span class="header-section-number">8.3.7</span> Correlation and Covariance Matrices Revisited</a></li>
  </ul></li>
  <li><a href="#implementing-an-mle-of-the-model-parameters" id="toc-implementing-an-mle-of-the-model-parameters" class="nav-link" data-scroll-target="#implementing-an-mle-of-the-model-parameters"><span class="header-section-number">8.4</span> Implementing an MLE of the Model Parameters</a></li>
  <li><a href="#kriging-prediction" id="toc-kriging-prediction" class="nav-link" data-scroll-target="#kriging-prediction"><span class="header-section-number">8.5</span> Kriging Prediction</a></li>
  <li><a href="#kriging-example-sinusoid-function" id="toc-kriging-example-sinusoid-function" class="nav-link" data-scroll-target="#kriging-example-sinusoid-function"><span class="header-section-number">8.6</span> Kriging Example: Sinusoid Function</a>
  <ul class="collapse">
  <li><a href="#calculating-the-correlation-matrix-psi" id="toc-calculating-the-correlation-matrix-psi" class="nav-link" data-scroll-target="#calculating-the-correlation-matrix-psi"><span class="header-section-number">8.6.1</span> Calculating the Correlation Matrix <span class="math inline">\(\Psi\)</span></a></li>
  <li><a href="#computing-the-psi-matrix" id="toc-computing-the-psi-matrix" class="nav-link" data-scroll-target="#computing-the-psi-matrix"><span class="header-section-number">8.6.2</span> Computing the <span class="math inline">\(\Psi\)</span> Matrix</a></li>
  <li><a href="#selecting-the-new-locations" id="toc-selecting-the-new-locations" class="nav-link" data-scroll-target="#selecting-the-new-locations"><span class="header-section-number">8.6.3</span> Selecting the New Locations</a></li>
  <li><a href="#computing-the-psi-vector" id="toc-computing-the-psi-vector" class="nav-link" data-scroll-target="#computing-the-psi-vector"><span class="header-section-number">8.6.4</span> Computing the <span class="math inline">\(\psi\)</span> Vector</a></li>
  <li><a href="#predicting-at-new-locations" id="toc-predicting-at-new-locations" class="nav-link" data-scroll-target="#predicting-at-new-locations"><span class="header-section-number">8.6.5</span> Predicting at New Locations</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization"><span class="header-section-number">8.6.6</span> Visualization</a></li>
  <li><a href="#sec-kriging-example-006" id="toc-sec-kriging-example-006" class="nav-link" data-scroll-target="#sec-kriging-example-006"><span class="header-section-number">8.6.7</span> The Complete Python Code for the Example</a></li>
  </ul></li>
  <li><a href="#jupyter-notebook" id="toc-jupyter-notebook" class="nav-link" data-scroll-target="#jupyter-notebook"><span class="header-section-number">8.7</span> Jupyter Notebook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./001_surrogate.html">Numerical Methods</a></li><li class="breadcrumb-item"><a href="./006_num_gp.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Kriging (Gaussian Process Regression)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>This section is based on chapter 2.4 in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>.</li>
<li>The following Python packages are imported:</li>
</ul>
<div id="5aace5a9" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> (array, zeros, power, ones, exp, multiply,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                    eye, linspace, spacing, sqrt, arange,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                    append, ravel)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> cholesky, solve</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> squareform, pdist, cdist</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<!-- bart21mSlides2022Lec-05 -->
<section id="from-gaussian-rbf-to-kriging-basis-functions" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="from-gaussian-rbf-to-kriging-basis-functions"><span class="header-section-number">8.1</span> From Gaussian RBF to Kriging Basis Functions</h2>
<p>Kriging can be explained using the concept of radial basis functions (RBFs), which were introduced in <a href="006_num_rbf.html" class="quarto-xref"><span>Chapter 7</span></a>. An RBF is a real-valued function whose value depends only on the distance from a certain point, called the center, usually in a multidimensional space. The basis function is a function of the distance between the point <span class="math inline">\(\vec{x}\)</span> and the center <span class="math inline">\(\vec{x}^{(i)}\)</span>. Other names for basis functions are <em>kernel</em> or <em>covariance</em> functions.</p>
<div id="def-kriging-basis-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.1 (The Kriging Basis Functions)</strong></span> Kriging (also known as Gaussian Process Regression) uses <span class="math inline">\(k\)</span>-dimensional basis functions of the form <span id="eq-krigingbase"><span class="math display">\[
\psi^{(i)}(\vec{x}) =
\psi(\vec{x}^{(i)}, \vec{x}) = \exp \left( - \sum_{j=1}^k \theta_j | x_{j}^{(i)} - x_{j} | ^{p_j} \right),
\tag{8.1}\]</span></span> where <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{x}^{(i)}\)</span> denote the <span class="math inline">\(k\)</span>-dim vector <span class="math inline">\(\vec{x}= (x_1, \ldots, x_k)^T\)</span> and <span class="math inline">\(\vec{x}^{(i)}= (x_1^{(i)}, \ldots, x_k^{(i)})^T\)</span>, respectively.</p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<p>Kriging uses a specialized basis function that offers greater flexibility than standard RBFs. Examining <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>, we can observe how Kriging builds upon and extends the Gaussian basis concept. The key enhancements of Kriging over Gaussian RBF can be summarized as follows:</p>
<ul>
<li>Dimension-specific width parameters: While a Gaussian RBF uses a single width parameter <span class="math inline">\(1/\sigma^2\)</span>, Kriging employs a vector <span class="math inline">\(\vec{\theta} = (\theta_1, \theta_2, \ldots, \theta_k)^T\)</span>. This allows the model to automatically adjust its sensitivity to each input dimension, effectively performing automatic feature relevance determination.</li>
<li>Flexible smoothness control: The Gaussian RBF fixes the exponent at 2, producing uniformly smooth functions. In contrast, Kriging’s dimension-specific exponents <span class="math inline">\(\vec{p} = (p_1, p_2, \ldots, p_k)^T\)</span> (typically with <span class="math inline">\(p_j \in [1, 2]\)</span>) enable precise control over smoothness properties in each dimension.</li>
<li>Unifying framework: When all exponents are set to <span class="math inline">\(p_j = 2\)</span> and all width parameters are equal (<span class="math inline">\(\theta_j = 1/\sigma^2\)</span> for all <span class="math inline">\(j\)</span>), the Kriging basis function reduces exactly to the Gaussian RBF. This makes Gaussian RBF a special case within the more general Kriging framework.</li>
</ul>
<p>These enhancements make Kriging particularly well-suited for engineering problems where variables may operate at different scales or exhibit varying degrees of smoothness across dimensions. For now, we will only consider Kriging interpolation. We will cover Kriging regression later.</p>
</section>
<section id="building-the-kriging-model" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="building-the-kriging-model"><span class="header-section-number">8.2</span> Building the Kriging Model</h2>
<p>Consider sample data <span class="math inline">\(X\)</span> and <span class="math inline">\(\vec{y}\)</span> from <span class="math inline">\(n\)</span> locations that are available in matrix form: <span class="math inline">\(X\)</span> is a <span class="math inline">\((n \times k)\)</span> matrix, where <span class="math inline">\(k\)</span> denotes the problem dimension and <span class="math inline">\(\vec{y}\)</span> is a <span class="math inline">\((n\times 1)\)</span> vector. We want to find an expression for a predicted values at a new point <span class="math inline">\(\vec{x}\)</span>, denoted as <span class="math inline">\(\hat{y}\)</span>.</p>
<p>We start with an abstract, not really intuitive concept: The observed responses <span class="math inline">\(\vec{y}\)</span> are considered as if they are from a stochastic process, which will be denoted as <span id="eq-yvec-51"><span class="math display">\[
\begin{pmatrix}
Y(\vec{x}^{(1)})\\
\vdots\\
Y(\vec{x}^{(n)})\\
\end{pmatrix}.
\tag{8.2}\]</span></span></p>
<p>The set of random vectors from <a href="#eq-yvec-51" class="quarto-xref">Equation&nbsp;<span>8.2</span></a> (also referred to as a <em>random field</em>) has a mean of <span class="math inline">\(\vec{1} \mu\)</span>, which is a <span class="math inline">\((n\times 1)\)</span> vector. The random vectors are correlated with each other using the basis function expression from <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>: <span id="eq-corr-kriging-51"><span class="math display">\[
\text{cor} \left(Y(\vec{x}^{(i)}),Y(\vec{x}^{(l)}) \right) = \exp\left(- \sum_{j=1}^k \theta_j |x_j^{(i)} - x_j^{(l)} |^{p_j}\right).
\tag{8.3}\]</span></span> Using <a href="#eq-corr-kriging-51" class="quarto-xref">Equation&nbsp;<span>8.3</span></a>, we can compute the <span class="math inline">\((n \times n)\)</span> correlation matrix <span class="math inline">\(\Psi\)</span> of the observed sample data as shown in <a href="#eq-corr-matrix-kriging-51" class="quarto-xref">Equation&nbsp;<span>8.4</span></a>,</p>
<p><span id="eq-corr-matrix-kriging-51"><span class="math display">\[
\Psi = \begin{pmatrix}
\text{cor}\left(
Y(\vec{x}^{(1)}),
Y(\vec{x}^{(1)})
\right) &amp; \ldots &amp;
\text{cor}\left(
Y(\vec{x}^{(1)}),
Y(\vec{x}^{(n)})
\right)\\
\vdots  &amp; \vdots &amp;  \vdots\\
\text{cor}\left(
Y(\vec{x}^{(n)}),
Y(\vec{x}^{(1)})
\right)&amp;
\ldots &amp;
\text{cor}\left(
Y(\vec{x}^{(n)}),
Y(\vec{x}^{(n)})
\right)
\end{pmatrix},
\tag{8.4}\]</span></span></p>
<p>and a covariance matrix as shown in <a href="#eq-cov-matrix-kriging-52" class="quarto-xref">Equation&nbsp;<span>8.5</span></a>,</p>
<p><span id="eq-cov-matrix-kriging-52"><span class="math display">\[
\text{Cov}(Y, Y ) = \sigma^2\Psi.
\tag{8.5}\]</span></span></p>
<p>This assumed correlation between the sample data reflects our expectation that an engineering function will behave in a certain way and it will be smoothly and continuous.</p>
<div id="rem-stocastic-process" class="proof remark">
<p><span class="proof-title"><em>Remark 8.1</em> (Note on Stochastic Processes). </span>See <span class="quarto-unresolved-ref">?sec-random-samples-gp</span> for a more detailed discussion on realizations of stochastic processes.</p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<p>We now have a set of <span class="math inline">\(n\)</span> random variables (<span class="math inline">\(\mathbf{Y}\)</span>) that are correlated with each other as described in the <span class="math inline">\((n \times n)\)</span> correlation matrix <span class="math inline">\(\Psi\)</span>, see <a href="#eq-corr-matrix-kriging-51" class="quarto-xref">Equation&nbsp;<span>8.4</span></a>. The correlations depend on the absolute distances in dimension <span class="math inline">\(j\)</span> between the <span class="math inline">\(i\)</span>-th and the <span class="math inline">\(l\)</span>-th sample point <span class="math inline">\(|x_j^{(i)} - x_j^{(l)}|\)</span> and the corresponding parameters <span class="math inline">\(p_j\)</span> and <span class="math inline">\(\theta_j\)</span> for dimension <span class="math inline">\(j\)</span>. The correlation is intuitive, because when</p>
<ul>
<li>two points move close together, then <span class="math inline">\(|x_j^{(i)} - x_j| \to 0\)</span> and <span class="math inline">\(\exp \left(-|x_j^{(i)} - x_j|^{p_j} \right) \to 1\)</span> (these points show very close correlation and <span class="math inline">\(Y(x_j^{(i)}) = Y(x_j)\)</span>).</li>
<li>two points move far apart, then <span class="math inline">\(|x_j^{(i)} - x_j| \to \infty\)</span> and <span class="math inline">\(\exp \left(-|x_j^{(i)} - x_j|^{p_j} \right) \to 0\)</span> (these points show very low correlation).</li>
</ul>
<div id="exm-kriging-corr-1" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.1 (Correlations for different <span class="math inline">\(p_j\)</span>)</strong></span> Three different correlations are shown in <a href="#fig-pval12" class="quarto-xref">Figure&nbsp;<span>8.1</span></a>: <span class="math inline">\(p_j= 0.1, 1, 2\)</span>. The smoothness parameter <span class="math inline">\(p_j\)</span> affects the correlation:</p>
<ul>
<li>With <span class="math inline">\(p_j=0.1\)</span>, there is basicaly no immediate correlation between the points and there is a near discontinuity between the points <span class="math inline">\(Y(\vec{x}_j^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}_j)\)</span>.</li>
<li>With <span class="math inline">\(p_j=2\)</span>, the correlation is more smooth and we have a continuous gradient through <span class="math inline">\(x_j^{(i)} - x_j\)</span>.</li>
</ul>
<p>Reducing <span class="math inline">\(p_j\)</span> increases the rate at which the correlation initially drops with distance. This is shown in <a href="#fig-pval12" class="quarto-xref">Figure&nbsp;<span>8.1</span></a>.</p>
<div id="cell-fig-pval12" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-pval12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pval12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-pval12-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pval12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Correlations with varying <span class="math inline">\(p\)</span>. <span class="math inline">\(\theta\)</span> set to 1.
</figcaption>
</figure>
</div>
</div>
</div>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<div id="exm-kriging-corr-2" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.2 (Correlations for different <span class="math inline">\(\theta\)</span>)</strong></span> <a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>8.2</span></a> visualizes the correlation between two points <span class="math inline">\(Y(\vec{x}_j^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}_j)\)</span> for different values of <span class="math inline">\(\theta\)</span>. The parameter <span class="math inline">\(\theta\)</span> can be seen as a <em>width</em> parameter:</p>
<ul>
<li>low <span class="math inline">\(\theta_j\)</span> means that all points will have a high correlation, with <span class="math inline">\(Y(x_j)\)</span> being similar across the sample.</li>
<li>high <span class="math inline">\(\theta_j\)</span> means that there is a significant difference between the <span class="math inline">\(Y(x_j)\)</span>’s.</li>
<li><span class="math inline">\(\theta_j\)</span> is a measure of how <em>active</em> the function we are approximating is.</li>
<li>High <span class="math inline">\(\theta_j\)</span> indicate important parameters, see <a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>.</li>
</ul>
<div id="cell-fig-theta12" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="fig-theta12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-theta12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-theta12-output-1.png" width="571" height="411" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-theta12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Correlations with varying <span class="math inline">\(\theta\)</span>. <span class="math inline">\(p\)</span> set to 2.
</figcaption>
</figure>
</div>
</div>
</div>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<p>Considering the activity parameter <span class="math inline">\(\theta\)</span> is useful in high-dimensional problems where it is difficult to visualize the design landscape and the effect of the variable is unknown. By examining the elements of the vector <span class="math inline">\(\vec{\theta}\)</span>, we can identify the most important variables and focus on them. This is a crucial step in the optimization process, as it allows us to reduce the dimensionality of the problem and focus on the most important variables.</p>
<div id="exm-corr-matrix-detailed" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.3 (The Correlation Matrix (Detailed Computation))</strong></span> Let <span class="math inline">\(n=4\)</span> and <span class="math inline">\(k=3\)</span>. The sample plan is represented by the following matrix <span class="math inline">\(X\)</span>: <span class="math display">\[
X = \begin{pmatrix} x_{11} &amp; x_{12} &amp; x_{13}\\
x_{21} &amp; x_{22} &amp; x_{23}\\
x_{31} &amp; x_{32} &amp; x_{33}\\
x_{41} &amp; x_{42} &amp; x_{43}\\
\end{pmatrix}
\]</span></p>
<p>To compute the elements of the matrix <span class="math inline">\(\Psi\)</span>, the following <span class="math inline">\(k\)</span> (one for each of the <span class="math inline">\(k\)</span> dimensions) <span class="math inline">\((n,n)\)</span>-matrices have to be computed:</p>
<ul>
<li><p>For <span class="math inline">\(k=1\)</span>, i.e., the first column of <span class="math inline">\(X\)</span>: <span class="math display">\[
D_1 = \begin{pmatrix} x_{11} - x_{11} &amp; x_{11} - x_{21} &amp; x_{11} -x_{31} &amp; x_{11} - x_{41} \\  x_{21} - x_{11} &amp; x_{21} - x_{21} &amp; x_{21} -x_{31} &amp; x_{21} - x_{41} \\ x_{31} - x_{11} &amp; x_{31} - x_{21} &amp; x_{31} -x_{31} &amp; x_{31} - x_{41} \\ x_{41} - x_{11} &amp; x_{41} - x_{21} &amp; x_{41} -x_{31} &amp; x_{41} - x_{41} \\
\end{pmatrix}
\]</span></p></li>
<li><p>For <span class="math inline">\(k=2\)</span>, i.e., the second column of <span class="math inline">\(X\)</span>: <span class="math display">\[
D_2 = \begin{pmatrix} x_{12} - x_{12} &amp; x_{12} - x_{22} &amp; x_{12} -x_{32} &amp; x_{12} - x_{42} \\  x_{22} - x_{12} &amp; x_{22} - x_{22} &amp; x_{22} -x_{32} &amp; x_{22} - x_{42} \\ x_{32} - x_{12} &amp; x_{32} - x_{22} &amp; x_{32} -x_{32} &amp; x_{32} - x_{42} \\ x_{42} - x_{12} &amp; x_{42} - x_{22} &amp; x_{42} -x_{32} &amp; x_{42} - x_{42} \\
\end{pmatrix}
\]</span></p></li>
<li><p>For <span class="math inline">\(k=3\)</span>, i.e., the third column of <span class="math inline">\(X\)</span>: <span class="math display">\[
D_3 = \begin{pmatrix} x_{13} - x_{13} &amp; x_{13} - x_{23} &amp; x_{13} -x_{33} &amp; x_{13} - x_{43} \\  x_{23} - x_{13} &amp; x_{23} - x_{23} &amp; x_{23} -x_{33} &amp; x_{23} - x_{43} \\ x_{33} - x_{13} &amp; x_{33} - x_{23} &amp; x_{33} -x_{33} &amp; x_{33} - x_{43} \\ x_{43} - x_{13} &amp; x_{43} - x_{23} &amp; x_{43} -x_{33} &amp; x_{43} - x_{43} \\\end{pmatrix}
\]</span></p></li>
</ul>
<p>Since the matrices are symmetric and the main diagonals are zero, it is sufficient to compute the following matrices: <span class="math display">\[
D_1 = \begin{pmatrix} 0 &amp; x_{11} - x_{21} &amp; x_{11} -x_{31} &amp; x_{11} - x_{41} \\  0 &amp;  0 &amp; x_{21} -x_{31} &amp; x_{21} - x_{41} \\ 0 &amp; 0 &amp; 0 &amp; x_{31} - x_{41} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span> <span class="math display">\[
D_2 = \begin{pmatrix} 0 &amp; x_{12} - x_{22} &amp; x_{12} -x_{32} &amp; x_{12} - x_{42} \\  0 &amp; 0 &amp; x_{22} -x_{32} &amp; x_{22} - x_{42} \\ 0 &amp; 0 &amp; 0 &amp; x_{32} - x_{42} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \begin{pmatrix} 0 &amp; x_{13} - x_{23} &amp; x_{13} -x_{33} &amp; x_{13} - x_{43} \\  0 &amp; 0 &amp; x_{23} -x_{33} &amp; x_{23} - x_{43} \\ 0 &amp; 0 &amp; 0 &amp; x_{33} - x_{43} \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>We will consider <span class="math inline">\(p_l=2\)</span>. The differences will be squared and multiplied by <span class="math inline">\(\theta_i\)</span>, i.e.:</p>
<p><span class="math display">\[
D_1 = \theta_1 \begin{pmatrix} 0 &amp; (x_{11} - x_{21})^2 &amp; (x_{11} -x_{31})^2 &amp; (x_{11} - x_{41})^2 \\  0 &amp;  0 &amp; (x_{21} -x_{31})^2 &amp; (x_{21} - x_{41})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{31} - x_{41})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_2 = \theta_2 \begin{pmatrix} 0 &amp; (x_{12} - x_{22})^2 &amp; (x_{12} -x_{32})^2 &amp; (x_{12} - x_{42})^2 \\  0 &amp; 0 &amp; (x_{22} -x_{32})^2 &amp; (x_{22} - x_{42})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{32} - x_{42})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
D_3 = \theta_3 \begin{pmatrix} 0 &amp; (x_{13} - x_{23})^2 &amp; (x_{13} -x_{33})^2 &amp; (x_{13} - x_{43})^2 \\  0 &amp; 0 &amp; (x_{23} -x_{33})^2 &amp; (x_{23} - x_{43})^2 \\ 0 &amp; 0 &amp; 0 &amp; (x_{33} - x_{43})^2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>The sum of the three matrices <span class="math inline">\(D=D_1+ D_2 + D_3\)</span> will be calculated next:</p>
<p><span class="math display">\[
\begin{pmatrix} 0 &amp;
\theta_1  (x_{11} - x_{21})^2 + \theta_2 (x_{12} - x_{22})^2 + \theta_3  (x_{13} - x_{23})^2  &amp;
\theta_1 (x_{11} -x_{31})^2 + \theta_2  (x_{12} -x_{32})^2 + \theta_3  (x_{13} -x_{33})^2 &amp;
\theta_1  (x_{11} - x_{41})^2 + \theta_2  (x_{12} - x_{42})^2 + \theta_3 (x_{13} - x_{43})^2
\\  0 &amp;  0 &amp;
\theta_1  (x_{21} -x_{31})^2 + \theta_2 (x_{22} -x_{32})^2 + \theta_3  (x_{23} -x_{33})^2 &amp;
\theta_1  x_{21} - x_{41})^2 + \theta_2  (x_{22} - x_{42})^2 + \theta_3 (x_{23} - x_{43})^2
\\ 0 &amp; 0 &amp; 0 &amp;
\theta_1 (x_{31} - x_{41})^2 + \theta_2 (x_{32} - x_{42})^2 + \theta_3 (x_{33} - x_{43})^2
\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\end{pmatrix}
\]</span></p>
<p>Finally, <span class="math display">\[ \Psi = \exp(-D)\]</span> is computed.</p>
<p>Next, we will demonstrate how this computation can be implemented in Python. We will consider four points in three dimensions and compute the correlation matrix <span class="math inline">\(\Psi\)</span> using the basis function from <a href="#eq-krigingbase" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>. These points are placed at the origin, at the unit vectors, and at the points <span class="math inline">\((100, 100, 100)\)</span> and <span class="math inline">\((101, 100, 100)\)</span>. So, they form two clusters: one at the origin and one at <span class="math inline">\((100, 100, 100)\)</span>.</p>
<div id="f317861f" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([ [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>], [<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>], [<span class="dv">101</span>, <span class="dv">100</span>, <span class="dv">100</span>]])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[  1,   0,   0],
       [  0,   1,   0],
       [100, 100, 100],
       [101, 100, 100]])</code></pre>
</div>
</div>
<div id="ea75ab30" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((k, n, n))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, n):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                D[l, i, j] <span class="op">=</span> theta[l]<span class="op">*</span>(X[i,l] <span class="op">-</span> X[j,l])<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> <span class="bu">sum</span>(D)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> D <span class="op">+</span> D.T</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span>D)  </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="df3d07ab" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>Psi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])</code></pre>
</div>
</div>
<div id="cell-fig-corr-matrix-build_Psi" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-corr-matrix-build_psi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-corr-matrix-build_psi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-corr-matrix-build_psi-output-1.png" width="505" height="416" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-corr-matrix-build_psi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Correlation matrix <span class="math inline">\(\Psi\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<div id="exm-corr-matrix-existing" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.4 (Example: The Correlation Matrix (Using Existing Functions))</strong></span> The same result as computed in <a href="#exm-corr-matrix-detailed" class="quarto-xref">Example&nbsp;<span>8.3</span></a> can be obtained with existing python functions, e.g., from the package <code>scipy</code>.</p>
<div id="155f5347" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(<span class="op">-</span> squareform(pdist(X,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                            metric<span class="op">=</span><span class="st">'sqeuclidean'</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                            out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                            w<span class="op">=</span>theta))) <span class="op">+</span>  multiply(eye(X.shape[<span class="dv">0</span>]),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                                                   eps)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta, eps<span class="op">=</span><span class="fl">.0</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>Psi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([[1.        , 0.04978707, 0.        , 0.        ],
       [0.04978707, 1.        , 0.        , 0.        ],
       [0.        , 0.        , 1.        , 0.36787944],
       [0.        , 0.        , 0.36787944, 1.        ]])</code></pre>
</div>
</div>
<p>The condition number of the correlation matrix <span class="math inline">\(\Psi\)</span> is a measure of how well the matrix can be inverted. A high condition number indicates that the matrix is close to singular, which can lead to numerical instability in computations involving the inverse of the matrix, see <a href="006_matrices.html#sec-conditon-number" class="quarto-xref"><span>Section 9.2</span></a>.</p>
<div id="d53de086" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>np.linalg.cond(Psi)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>np.float64(2.163953413738652)</code></pre>
</div>
</div>
<p><span class="math inline">\(\Box\)</span></p>
</div>
</section>
<section id="mle-to-estimate-theta-and-p" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="mle-to-estimate-theta-and-p"><span class="header-section-number">8.3</span> MLE to estimate <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span></h2>
<section id="the-log-likelihood" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="the-log-likelihood"><span class="header-section-number">8.3.1</span> The Log-Likelihood</h3>
<p>Until now, the observed data <span class="math inline">\(\vec{y}\)</span> was not used. We know what the correlations mean, but how do we estimate the values of <span class="math inline">\(\theta_j\)</span> and where does our observed data <span class="math inline">\(y\)</span> come in? To estimate the values of <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span>, they are chosen to maximize the likelihood of <span class="math inline">\(\vec{y}\)</span>, <span id="eq-likelihood-55-a"><span class="math display">\[
L = L\left(Y(\vec{x}^{(1)}), \ldots, Y(\vec{x}^{(n)}) | \mu, \sigma \right) = \frac{1}{(2\pi \sigma^2)^{n/2}} \exp\left[ - \frac{\sum_{i=1}^n(Y(\vec{x}^{(i)})-\mu)^2}{2 \sigma^2}\right],
\tag{8.6}\]</span></span> where <span class="math inline">\(\mu\)</span> is the mean of the observed data <span class="math inline">\(\vec{y}\)</span> and <span class="math inline">\(\sigma\)</span> is the standard deviation of the errors <span class="math inline">\(\epsilon\)</span>, which can be expressed in terms of the sample data <span id="eq-likelihood-55"><span class="math display">\[
L = \frac{1}{(2\pi \sigma^2)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left[ - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}\right].
\tag{8.7}\]</span></span></p>
<div id="rem-likelihood-55" class="proof remark">
<p><span class="proof-title"><em>Remark 8.2</em>. </span>The transition from <a href="#eq-likelihood-55-a" class="quarto-xref">Equation&nbsp;<span>8.6</span></a> to <a href="#eq-likelihood-55" class="quarto-xref">Equation&nbsp;<span>8.7</span></a> reflects a shift from assuming independent errors in the observed data to explicitly modeling the <em>correlation structure</em> between the observed responses, which is a key aspect of the stochastic process framework used in methods like Kriging. It can be explained as follows:</p>
<ol type="1">
<li><p><strong>Initial Likelihood Expression (assuming independent errors):</strong> <a href="#eq-likelihood-55-a" class="quarto-xref">Equation&nbsp;<span>8.6</span></a> is an expression for the likelihood of the data set, which is based on the assumption that the errors <span class="math inline">\(\epsilon\)</span> are <em>independently randomly distributed according to a normal distribution with standard deviation <span class="math inline">\(\sigma\)</span></em>. This form is characteristic of the likelihood of <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(Y(\vec{x}^{(i)})\)</span>, each following a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p><strong>Using Vector Notation.</strong> The sum in the exponent, i.e., <span class="math display">\[
\sum_{i=1}^n(Y(\vec{x}^{(i)})-\mu)^2
\]</span> is equivalent to <span class="math display">\[
(\vec{y} - \vec{1}\mu)^T (\vec{y} - \vec{1}\mu),
\]</span> assuming <span class="math inline">\(Y(\vec{x}^{(i)}) = y^{(i)}\)</span> and using vector notation for <span class="math inline">\(\vec{y}\)</span> and <span class="math inline">\(\vec{1}\mu\)</span>.</p></li>
<li><p><strong>Assuming Independent Observations:</strong> <a href="#eq-likelihood-55-a" class="quarto-xref">Equation&nbsp;<span>8.6</span></a> assumes that the observations are independent, which means that the covariance between any two observations <span class="math inline">\(Y(\vec{x}^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}^{(l)})\)</span> is zero for <span class="math inline">\(i \neq l\)</span>. In this case, the covariance matrix of the observations would be a diagonal matrix with <span class="math inline">\(\sigma^2\)</span> along the diagonal (i.e., <span class="math inline">\(\sigma^2 I\)</span>), where <span class="math inline">\(I\)</span> is the identity matrix.</p></li>
<li><p><strong>Stochastic Process and Correlation:</strong> In the context of Kriging, the observed responses <span class="math inline">\(\vec{y}\)</span> are considered as if they are from a <em>stochastic process</em> or <em>random field</em>. This means the random variables <span class="math inline">\(Y(\vec{x}^{(i)})\)</span> at different locations <span class="math inline">\(\vec{x}^{(i)}\)</span> are not independent, but they correlated with each other. This correlation is described by an <span class="math inline">\((n \times n)\)</span> <strong>correlation matrix <span class="math inline">\(\Psi\)</span></strong>, which is used instead of <span class="math inline">\(\sigma^2 I\)</span>. The strength of the correlation between two points <span class="math inline">\(Y(\vec{x}^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}^{(l)})\)</span> depends on the distance between them and model parameters <span class="math inline">\(\theta_j\)</span> and <span class="math inline">\(p_j\)</span>.</p></li>
<li><p><strong>Multivariate Normal Distribution:</strong> When random variables are correlated, their joint probability distribution is generally described by a <em>multivariate distribution</em>. Assuming the stochastic process follows a Gaussian process, the joint distribution of the observed responses <span class="math inline">\(\vec{y}\)</span> is a <strong>multivariate normal distribution</strong>. A multivariate normal distribution for a vector <span class="math inline">\(\vec{Y}\)</span> with mean vector <span class="math inline">\(\vec{\mu}\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> has a probability density function given by: <span class="math display">\[
p(\vec{y}) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp\left[ -\frac{1}{2}(\vec{y} - \vec{\mu})^T \Sigma^{-1}(\vec{y} - \vec{\mu}) \right].
\]</span></p></li>
<li><p><strong>Connecting the Expressions:</strong> In the stochastic process framework, the following holds:</p>
<ul>
<li>The mean vector of the observed data <span class="math inline">\(\vec{y}\)</span> is <span class="math inline">\(\vec{1}\mu\)</span>.</li>
<li>The covariance matrix <span class="math inline">\(\Sigma\)</span> is constructed by considering both the variance <span class="math inline">\(\sigma^2\)</span> and the correlations <span class="math inline">\(\Psi\)</span>.</li>
<li>The covariance between <span class="math inline">\(Y(\vec{x}^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}^{(l)})\)</span> is <span class="math inline">\(\sigma^2 \text{cor}(Y(\vec{x}^{(i)}), Y(\vec{x}^{(l)}))\)</span>.</li>
<li>Therefore, the covariance matrix is <span class="math inline">\(\Sigma = \sigma^2 \vec{\Psi}\)</span>.</li>
<li>Substituting <span class="math inline">\(\vec{\mu} = \vec{1}\mu\)</span> and <span class="math inline">\(\Sigma = \sigma^2 \vec{\Psi}\)</span> into the multivariate normal PDF formula, we get: <span class="math display">\[
\Sigma^{-1} = (\sigma^2 \vec{\Psi})^{-1} = \frac{1}{\sigma^2} \vec{\Psi}^{-1}
\]</span> and <span class="math display">\[
|\Sigma| = |\sigma^2 \vec{\Psi}| = (\sigma^2)^n |\vec{\Psi}|.
\]</span> The PDF becomes: <span class="math display">\[
p(\vec{y}) = \frac{1}{\sqrt{(2\pi)^n (\sigma^2)^n |\vec{\Psi}|}} \exp\left[ -\frac{1}{2}(\vec{y} - \vec{1}\mu)^T \left(\frac{1}{\sigma^2} \vec{\Psi}^{-1}\right)(\vec{y} - \vec{1}\mu) \right]
\]</span> and simplifies to: <span class="math display">\[
p(\vec{y}) = \frac{1}{(2\pi \sigma^2)^{n/2} |\vec{\Psi}|^{1/2}} \exp\left[ -\frac{1}{2\sigma^2}(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) \right].
\]</span> This is the <strong>likelihood of the sample data <span class="math inline">\(\vec{y}\)</span></strong> given the parameters <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span>, and the correlation structure defined by the parameters within <span class="math inline">\(\vec{\Psi}\)</span> (i.e., <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span>).</li>
</ul></li>
</ol>
<p>In summary, the <a href="#eq-likelihood-55-a" class="quarto-xref">Equation&nbsp;<span>8.6</span></a> represents the likelihood under a simplified assumption of independent errors, whereas <a href="#eq-likelihood-55" class="quarto-xref">Equation&nbsp;<span>8.7</span></a> is the likelihood derived from the assumption that the observed data comes from a <strong>multivariate normal distribution</strong> where observations are correlated according to the matrix <span class="math inline">\(\vec{\Psi}\)</span>. <a href="#eq-likelihood-55" class="quarto-xref">Equation&nbsp;<span>8.7</span></a>, using the sample data vector <span class="math inline">\(\vec{y}\)</span> and the correlation matrix <span class="math inline">\(\vec{\Psi}\)</span>, properly accounts for the dependencies between data points inherent in the stochastic process model. Maximizing this likelihood is how the correlation parameters <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> are estimated in Kriging.</p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<p><a href="#eq-likelihood-55" class="quarto-xref">Equation&nbsp;<span>8.7</span></a> can be formulated as the log-likelihood: <span id="eq-loglikelihood-55"><span class="math display">\[
\ln(L) = - \frac{n}{2} \ln(2\pi \sigma) - \frac{1}{2} \ln |\vec{\Psi}| - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu) }{2 \sigma^2}.
\tag{8.8}\]</span></span></p>
</section>
<section id="differentiation-with-respect-to-mu" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="differentiation-with-respect-to-mu"><span class="header-section-number">8.3.2</span> Differentiation with Respect to <span class="math inline">\(\mu\)</span></h3>
<p>Looking at the log-likelihood function, only the last term depends on <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
\frac{1}{2 \sigma^2} (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu)
\]</span></p>
<p>To differentiate this with respect to the scalar <span class="math inline">\(\mu\)</span>, we can use matrix calculus rules.</p>
<p>Let <span class="math inline">\(\mathbf{v} = \vec{y} - \vec{1}\mu\)</span>. <span class="math inline">\(\vec{y}\)</span> is a constant vector with respect to <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\vec{1}\mu\)</span> is a vector whose derivative with respect to the scalar <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\vec{1}\)</span>. So, <span class="math inline">\(\frac{\partial \mathbf{v}}{\partial \mu} = -\vec{1}\)</span>.</p>
<p>The term is in the form <span class="math inline">\(\mathbf{v}^T \mathbf{A} \mathbf{v}\)</span>, where <span class="math inline">\(\mathbf{A} = \vec{\Psi}^{-1}\)</span> is a symmetric matrix. The derivative of <span class="math inline">\(\mathbf{v}^T \mathbf{A} \mathbf{v}\)</span> with respect to <span class="math inline">\(\mathbf{v}\)</span> is <span class="math inline">\(2 \mathbf{A} \mathbf{v}\)</span> as explained in <a href="#rem-derivative-quadratic-form" class="quarto-xref">Remark&nbsp;<span>8.3</span></a>.</p>
<div id="rem-derivative-quadratic-form" class="proof remark">
<p><span class="proof-title"><em>Remark 8.3</em> (Derivative of a Quadratic Form). </span>Consider the derivative of <span class="math inline">\(\mathbf{v}^T \mathbf{A} \mathbf{v}\)</span> with respect to <span class="math inline">\(\mathbf{v}\)</span>:</p>
<ul>
<li>The derivative of a scalar function <span class="math inline">\(f(\mathbf{v})\)</span> with respect to a vector <span class="math inline">\(\mathbf{v}\)</span> is a vector (the gradient).</li>
<li>For a quadratic form <span class="math inline">\(\mathbf{v}^T \mathbf{A} \mathbf{v}\)</span>, where <span class="math inline">\(\mathbf{A}\)</span> is a matrix and <span class="math inline">\(\mathbf{v}\)</span> is a vector, the general formula for the derivative with respect to <span class="math inline">\(\mathbf{v}\)</span> is <span class="math inline">\(\frac{\partial}{\partial \mathbf{v}} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = \mathbf{A} \mathbf{v} + \mathbf{A}^T \mathbf{v}\)</span>. (This is a standard result in matrix calculus and explained in <a href="006_matrices.html#eq-derivative-quadratic-form" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>).</li>
<li>Since <span class="math inline">\(\mathbf{A} = \vec{\Psi}^{-1}\)</span> is a symmetric matrix, its transpose <span class="math inline">\(\mathbf{A}^T\)</span> is equal to <span class="math inline">\(\mathbf{A}\)</span>.</li>
<li>Substituting <span class="math inline">\(\mathbf{A}^T = \mathbf{A}\)</span> into the general derivative formula, we get <span class="math inline">\(\mathbf{A} \mathbf{v} + \mathbf{A} \mathbf{v} = 2 \mathbf{A} \mathbf{v}\)</span>.</li>
</ul>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<p>Using the chain rule for differentiation with respect to the scalar <span class="math inline">\(\mu\)</span>: <span class="math display">\[ \frac{\partial}{\partial \mu} (\mathbf{v}^T \mathbf{A} \mathbf{v}) = 2 \left(\frac{\partial \mathbf{v}}{\partial \mu}\right)^T \mathbf{A} \mathbf{v} \]</span> Substituting <span class="math inline">\(\frac{\partial \mathbf{v}}{\partial \mu} = -\vec{1}\)</span> and <span class="math inline">\(\mathbf{v} = \vec{y} - \vec{1}\mu\)</span>: <span class="math display">\[
\frac{\partial}{\partial \mu} (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = 2 (-\vec{1})^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = -2 \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu)
\]</span></p>
<p>Now, differentiate the full log-likelihood term depending on <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \mu} \left( - \frac{1}{2 \sigma^2} (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) \right) = - \frac{1}{2 \sigma^2} \left( -2 \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) \right) = \frac{1}{\sigma^2} \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu)
\]</span></p>
<p>Setting this to zero for maximization gives:</p>
<p><span class="math display">\[
\frac{1}{\sigma^2} \vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = 0.
\]</span></p>
<p>Rearranging gives: <span class="math display">\[
\vec{1}^T \vec{\Psi}^{-1} (\vec{y} - \vec{1}\mu) = 0.
\]</span></p>
<p>Solving for <span class="math inline">\(\mu\)</span> gives: <span class="math display">\[
\vec{1}^T \vec{\Psi}^{-1} \vec{y} = \mu \vec{1}^T \vec{\Psi}^{-1} \vec{1}.
\]</span></p>
</section>
<section id="differentiation-with-respect-to-sigma" class="level3" data-number="8.3.3">
<h3 data-number="8.3.3" class="anchored" data-anchor-id="differentiation-with-respect-to-sigma"><span class="header-section-number">8.3.3</span> Differentiation with Respect to <span class="math inline">\(\sigma\)</span></h3>
<p>Let <span class="math inline">\(\nu = \sigma^2\)</span> for simpler differentiation notation. The log-likelihood becomes: <span class="math display">\[
\ln(L) = C_1 - \frac{n}{2} \ln(\nu) - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2\nu},
\]</span> where <span class="math inline">\(C_1 = - \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln |\vec{\Psi}|\)</span> is a constant with respect to <span class="math inline">\(\nu = \sigma^2\)</span>.</p>
<p>We differentiate with respect to <span class="math inline">\(\nu\)</span>: <span class="math display">\[
\frac{\partial \ln(L)}{\partial \nu} = \frac{\partial}{\partial \nu} \left( -\frac{n}{2} \ln(\nu) \right) + \frac{\partial}{\partial \nu} \left( - \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2\nu} \right).
\]</span></p>
<p>The first term’s derivative is straightforward: <span class="math display">\[
\frac{\partial}{\partial \nu} \left( -\frac{n}{2} \ln(\nu) \right) = -\frac{n}{2} \cdot \frac{1}{\nu} = -\frac{n}{2\sigma^2}.
\]</span></p>
<p>For the second term, let <span class="math inline">\(C_2 = (\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)\)</span>. This term is constant with respect to <span class="math inline">\(\sigma^2\)</span>. The derivative is:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \nu} \left( - \frac{C_2}{2\nu} \right) = - \frac{C_2}{2} \frac{\partial}{\partial \nu} (\nu^{-1}) = - \frac{C_2}{2} (-\nu^{-2}) = \frac{C_2}{2\nu^2} = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2(\sigma^2)^2}.
\]</span></p>
<p>Combining the derivatives, the gradient of the log-likelihood with respect to <span class="math inline">\(\sigma^2\)</span> is: <span class="math display">\[
\frac{\partial \ln(L)}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2(\sigma^2)^2}.
\]</span></p>
<p>Setting this to zero for maximization gives: <span class="math display">\[
-\frac{n}{2\sigma^2} + \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{2(\sigma^2)^2} = 0.
\]</span></p>
</section>
<section id="results-of-the-optimizations" class="level3" data-number="8.3.4">
<h3 data-number="8.3.4" class="anchored" data-anchor-id="results-of-the-optimizations"><span class="header-section-number">8.3.4</span> Results of the Optimizations</h3>
<p>Optimization of the log-likelihood by taking derivatives with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> results in <span id="eq-muhat-55"><span class="math display">\[
\hat{\mu} = \frac{\vec{1}^T \vec{\Psi}^{-1} \vec{y}^T}{\vec{1}^T \vec{\Psi}^{-1} \vec{1}^T}
\tag{8.9}\]</span></span> and <span id="eq-sigmahat-55"><span class="math display">\[
\hat{\sigma}^2 = \frac{(\vec{y} - \vec{1}\mu)^T \vec{\Psi}^{-1}(\vec{y} - \vec{1}\mu)}{n}.
\tag{8.10}\]</span></span></p>
</section>
<section id="the-concentrated-log-likelihood-function" class="level3" data-number="8.3.5">
<h3 data-number="8.3.5" class="anchored" data-anchor-id="the-concentrated-log-likelihood-function"><span class="header-section-number">8.3.5</span> The Concentrated Log-Likelihood Function</h3>
<p>Combining the equations, i.e., substituting <a href="#eq-muhat-55" class="quarto-xref">Equation&nbsp;<span>8.9</span></a> and <a href="#eq-sigmahat-55" class="quarto-xref">Equation&nbsp;<span>8.10</span></a> into <a href="#eq-loglikelihood-55" class="quarto-xref">Equation&nbsp;<span>8.8</span></a> leads to the concentrated log-likelihood function: <span id="eq-concentrated-loglikelihood"><span class="math display">\[
\ln(L) \approx - \frac{n}{2} \ln(\hat{\sigma}) - \frac{1}{2} \ln |\vec{\Psi}|.
\tag{8.11}\]</span></span></p>
<div id="rem-concentrated-loglikelihood" class="proof remark">
<p><span class="proof-title"><em>Remark 8.4</em> (The Concentrated Log-Likelihood). </span></p>
<ul>
<li>The first term in <a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>8.11</span></a> requires information about the measured point (observations) <span class="math inline">\(y_i\)</span>.</li>
<li>To maximize <span class="math inline">\(\ln(L)\)</span>, optimal values of <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> are determined numerically, because the function (<a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>8.11</span></a>) is not differentiable.</li>
</ul>
<p><span class="math inline">\(\Box\)</span></p>
</div>
</section>
<section id="optimizing-the-parameters-vectheta-and-vecp" class="level3" data-number="8.3.6">
<h3 data-number="8.3.6" class="anchored" data-anchor-id="optimizing-the-parameters-vectheta-and-vecp"><span class="header-section-number">8.3.6</span> Optimizing the Parameters <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span></h3>
<p>The concentrated log-likelihood function is very quick to compute. We do not need a statistical model, because we are only interested in the maximum likelihood estimate (MLE) of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span>. Optimizers such as Nelder-Mead, Conjugate Gradient, or Simulated Annealing can be used to determine optimal values for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span>. After the optimization, the correlation matrix <span class="math inline">\(\Psi\)</span> is build with the optimized <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span> values. This is best (most likely) Kriging model for the given data <span class="math inline">\(y\)</span>.</p>
<p>Observing <a href="#fig-theta12" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>, there’s significant change between <span class="math inline">\(\theta = 0.1\)</span> and <span class="math inline">\(\theta = 1\)</span>, just as there is between <span class="math inline">\(\theta = 1\)</span> and <span class="math inline">\(\theta = 10\)</span>. Hence, it is sensible to search for <span class="math inline">\(\theta\)</span> on a logarithmic scale. Suitable search bounds typically range from <span class="math inline">\(10^{-3}\)</span> to <span class="math inline">\(10^2\)</span>, although this is not a stringent requirement. Importantly, the scaling of the observed data does not affect the values of <span class="math inline">\(\hat{\theta}\)</span>, but the scaling of the design space does. Therefore, it is advisable to consistently scale variable ranges between zero and one to ensure consistency in the degree of activity <span class="math inline">\(\hat{\theta}_j\)</span> represents across different problems.</p>
</section>
<section id="correlation-and-covariance-matrices-revisited" class="level3" data-number="8.3.7">
<h3 data-number="8.3.7" class="anchored" data-anchor-id="correlation-and-covariance-matrices-revisited"><span class="header-section-number">8.3.7</span> Correlation and Covariance Matrices Revisited</h3>
<p>The covariance matrix <span class="math inline">\(\Sigma\)</span> is constructed by considering both the variance <span class="math inline">\(\sigma^2\)</span> and the correlation matrix <span class="math inline">\(\Psi\)</span>. They are related as follows:</p>
<ol type="1">
<li><strong>Covariance vs.&nbsp;Correlation:</strong> Covariance is a measure of the joint variability of two random variables, while correlation is a standardized measure of this relationship, ranging from -1 to 1. The relationship between covariance and correlation for two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by <span class="math inline">\(\text{cor}(X, Y) = \text{cov}(X, Y) / (\sigma_X \sigma_Y)\)</span>, where <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are their standard deviations.</li>
<li><strong>The Covariance Matrix <span class="math inline">\(\Sigma\)</span>:</strong> The <strong>covariance matrix <span class="math inline">\(\Sigma\)</span></strong> (or <span class="math inline">\(\text{Cov}(Y, Y)\)</span> for the vector <span class="math inline">\(\vec{Y}\)</span>) captures the <strong>pairwise covariances</strong> between all elements of the vector of observed responses.</li>
<li><strong>Connecting <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\Psi\)</span> to <span class="math inline">\(\Sigma\)</span>:</strong> In the Kriging framework described, the variance of each observation is often assumed to be constant, <span class="math inline">\(\sigma^2\)</span>. The covariance between any two observations <span class="math inline">\(Y(\vec{x}^{(i)})\)</span> and <span class="math inline">\(Y(\vec{x}^{(l)})\)</span> is given by <span class="math inline">\(\sigma^2\)</span> multiplied by their correlation. That is, <span class="math display">\[
\text{cov}(Y(\vec{x}^{(i)}), Y(\vec{x}^{(l)})) = \sigma^2 \text{cor}(Y(\vec{x}^{(i)}), Y(\vec{x}^{(l)})).
\]</span> This relationship holds for <em>all</em> pairs of points. When expressed in matrix form, the covariance matrix <span class="math inline">\(\Sigma\)</span> is the product of the variance <span class="math inline">\(\sigma^2\)</span> (a scalar) and the correlation matrix <span class="math inline">\(\Psi\)</span>: <span class="math display">\[
\Sigma = \sigma^2 \Psi.
\]</span></li>
</ol>
<p>In essence, the correlation matrix <span class="math inline">\(\Psi\)</span> defines the <em>structure</em> or <em>shape</em> of the dependencies between the data points based on their locations. The parameter <span class="math inline">\(\sigma^2\)</span> acts as a <strong>scaling factor</strong> that converts these unitless correlation values (which are between -1 and 1) into actual covariance values with units of variance, setting the overall level of variability in the system.</p>
<p>So, <span class="math inline">\(\sigma^2\)</span> tells us about the general spread or variability of the underlying process, while <span class="math inline">\(\Psi\)</span> tells you <em>how</em> that variability is distributed and how strongly points are related to each other based on their positions. Together, they completely define the covariance structure of your observed data in the multivariate normal distribution used in Kriging.</p>
</section>
</section>
<section id="implementing-an-mle-of-the-model-parameters" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="implementing-an-mle-of-the-model-parameters"><span class="header-section-number">8.4</span> Implementing an MLE of the Model Parameters</h2>
<p>The matrix algebra necessary for calculating the likelihood is the most computationally intensive aspect of the Kriging process. It is crucial to ensure that the code implementation is as efficient as possible.</p>
<p>Given that <span class="math inline">\(\Psi\)</span> (our correlation matrix) is symmetric, only half of the matrix needs to be computed before adding it to its transpose. When calculating the log-likelihood, several matrix inversions are required. The fastest approach is to conduct one Cholesky factorization and then apply backward and forward substitution for each inverse.</p>
<p>The Cholesky factorization is applicable only to positive-definite matrices, which <span class="math inline">\(\Psi\)</span> generally is. However, if <span class="math inline">\(\Psi\)</span> becomes nearly singular, such as when the <span class="math inline">\(\vec{x}^{(i)}\)</span>’s are densely packed, the Cholesky factorization might fail. In these cases, one could employ an LU-decomposition, though the result might be unreliable. When <span class="math inline">\(\Psi\)</span> is near singular, the best course of action is to either use regression techniques or, as we do here, assign a poor likelihood value to parameters generating the near singular matrix, thus diverting the MLE search towards better-conditioned <span class="math inline">\(\Psi\)</span> matrices.</p>
<p>When working with correlation matrices, increasing the values on the main diagonal of a matrix will increase the absolute value of its determinant. A critical numerical consideration in calculating the concentrated log-likelihood is that for poorly conditioned matrices, <span class="math inline">\(\det(\Psi)\)</span> approaches zero, leading to potential numerical instability. To address this issue, it is advisable to calculate <span class="math inline">\(\ln(\lvert\Psi\rvert)\)</span> in <a href="#eq-concentrated-loglikelihood" class="quarto-xref">Equation&nbsp;<span>8.11</span></a> using twice the sum of the logarithms of the diagonal elements of the Cholesky factorization. This approach provides a more numerically stable method for computing the log-determinant, as the Cholesky decomposition <span class="math inline">\(\Psi = L L^T\)</span> allows us to express <span class="math inline">\(\ln(\lvert\Psi\rvert) = 2\sum_{i=1}^{n} \ln(L_{ii})\)</span>, avoiding the direct computation of potentially very small determinant values.</p>
</section>
<section id="kriging-prediction" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="kriging-prediction"><span class="header-section-number">8.5</span> Kriging Prediction</h2>
<p>We will use the Kriging correlation <span class="math inline">\(\Psi\)</span> to predict new values based on the observed data. The presentation follows the approach described in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span> and <span class="citation" data-cites="bart21i">Bartz et al. (<a href="references.html#ref-bart21i" role="doc-biblioref">2022</a>)</span>.</p>
<p>Main idea for prediction is that the new <span class="math inline">\(Y(\vec{x})\)</span> should be consistent with the old sample data <span class="math inline">\(X\)</span>. For a new prediction <span class="math inline">\(\hat{y}\)</span> at <span class="math inline">\(\vec{x}\)</span>, the value of <span class="math inline">\(\hat{y}\)</span> is chosen so that it maximizes the likelihood of the sample data <span class="math inline">\(X\)</span> and the prediction, given the (optimized) correlation parameter <span class="math inline">\(\vec{\theta}\)</span> and <span class="math inline">\(\vec{p}\)</span> from above. The observed data <span class="math inline">\(\vec{y}\)</span> is augmented with the new prediction <span class="math inline">\(\hat{y}\)</span> which results in the augmented vector <span class="math inline">\(\vec{\tilde{y}} = ( \vec{y}^T, \hat{y})^T\)</span>. A vector of correlations between the observed data and the new prediction is defined as</p>
<p><span class="math display">\[ \vec{\psi} = \begin{pmatrix}
\text{cor}\left(
Y(\vec{x}^{(1)}),
Y(\vec{x})
\right) \\
\vdots  \\
\text{cor}\left(
Y(\vec{x}^{(n)}),
Y(\vec{x})
\right)
\end{pmatrix}
=
\begin{pmatrix}
\vec{\psi}^{(1)}\\
\vdots\\
\vec{\psi}^{(n)}
\end{pmatrix}.
\]</span></p>
<div id="def-augmented-correlation-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.2 (The Augmented Correlation Matrix)</strong></span> The augmented correlation matrix is constructed as <span class="math display">\[ \tilde{\vec{\Psi}} =
\begin{pmatrix}
\vec{\Psi} &amp; \vec{\psi} \\
\vec{\psi}^T &amp; 1
\end{pmatrix}.
\]</span></p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<p>The log-likelihood of the augmented data is <span id="eq-loglikelihood-augmented"><span class="math display">\[
\ln(L) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\hat{\sigma}^2) - \frac{1}{2} \ln |\vec{\hat{\Psi}}| -  \frac{(\vec{\tilde{y}} - \vec{1}\hat{\mu})^T \vec{\tilde{\Psi}}^{-1}(\vec{\tilde{y}} - \vec{1}\hat{\mu})}{2 \hat{\sigma}^2},
\tag{8.12}\]</span></span></p>
<p>where <span class="math inline">\(\vec{1}\)</span> is a vector of ones and <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span> are the MLEs from <a href="#eq-muhat-55" class="quarto-xref">Equation&nbsp;<span>8.9</span></a> and <a href="#eq-sigmahat-55" class="quarto-xref">Equation&nbsp;<span>8.10</span></a>. Only the last term in <a href="#eq-loglikelihood-augmented" class="quarto-xref">Equation&nbsp;<span>8.12</span></a> depends on <span class="math inline">\(\hat{y}\)</span>, so we need only consider this term in the maximization. Details can be found in <span class="citation" data-cites="Forr08a">Forrester, Sóbester, and Keane (<a href="references.html#ref-Forr08a" role="doc-biblioref">2008</a>)</span>. Finally, the MLE for <span class="math inline">\(\hat{y}\)</span> can be calculated as <span id="eq-mle-yhat"><span class="math display">\[
\hat{y}(\vec{x}) = \hat{\mu} + \vec{\psi}^T \vec{\tilde{\Psi}}^{-1} (\vec{y} - \vec{1}\hat{\mu}).
\tag{8.13}\]</span></span></p>
<p><a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>8.13</span></a> reveals two important properties of the Kriging predictor:</p>
<ul>
<li>Basis functions: The basis function impacts the vector <span class="math inline">\(\vec{\psi}\)</span>, which contains the <span class="math inline">\(n\)</span> correlations between the new point <span class="math inline">\(\vec{x}\)</span> and the observed locations. Values from the <span class="math inline">\(n\)</span> basis functions are added to a mean base term <span class="math inline">\(\mu\)</span> with weightings <span class="math display">\[
\vec{w} = \vec{\tilde{\Psi}}^{(-1)} (\vec{y} - \vec{1}\hat{\mu}).
\]</span></li>
<li>Interpolation: The predictions interpolate the sample data. When calculating the prediction at the <span class="math inline">\(i\)</span>th sample point, <span class="math inline">\(\vec{x}^{(i)}\)</span>, the <span class="math inline">\(i\)</span>th column of <span class="math inline">\(\vec{\Psi}^{-1}\)</span> is <span class="math inline">\(\vec{\psi}\)</span>, and <span class="math inline">\(\vec{\psi}  \vec{\Psi}^{-1}\)</span> is the <span class="math inline">\(i\)</span>th unit vector. Hence,</li>
</ul>
<p><span class="math display">\[
\hat{y}(\vec{x}^{(i)}) = y^{(i)}.
\]</span></p>
</section>
<section id="kriging-example-sinusoid-function" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="kriging-example-sinusoid-function"><span class="header-section-number">8.6</span> Kriging Example: Sinusoid Function</h2>
<p>Toy example in 1d where the response is a simple sinusoid measured at eight equally spaced <span class="math inline">\(x\)</span>-locations in the span of a single period of oscillation.</p>
<section id="calculating-the-correlation-matrix-psi" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="calculating-the-correlation-matrix-psi"><span class="header-section-number">8.6.1</span> Calculating the Correlation Matrix <span class="math inline">\(\Psi\)</span></h3>
<p>The correlation matrix <span class="math inline">\(\Psi\)</span> is based on the pairwise squared distances between the input locations. Here we will use <span class="math inline">\(n=8\)</span> sample locations and <span class="math inline">\(\theta\)</span> is set to 1.0.</p>
<div id="de502629" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(X, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]</code></pre>
</div>
</div>
<p>Evaluate at sample points</p>
<div id="267be7e6" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(X)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(y, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]</code></pre>
</div>
</div>
<p>We have the data points shown in <a href="#tbl-sin-data" class="quarto-xref">Table&nbsp;<span>8.1</span></a>.</p>
<div id="tbl-sin-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sin-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.1: Data points for the sinusoid function
</figcaption>
<div aria-describedby="tbl-sin-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">\(x\)</span></th>
<th style="text-align: right;"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.79</td>
<td style="text-align: right;">0.71</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1.57</td>
<td style="text-align: right;">1.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">2.36</td>
<td style="text-align: right;">0.71</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3.14</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">3.93</td>
<td style="text-align: right;">-0.71</td>
</tr>
<tr class="odd">
<td style="text-align: right;">4.71</td>
<td style="text-align: right;">-1.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">5.5</td>
<td style="text-align: right;">-0.71</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The data points are visualized in <a href="#fig-sin-data" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>.</p>
<div id="cell-fig-sin-data" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"bo"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Sin(x) evaluated at </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> points"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="fig-sin-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sin-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-sin-data-output-1.png" width="590" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sin-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Sin(x) evaluated at 8 points.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="computing-the-psi-matrix" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="computing-the-psi-matrix"><span class="header-section-number">8.6.2</span> Computing the <span class="math inline">\(\Psi\)</span> Matrix</h3>
<p>We will use the <code>build_Psi</code> function from <a href="#exm-corr-matrix-existing" class="quarto-xref">Example&nbsp;<span>8.4</span></a> to compute the correlation matrix <span class="math inline">\(\Psi\)</span>. <span class="math inline">\(\theta\)</span> should be an array of one value, because we are only working in one dimension (<span class="math inline">\(k=1\)</span>).</p>
<div id="70f951f8" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X, theta)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(Psi, <span class="dv">2</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1.   0.54 0.08 0.   0.   0.   0.   0.  ]
 [0.54 1.   0.54 0.08 0.   0.   0.   0.  ]
 [0.08 0.54 1.   0.54 0.08 0.   0.   0.  ]
 [0.   0.08 0.54 1.   0.54 0.08 0.   0.  ]
 [0.   0.   0.08 0.54 1.   0.54 0.08 0.  ]
 [0.   0.   0.   0.08 0.54 1.   0.54 0.08]
 [0.   0.   0.   0.   0.08 0.54 1.   0.54]
 [0.   0.   0.   0.   0.   0.08 0.54 1.  ]]</code></pre>
</div>
</div>
<p><a href="#fig-sin-corr" class="quarto-xref">Figure&nbsp;<span>8.5</span></a> visualizes the <span class="math inline">\((8, 8)\)</span> correlation matrix <span class="math inline">\(\Psi\)</span>.</p>
<div id="cell-fig-sin-corr" class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="fig-sin-corr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sin-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-sin-corr-output-1.png" width="482" height="416" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sin-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Correlation matrix <span class="math inline">\(\Psi\)</span> for the sinusoid function.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="selecting-the-new-locations" class="level3" data-number="8.6.3">
<h3 data-number="8.6.3" class="anchored" data-anchor-id="selecting-the-new-locations"><span class="header-section-number">8.6.3</span> Selecting the New Locations</h3>
<p>We would like to predict at <span class="math inline">\(m = 100\)</span> new locations (or testign locations) in the interval <span class="math inline">\([0, 2\pi]\)</span>. The new locations are stored in the variable <code>x</code>.</p>
<div id="4408c819" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, m, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="computing-the-psi-vector" class="level3" data-number="8.6.4">
<h3 data-number="8.6.4" class="anchored" data-anchor-id="computing-the-psi-vector"><span class="header-section-number">8.6.4</span> Computing the <span class="math inline">\(\psi\)</span> Vector</h3>
<p>Distances between testing locations <span class="math inline">\(x\)</span> and training data locations <span class="math inline">\(X\)</span>.</p>
<div id="b4c9d791" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_psi(X, x, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> zeros((n, m))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">*</span> ones(k)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> zeros((n, m))</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> cdist(x.reshape(<span class="op">-</span><span class="dv">1</span>, k),</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>              X.reshape(<span class="op">-</span><span class="dv">1</span>, k),</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>              metric<span class="op">=</span><span class="st">'sqeuclidean'</span>,</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>              out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>              w<span class="op">=</span>theta)    </span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> exp(<span class="op">-</span>D)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return psi transpose to be consistent with the literature</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Dimensions of psi: </span><span class="sc">{</span>psi<span class="sc">.</span>T<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(psi.T)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>psi <span class="op">=</span> build_psi(X, x, theta)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions of psi: (8, 100)</code></pre>
</div>
</div>
<p><a href="#fig-sin-corr-pred" class="quarto-xref">Figure&nbsp;<span>8.6</span></a> visualizes the <span class="math inline">\((8, 100)\)</span> prediction matrix <span class="math inline">\(\psi\)</span>.</p>
<div id="cell-fig-sin-corr-pred" class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div id="fig-sin-corr-pred" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sin-corr-pred-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="006_num_gp_files/figure-html/fig-sin-corr-pred-output-1.png" width="558" height="86" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sin-corr-pred-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: Visualization of the predition matrix <span class="math inline">\(\psi\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="predicting-at-new-locations" class="level3" data-number="8.6.5">
<h3 data-number="8.6.5" class="anchored" data-anchor-id="predicting-at-new-locations"><span class="header-section-number">8.6.5</span> Predicting at New Locations</h3>
<p>Computation of the predictive equations.</p>
<div id="1cbc2903" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> cholesky(Psi).T</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> np.ones(n).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> (one.T.dot(solve(U, solve(U.T, y)))) <span class="op">/</span> one.T.dot(solve(U, solve(U.T, one)))</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> mu <span class="op">*</span> ones(m).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="op">+</span> psi.T.dot(solve(U, solve(U.T, y <span class="op">-</span> one <span class="op">*</span> mu)))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dimensions of f: </span><span class="sc">{</span>f<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions of f: (100, 1)</code></pre>
</div>
</div>
<p>To compute <span class="math inline">\(f\)</span>, <a href="#eq-mle-yhat" class="quarto-xref">Equation&nbsp;<span>8.13</span></a> is used.</p>
</section>
<section id="visualization" class="level3" data-number="8.6.6">
<h3 data-number="8.6.6" class="anchored" data-anchor-id="visualization"><span class="header-section-number">8.6.6</span> Visualization</h3>
<div id="37aaa96a" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>plt.plot(x, f, color <span class="op">=</span> <span class="st">"orange"</span>, label<span class="op">=</span><span class="st">"Fitted"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x, np.sin(x), color <span class="op">=</span> <span class="st">"grey"</span>, label<span class="op">=</span><span class="st">"Original"</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"bo"</span>, label<span class="op">=</span><span class="st">"Measurements"</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Kriging prediction of sin(x) with </span><span class="sc">{}</span><span class="st"> points.</span><span class="ch">\n</span><span class="st"> theta: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(n, theta[<span class="dv">0</span>]))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-21-output-1.png" width="590" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-kriging-example-006" class="level3" data-number="8.6.7">
<h3 data-number="8.6.7" class="anchored" data-anchor-id="sec-kriging-example-006"><span class="header-section-number">8.6.7</span> The Complete Python Code for the Example</h3>
<p>Here is the self-contained Python code for direct use in a notebook:</p>
<div id="a9e22a3e" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> (array, zeros, power, ones, exp, multiply, eye, linspace, spacing, sqrt, arange, append, ravel)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> cholesky, solve</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> squareform, pdist, cdist</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Kriging Basis Functions (Defining the Correlation) ---</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The core of Kriging uses a specialized basis function for correlation:</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># psi(x^(i), x) = exp(- sum_{j=1}^k theta_j |x_j^(i) - x_j|^p_j)</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># For this 1D example (k=1), and with p_j=2 (squared Euclidean distance implicit from pdist usage)</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># and theta_j = theta (a single value), it simplifies.</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_Psi(X, theta, eps<span class="op">=</span>sqrt(spacing(<span class="dv">1</span>))):</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the correlation matrix Psi based on pairwise squared Euclidean distances</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co">    between input locations, scaled by theta.</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Adds a small epsilon to the diagonal for numerical stability (nugget effect).</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate pairwise squared Euclidean distances (D) between points in X</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> squareform(pdist(X, metric<span class="op">=</span><span class="st">'sqeuclidean'</span>, out<span class="op">=</span><span class="va">None</span>, w<span class="op">=</span>theta))</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute Psi = exp(-D)</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    Psi <span class="op">=</span> exp(<span class="op">-</span>D)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a small value to the diagonal for numerical stability (nugget)</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is often done in Kriging implementations, though a regression method</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># with a 'nugget' parameter (Lambda) is explicitly mentioned for noisy data later.</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The source code snippet for build_Psi explicitly includes `multiply(eye(X.shape), eps)`.</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># FIX: Use X.shape to get the number of rows for the identity matrix</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    Psi <span class="op">+=</span> multiply(eye(X.shape[<span class="dv">0</span>]), eps) <span class="co"># Corrected line</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Psi</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_psi(X_train, x_predict, theta):</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the correlation vector (or matrix) psi between new prediction locations</span></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a><span class="co">    and training data locations.</span></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate pairwise squared Euclidean distances (D) between prediction points (x_predict)</span></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and training points (X_train).</span></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `cdist` computes distances between each pair of the two collections of inputs.</span></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> cdist(x_predict, X_train, metric<span class="op">=</span><span class="st">'sqeuclidean'</span>, out<span class="op">=</span><span class="va">None</span>, w<span class="op">=</span>theta)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute psi = exp(-D)</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> exp(<span class="op">-</span>D)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> psi.T <span class="co"># Return transpose to be consistent with literature (n x m or n x 1)</span></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. Data Points for the Sinusoid Function Example ---</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="co"># The example uses a 1D sinusoid measured at eight equally spaced x-locations [153, Table 9.1].</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">8</span> <span class="co"># Number of sample locations</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, n, endpoint<span class="op">=</span><span class="va">False</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># Generate x-locations</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.sin(X_train) <span class="co"># Corresponding y-values (sine of x)</span></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"--- Training Data (X_train, y_train) ---"</span>)</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"x values:</span><span class="ch">\n</span><span class="st">"</span>, np.<span class="bu">round</span>(X_train, <span class="dv">2</span>))</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y values:</span><span class="ch">\n</span><span class="st">"</span>, np.<span class="bu">round</span>(y_train, <span class="dv">2</span>))</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the data points</span></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>plt.plot(X_train, y_train, <span class="st">"bo"</span>, label<span class="op">=</span><span class="ss">f"Measurements (</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> points)"</span>)</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Sin(x) evaluated at </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> points"</span>)</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"sin(x)"</span>)</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. Calculating the Correlation Matrix (Psi) ---</span></span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Psi is based on pairwise squared distances between input locations.</span></span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a><span class="co"># theta is set to 1.0 for this 1D example.</span></span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>Psi <span class="op">=</span> build_Psi(X_train, theta)</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Computed Correlation Matrix (Psi) ---"</span>)</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dimensions of Psi:"</span>, Psi.shape) <span class="co"># Should be (8, 8)</span></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 5x5 block of Psi:</span><span class="ch">\n</span><span class="st">"</span>, np.<span class="bu">round</span>(Psi[:<span class="dv">5</span>,:<span class="dv">5</span>], <span class="dv">2</span>))</span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. Selecting New Locations (for Prediction) ---</span></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a><span class="co"># We want to predict at m = 100 new locations in the interval [0, 2*pi].</span></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span> <span class="co"># Number of new locations</span></span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>x_predict <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, m, endpoint<span class="op">=</span><span class="va">True</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- New Locations for Prediction (x_predict) ---"</span>)</span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of prediction points: </span><span class="sc">{</span>m<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 5 prediction points:</span><span class="ch">\n</span><span class="st">"</span>, np.<span class="bu">round</span>(x_predict[:<span class="dv">5</span>], <span class="dv">2</span>).flatten())</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 5. Computing the psi Vector ---</span></span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a><span class="co"># This vector contains correlations between each of the n observed data points</span></span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a><span class="co"># and each of the m new prediction locations.</span></span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>psi <span class="op">=</span> build_psi(X_train, x_predict, theta)</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Computed Prediction Correlation Matrix (psi) ---"</span>)</span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dimensions of psi:"</span>, psi.shape) <span class="co"># Should be (8, 100)</span></span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 5x5 block of psi:</span><span class="ch">\n</span><span class="st">"</span>, np.<span class="bu">round</span>(psi[:<span class="dv">5</span>,:<span class="dv">5</span>], <span class="dv">2</span>))</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 6. Predicting at New Locations (Kriging Prediction) ---</span></span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a><span class="co"># The Maximum Likelihood Estimate (MLE) for y_hat is calculated using the formula:</span></span>
<span id="cb24-98"><a href="#cb24-98" aria-hidden="true" tabindex="-1"></a><span class="co"># y_hat(x) = mu_hat + psi.T @ Psi_inv @ (y - 1 * mu_hat) [p. 2 of previous response, and 263]</span></span>
<span id="cb24-99"><a href="#cb24-99" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix inversion is efficiently performed using Cholesky factorization.</span></span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6a: Cholesky decomposition of Psi</span></span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> cholesky(Psi).T <span class="co"># Note: `cholesky` in numpy returns lower triangular L, we need U (upper) so transpose L.</span></span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6b: Calculate mu_hat (estimated mean)</span></span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a><span class="co"># mu_hat = (one.T @ Psi_inv @ y) / (one.T @ Psi_inv @ one) [p. 2 of previous response]</span></span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> np.ones(n).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># Vector of ones</span></span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="op">=</span> (one.T <span class="op">@</span> solve(U, solve(U.T, y_train))) <span class="op">/</span> (one.T <span class="op">@</span> solve(U, solve(U.T, one)))</span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="op">=</span> mu_hat.item() <span class="co"># Extract scalar value</span></span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Kriging Prediction Calculation ---"</span>)</span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated mean (mu_hat): </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(mu_hat, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6c: Calculate predictions f (y_hat) at new locations</span></span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a><span class="co"># f = mu_hat * ones(m) + psi.T @ Psi_inv @ (y - one * mu_hat)</span></span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a>f_predict <span class="op">=</span> mu_hat <span class="op">*</span> np.ones(m).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> psi.T <span class="op">@</span> solve(U, solve(U.T, y_train <span class="op">-</span> one <span class="op">*</span> mu_hat))</span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dimensions of predicted values (f_predict): </span><span class="sc">{</span>f_predict<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Should be (100, 1)</span></span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First 5 predicted f values:</span><span class="ch">\n</span><span class="st">"</span>, np.<span class="bu">round</span>(f_predict[:<span class="dv">5</span>], <span class="dv">2</span>).flatten())</span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 7. Visualization ---</span></span>
<span id="cb24-122"><a href="#cb24-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the original sinusoid function, the measured points, and the Kriging predictions.</span></span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a>plt.plot(x_predict, f_predict, color<span class="op">=</span><span class="st">"orange"</span>, label<span class="op">=</span><span class="st">"Kriging Prediction"</span>)</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a>plt.plot(x_predict, np.sin(x_predict), color<span class="op">=</span><span class="st">"grey"</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"True Sinusoid Function"</span>)</span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a>plt.plot(X_train, y_train, <span class="st">"bo"</span>, markersize<span class="op">=</span><span class="dv">8</span>, label<span class="op">=</span><span class="st">"Measurements"</span>)</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Kriging prediction of sin(x) with </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> points. (theta: </span><span class="sc">{</span>theta<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>--- Training Data (X_train, y_train) ---
x values:
 [[0.  ]
 [0.79]
 [1.57]
 [2.36]
 [3.14]
 [3.93]
 [4.71]
 [5.5 ]]
y values:
 [[ 0.  ]
 [ 0.71]
 [ 1.  ]
 [ 0.71]
 [ 0.  ]
 [-0.71]
 [-1.  ]
 [-0.71]]
----------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-22-output-2.png" width="683" height="449" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
--- Computed Correlation Matrix (Psi) ---
Dimensions of Psi: (8, 8)
First 5x5 block of Psi:
 [[1.   0.54 0.08 0.   0.  ]
 [0.54 1.   0.54 0.08 0.  ]
 [0.08 0.54 1.   0.54 0.08]
 [0.   0.08 0.54 1.   0.54]
 [0.   0.   0.08 0.54 1.  ]]
----------------------------------------

--- New Locations for Prediction (x_predict) ---
Number of prediction points: 100
First 5 prediction points:
 [0.   0.06 0.13 0.19 0.25]
----------------------------------------

--- Computed Prediction Correlation Matrix (psi) ---
Dimensions of psi: (8, 100)
First 5x5 block of psi:
 [[1.   1.   0.98 0.96 0.94]
 [0.54 0.59 0.65 0.7  0.75]
 [0.08 0.1  0.12 0.15 0.18]
 [0.   0.01 0.01 0.01 0.01]
 [0.   0.   0.   0.   0.  ]]
----------------------------------------

--- Kriging Prediction Calculation ---
Estimated mean (mu_hat): -0.0499
Dimensions of predicted values (f_predict): (100, 1)
First 5 predicted f values:
 [0.   0.05 0.1  0.15 0.21]
----------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="006_num_gp_files/figure-html/cell-22-output-4.png" width="832" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="jupyter-notebook" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="jupyter-notebook"><span class="header-section-number">8.7</span> Jupyter Notebook</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The Jupyter-Notebook of this lecture is available on GitHub in the <a href="https://github.com/sequential-parameter-optimization/Hyperparameter-Tuning-Cookbook/blob/main/006_num_gp.ipynb">Hyperparameter-Tuning-Cookbook Repository</a></li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bart21i" class="csl-entry" role="listitem">
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. <em><span class="nocase">Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide</span></em>. Springer.
</div>
<div id="ref-Forr08a" class="csl-entry" role="listitem">
Forrester, Alexander, András Sóbester, and Andy Keane. 2008. <em><span class="nocase">Engineering Design via Surrogate Modelling</span></em>. Wiley.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./006_num_rbf.html" class="pagination-link" aria-label="Radial Basis Function Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Radial Basis Function Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./006_matrices.html" class="pagination-link" aria-label="Matrices">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Matrices</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, T. Bartz-Beielstein</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/bartzbeielstein">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>