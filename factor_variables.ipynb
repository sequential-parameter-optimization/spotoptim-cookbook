{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Factor Variables for Categorical Hyperparameters\n",
        "sidebar_position: 5\n",
        "eval: true\n",
        "---\n",
        "\n",
        "SpotOptim supports factor variables for optimizing categorical hyperparameters, such as activation functions, optimizers, or any discrete string-based choices. Factor variables are automatically converted between string values (external interface) and integers (internal optimization), making categorical optimization seamless.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**What are Factor Variables?**\n",
        "\n",
        "Factor variables allow you to specify categorical choices as tuples of strings in the bounds. SpotOptim handles the conversion:\n",
        "\n",
        "1. **String tuples in bounds** → Internal integer mapping (0, 1, 2, ...)\n",
        "2. **Optimization uses integers** internally for surrogate modeling\n",
        "3. **Objective function receives strings** after automatic conversion\n",
        "4. **Results return strings** (not integers)\n",
        "\n",
        "**Module**: `spotoptim.SpotOptim`\n",
        "\n",
        "**Key Features**:\n",
        "\n",
        "- Define categorical choices as string tuples: `(\"ReLU\", \"Sigmoid\", \"Tanh\")`\n",
        "- Automatic integer↔string conversion\n",
        "- Seamless integration with neural network hyperparameters\n",
        "- Mix factor variables with numeric/integer variables\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "### Basic Factor Variable Usage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def objective_function(X):\n",
        "    \"\"\"Objective function receives string values.\"\"\"\n",
        "    results = []\n",
        "    for params in X:\n",
        "        activation = params[0]  # This is a string!\n",
        "        print(f\"Testing activation: {activation}\")\n",
        "        \n",
        "        # Simple scoring based on activation choice (for demonstration)\n",
        "        # In real use, you would train a model and return actual performance\n",
        "        scores = {\n",
        "            \"ReLU\": 3500.0,\n",
        "            \"Sigmoid\": 4200.0,\n",
        "            \"Tanh\": 3800.0,\n",
        "            \"LeakyReLU\": 3600.0\n",
        "        }\n",
        "        score = scores.get(activation, 5000.0) + np.random.normal(0, 100)\n",
        "        results.append(score)\n",
        "    return np.array(results)  # Return numpy array\n",
        "\n",
        "# Define bounds with factor variable\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective_function,\n",
        "    bounds=[(\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")],\n",
        "    var_type=[\"factor\"],\n",
        "    max_iter=20,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "print(f\"\\nBest activation: {result.x[0]}\")  # Returns string, e.g., \"ReLU\"\n",
        "print(f\"Best score: {result.fun:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Activation Function Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate(X):\n",
        "    \"\"\"Train models with different activation functions.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        activation = params[0]  # String: \"ReLU\", \"Sigmoid\", etc.\n",
        "        \n",
        "        # Load data\n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders()\n",
        "        \n",
        "        # Create model with the activation function\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=64,\n",
        "            num_hidden_layers=2,\n",
        "            activation=activation  # Pass string directly!\n",
        "        )\n",
        "        \n",
        "        # Train model\n",
        "        optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        for epoch in range(50):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        avg_loss = test_loss / len(test_loader)\n",
        "        results.append(avg_loss)\n",
        "    \n",
        "    return np.array(results)  # Return numpy array\n",
        "\n",
        "# Optimize activation function choice\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_and_evaluate,\n",
        "    bounds=[(\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\", \"ELU\")],\n",
        "    var_type=[\"factor\"],\n",
        "    max_iter=30\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "print(f\"Best activation function: {result.x[0]}\")\n",
        "print(f\"Best test MSE: {result.fun:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mixed Variable Types\n",
        "\n",
        "### Combining Factor, Integer, and Continuous Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "def comprehensive_optimization(X):\n",
        "    \"\"\"Optimize learning rate, layer size, depth, and activation.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        log_lr = params[0]      # Continuous (log scale)\n",
        "        l1 = int(params[1])     # Integer\n",
        "        n_layers = int(params[2])  # Integer\n",
        "        activation = params[3]   # Factor (string)\n",
        "        \n",
        "        lr = 10 ** log_lr  # Convert from log scale\n",
        "        \n",
        "        print(f\"lr={lr:.6f}, l1={l1}, layers={n_layers}, activation={activation}\")\n",
        "        \n",
        "        # Load data\n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
        "            batch_size=32,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        # Create model\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=l1,\n",
        "            num_hidden_layers=n_layers,\n",
        "            activation=activation\n",
        "        )\n",
        "        \n",
        "        # Train\n",
        "        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        for epoch in range(30):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        results.append(test_loss / len(test_loader))\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Optimize all four hyperparameters simultaneously\n",
        "optimizer = SpotOptim(\n",
        "    fun=comprehensive_optimization,\n",
        "    bounds=[\n",
        "        (-4, -2),                                    # log10(learning_rate)\n",
        "        (16, 128),                                   # l1 (neurons per layer)\n",
        "        (0, 4),                                      # num_hidden_layers\n",
        "        (\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")   # activation function\n",
        "    ],\n",
        "    var_type=[\"num\", \"int\", \"int\", \"factor\"],\n",
        "    max_iter=50\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "\n",
        "# Results contain original string values\n",
        "print(\"\\nOptimization Results:\")\n",
        "print(f\"Best learning rate: {10**result.x[0]:.6f}\")\n",
        "print(f\"Best layer size: {int(result.x[1])}\")\n",
        "print(f\"Best num layers: {int(result.x[2])}\")\n",
        "print(f\"Best activation: {result.x[3]}\")  # String value!\n",
        "print(f\"Best test MSE: {result.fun:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Factor Variables\n",
        "\n",
        "### Optimizing Both Activation and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def optimize_activation_and_optimizer(X):\n",
        "    \"\"\"Optimize both activation function and optimizer choice.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        activation = params[0]      # Factor variable 1\n",
        "        optimizer_name = params[1]  # Factor variable 2\n",
        "        lr = 10 ** params[2]        # Continuous variable\n",
        "        \n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders()\n",
        "        \n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=64,\n",
        "            num_hidden_layers=2,\n",
        "            activation=activation\n",
        "        )\n",
        "        \n",
        "        # Use the optimizer string\n",
        "        optimizer = model.get_optimizer(optimizer_name, lr=lr)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # Train\n",
        "        for epoch in range(30):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        results.append(test_loss / len(test_loader))\n",
        "    \n",
        "    return np.array(results)  # Return numpy array\n",
        "\n",
        "# Two factor variables + one continuous\n",
        "opt = SpotOptim(\n",
        "    fun=optimize_activation_and_optimizer,\n",
        "    bounds=[\n",
        "        (\"ReLU\", \"Tanh\", \"Sigmoid\", \"LeakyReLU\"),    # Activation\n",
        "        (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),         # Optimizer\n",
        "        (-4, -2)                                      # log10(lr)\n",
        "    ],\n",
        "    var_type=[\"factor\", \"factor\", \"num\"],\n",
        "    max_iter=40\n",
        ")\n",
        "\n",
        "result = opt.optimize()\n",
        "print(f\"Best activation: {result.x[0]}\")\n",
        "print(f\"Best optimizer: {result.x[1]}\")\n",
        "print(f\"Best learning rate: {10**result.x[2]:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage\n",
        "\n",
        "### Custom Categorical Choices\n",
        "\n",
        "Factor variables work with any string values, not just activation functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def train_model_with_config(dropout_policy, batch_norm, weight_init):\n",
        "    \"\"\"Simulate model training with different configurations.\"\"\"\n",
        "    # In real use, this would train an actual model\n",
        "    # Here we return synthetic scores for demonstration\n",
        "    base_score = 3000.0\n",
        "    \n",
        "    # Dropout impact\n",
        "    dropout_scores = {\"none\": 200, \"light\": 0, \"heavy\": 100}\n",
        "    # Batch norm impact\n",
        "    bn_scores = {\"before\": -50, \"after\": 0, \"none\": 150}\n",
        "    # Weight init impact\n",
        "    init_scores = {\"xavier\": 0, \"kaiming\": -30, \"normal\": 100}\n",
        "    \n",
        "    score = (base_score + \n",
        "             dropout_scores.get(dropout_policy, 0) + \n",
        "             bn_scores.get(batch_norm, 0) + \n",
        "             init_scores.get(weight_init, 0) +\n",
        "             np.random.normal(0, 50))\n",
        "    \n",
        "    return score\n",
        "\n",
        "def train_with_config(X):\n",
        "    \"\"\"Objective function with various categorical choices.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        dropout_policy = params[0]  # \"none\", \"light\", \"heavy\"\n",
        "        batch_norm = params[1]       # \"before\", \"after\", \"none\"\n",
        "        weight_init = params[2]      # \"xavier\", \"kaiming\", \"normal\"\n",
        "        \n",
        "        # Use these strings to configure your model\n",
        "        score = train_model_with_config(\n",
        "            dropout_policy=dropout_policy,\n",
        "            batch_norm=batch_norm,\n",
        "            weight_init=weight_init\n",
        "        )\n",
        "        results.append(score)\n",
        "    \n",
        "    return np.array(results)  # Return numpy array\n",
        "\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_with_config,\n",
        "    bounds=[\n",
        "        (\"none\", \"light\", \"heavy\"),           # Dropout policy\n",
        "        (\"before\", \"after\", \"none\"),          # Batch norm position\n",
        "        (\"xavier\", \"kaiming\", \"normal\")       # Weight initialization\n",
        "    ],\n",
        "    var_type=[\"factor\", \"factor\", \"factor\"],\n",
        "    max_iter=25,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "print(\"Best configuration:\")\n",
        "print(f\"  Dropout: {result.x[0]}\")\n",
        "print(f\"  Batch norm: {result.x[1]}\")\n",
        "print(f\"  Weight init: {result.x[2]}\")\n",
        "print(f\"  Score: {result.fun:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viewing All Evaluated Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate(X):\n",
        "    \"\"\"Train models with different activation functions.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        l1 = int(params[0])         # Integer: layer size\n",
        "        activation = params[1]       # String: activation function\n",
        "        \n",
        "        # Load data\n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders()\n",
        "        \n",
        "        # Create model with the activation function\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=l1,\n",
        "            num_hidden_layers=2,\n",
        "            activation=activation  # Pass string directly!\n",
        "        )\n",
        "        \n",
        "        # Train model\n",
        "        optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        for epoch in range(50):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        avg_loss = test_loss / len(test_loader)\n",
        "        results.append(avg_loss)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_and_evaluate,\n",
        "    bounds=[\n",
        "        (16, 128),                                   # Layer size\n",
        "        (\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")   # Activation\n",
        "    ],\n",
        "    var_type=[\"int\", \"factor\"],  # IMPORTANT: Specify variable types!\n",
        "    max_iter=30,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "\n",
        "# Access all evaluated configurations\n",
        "print(\"\\nAll evaluated configurations:\")\n",
        "print(\"Layer Size | Activation | Test MSE\")\n",
        "print(\"-\" * 42)\n",
        "for i in range(min(10, len(result.X))):  # Show first 10\n",
        "    l1 = int(result.X[i, 0])\n",
        "    activation = result.X[i, 1]  # String value!\n",
        "    loss = result.y[i]\n",
        "    print(f\"{l1:10d} | {activation:10s} | {loss:.4f}\")\n",
        "\n",
        "# Find top 5 configurations\n",
        "sorted_indices = result.y.argsort()[:5]\n",
        "print(\"\\nTop 5 configurations:\")\n",
        "for idx in sorted_indices:\n",
        "    print(f\"l1={int(result.X[idx, 0]):3d}, \"\n",
        "          f\"activation={result.X[idx, 1]:10s}, \"\n",
        "          f\"MSE={result.y[idx]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How It Works\n",
        "\n",
        "### Internal Mechanism\n",
        "\n",
        "SpotOptim handles factor variables through automatic conversion:\n",
        "\n",
        "1. **Initialization**: String tuples in bounds are detected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "bounds = [(\"ReLU\", \"Sigmoid\", \"Tanh\")]\n",
        "# Internally mapped to: {0: \"ReLU\", 1: \"Sigmoid\", 2: \"Tanh\"}\n",
        "# Bounds become: [(0, 2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Sampling**: Initial design samples from `[0, n_levels-1]` and rounds to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "# Samples might be: [0.3, 1.8, 2.1]\n",
        "# After rounding: [0, 2, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Evaluation**: Before calling objective function, integers → strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "# [0, 2, 2] → [\"ReLU\", \"Tanh\", \"Tanh\"]\n",
        "# Objective function receives strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Optimization**: Surrogate model works with integers `[0, n_levels-1]`\n",
        "\n",
        "5. **Results**: Final results mapped back to strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "   "
      },
      "source": [
        "result.x[0]  # Returns \"ReLU\", not 0\n",
        "result.X     # All rows contain strings for factor variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variable Type Auto-Detection\n",
        "\n",
        "If you don't specify `var_type`, SpotOptim automatically detects factor variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 1: Explicit var_type (recommended)\n",
        "# This shows the syntax - replace my_function with your actual function\n",
        "\n",
        "# optimizer = SpotOptim(\n",
        "#     fun=my_function,\n",
        "#     bounds=[(-4, -2), (\"ReLU\", \"Tanh\")],\n",
        "#     var_type=[\"num\", \"factor\"]  # Explicit\n",
        "# )\n",
        "\n",
        "# Example 2: Auto-detection (works but less explicit)\n",
        "# optimizer = SpotOptim(\n",
        "#     fun=my_function,\n",
        "#     bounds=[(-4, -2), (\"ReLU\", \"Tanh\")]\n",
        "#     # var_type automatically set to [\"float\", \"factor\"]\n",
        "# )\n",
        "\n",
        "# Here's a working example:\n",
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def demo_function(X):\n",
        "    results = []\n",
        "    for params in X:\n",
        "        lr = 10 ** params[0]  # Continuous parameter\n",
        "        activation = params[1]  # Factor parameter\n",
        "        score = 3000 + lr * 100 + {\"ReLU\": 0, \"Tanh\": 50}.get(activation, 100)\n",
        "        results.append(score + np.random.normal(0, 10))\n",
        "    return np.array(results)\n",
        "\n",
        "# With explicit var_type (recommended)\n",
        "optimizer = SpotOptim(\n",
        "    fun=demo_function,\n",
        "    bounds=[(-4, -2), (\"ReLU\", \"Tanh\")],\n",
        "    var_type=[\"num\", \"factor\"],  # Explicit is clearer\n",
        "    max_iter=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "print(f\"Best lr: {10**result.x[0]:.6f}, Best activation: {result.x[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Example: Full Workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Complete example: Neural network hyperparameter optimization with factor variables.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "\n",
        "def objective_function(X):\n",
        "    \"\"\"Train and evaluate models with given hyperparameters.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        # Extract hyperparameters\n",
        "        log_lr = params[0]\n",
        "        l1 = int(params[1])\n",
        "        num_layers = int(params[2])\n",
        "        activation = params[3]  # String!\n",
        "        \n",
        "        lr = 10 ** log_lr\n",
        "        \n",
        "        print(f\"Testing: lr={lr:.6f}, l1={l1}, layers={num_layers}, \"\n",
        "              f\"activation={activation}\")\n",
        "        \n",
        "        # Load data\n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
        "            test_size=0.2,\n",
        "            batch_size=32,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        # Create and train model\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=l1,\n",
        "            num_hidden_layers=num_layers,\n",
        "            activation=activation\n",
        "        )\n",
        "        \n",
        "        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # Training loop\n",
        "        num_epochs = 30\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                test_loss += loss.item()\n",
        "        \n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "        results.append(avg_test_loss)\n",
        "        print(f\"  → Test MSE: {avg_test_loss:.4f}\")\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Neural Network Hyperparameter Optimization with Factor Variables\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Define optimization problem\n",
        "    optimizer = SpotOptim(\n",
        "        fun=objective_function,\n",
        "        bounds=[\n",
        "            (-4, -2),                                    # log10(learning_rate)\n",
        "            (16, 128),                                   # l1 (neurons)\n",
        "            (0, 4),                                      # num_hidden_layers\n",
        "            (\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")   # activation (factor!)\n",
        "        ],\n",
        "        var_type=[\"num\", \"int\", \"int\", \"factor\"],\n",
        "        max_iter=50,\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    # Run optimization\n",
        "    print(\"\\nStarting optimization...\")\n",
        "    result = optimizer.optimize()\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OPTIMIZATION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Best learning rate: {10**result.x[0]:.6f}\")\n",
        "    print(f\"Best layer size (l1): {int(result.x[1])}\")\n",
        "    print(f\"Best num hidden layers: {int(result.x[2])}\")\n",
        "    print(f\"Best activation function: {result.x[3]}\")  # String value!\n",
        "    print(f\"Best test MSE: {result.fun:.4f}\")\n",
        "    \n",
        "    # Show top 5 configurations\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TOP 5 CONFIGURATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "    sorted_indices = result.y.argsort()[:5]\n",
        "    print(f\"{'Rank':<6} {'LR':<12} {'L1':<6} {'Layers':<8} \"\n",
        "          f\"{'Activation':<12} {'MSE':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "    for rank, idx in enumerate(sorted_indices, 1):\n",
        "        lr = 10 ** result.X[idx, 0]\n",
        "        l1 = int(result.X[idx, 1])\n",
        "        layers = int(result.X[idx, 2])\n",
        "        activation = result.X[idx, 3]\n",
        "        mse = result.y[idx]\n",
        "        print(f\"{rank:<6} {lr:<12.6f} {l1:<6} {layers:<8} \"\n",
        "              f\"{activation:<12} {mse:<10.4f}\")\n",
        "    \n",
        "    # Train final model with best configuration\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING FINAL MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    best_lr = 10 ** result.x[0]\n",
        "    best_l1 = int(result.x[1])\n",
        "    best_layers = int(result.x[2])\n",
        "    best_activation = result.x[3]\n",
        "    \n",
        "    print(f\"Configuration: lr={best_lr:.6f}, l1={best_l1}, \"\n",
        "          f\"layers={best_layers}, activation={best_activation}\")\n",
        "    \n",
        "    train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
        "        test_size=0.2,\n",
        "        batch_size=32,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    final_model = LinearRegressor(\n",
        "        input_dim=10,\n",
        "        output_dim=1,\n",
        "        l1=best_l1,\n",
        "        num_hidden_layers=best_layers,\n",
        "        activation=best_activation\n",
        "    )\n",
        "    \n",
        "    optimizer_final = final_model.get_optimizer(\"Adam\", lr=best_lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # Extended training\n",
        "    num_epochs = 100\n",
        "    print(f\"\\nTraining for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        final_model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            predictions = final_model(batch_X)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            optimizer_final.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_final.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}: Train MSE = {avg_train_loss:.4f}\")\n",
        "    \n",
        "    # Final evaluation\n",
        "    final_model.eval()\n",
        "    final_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            predictions = final_model(batch_X)\n",
        "            final_test_loss += criterion(predictions, batch_y).item()\n",
        "    \n",
        "    final_avg_loss = final_test_loss / len(test_loader)\n",
        "    print(f\"\\nFinal Test MSE: {final_avg_loss:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### Do's\n",
        "\n",
        "✅ **Use descriptive string values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bounds=[(\"xavier_uniform\", \"kaiming_normal\", \"orthogonal\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ **Explicitly specify var_type for clarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "var_type=[\"num\", \"int\", \"factor\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ **Access results as strings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Accessing factor variable results as strings\n",
        "# (This assumes you've run an optimization with activation as a factor variable)\n",
        "\n",
        "# If you have a result from the previous examples:\n",
        "# best_activation = result.x[3]  # For 4-parameter optimization\n",
        "# Or for simpler cases:\n",
        "# best_activation = result.x[0]  # For single-parameter optimization\n",
        "\n",
        "# Example with inline optimization:\n",
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def quick_test(X):\n",
        "    results = []\n",
        "    for params in X:\n",
        "        activation = params[0]\n",
        "        score = {\"ReLU\": 3500, \"Tanh\": 3600}.get(activation, 4000)\n",
        "        results.append(score + np.random.normal(0, 50))\n",
        "    return np.array(results)\n",
        "\n",
        "opt = SpotOptim(\n",
        "    fun=quick_test,\n",
        "    bounds=[(\"ReLU\", \"Tanh\")],\n",
        "    var_type=[\"factor\"],\n",
        "    max_iter=10,\n",
        "    seed=42\n",
        ")\n",
        "result = opt.optimize()\n",
        "\n",
        "# Access as string - this is the correct way\n",
        "best_activation = result.x[0]  # String value like \"ReLU\"\n",
        "print(f\"Best activation: {best_activation} (type: {type(best_activation).__name__})\")\n",
        "\n",
        "# You can use it directly in your model\n",
        "# model = LinearRegressor(activation=best_activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ **Mix factor variables with numeric/integer variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bounds=[(-4, -2), (16, 128), (\"ReLU\", \"Tanh\")]\n",
        "var_type=[\"num\", \"int\", \"factor\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Don'ts\n",
        "\n",
        "❌ **Don't use integers in factor bounds**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Wrong: Use strings, not integers\n",
        "bounds=[(0, 1, 2)]  # Wrong!\n",
        "bounds=[(\"ReLU\", \"Sigmoid\", \"Tanh\")]  # Correct!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "❌ **Don't expect integers in objective function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def objective(X):\n",
        "    activation = X[0][2]\n",
        "    # activation is a string, not an integer!\n",
        "    # Don't do: if activation == 0:  # Wrong!\n",
        "    # Do: if activation == \"ReLU\":   # Correct!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "❌ **Don't manually convert factor variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SpotOptim handles conversion automatically\n",
        "# Don't do manual mapping in your objective function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "❌ **Don't use empty tuples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Wrong: Empty tuple\n",
        "bounds=[()]\n",
        "\n",
        "# Correct: At least one string\n",
        "bounds=[(\"ReLU\",)]  # Single choice (will be treated as fixed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "**Issue**: Objective function receives integers instead of strings\n",
        "\n",
        "**Solution**: Ensure you're using the latest version of SpotOptim with factor variable support. Factor variables are automatically converted before calling the objective function.\n",
        "\n",
        "---\n",
        "\n",
        "**Issue**: `ValueError: could not convert string to float`\n",
        "\n",
        "**Solution**: This occurs if there's a version mismatch. Update SpotOptim to ensure the object array conversion is implemented correctly.\n",
        "\n",
        "---\n",
        "\n",
        "**Issue**: Results show integers instead of strings\n",
        "\n",
        "**Solution**: Check that you're accessing `result.x` (mapped values) instead of internal arrays. The result object automatically maps factor variables to their original strings.\n",
        "\n",
        "---\n",
        "\n",
        "**Issue**: Single-level factor variables cause dimension reduction\n",
        "\n",
        "**Behavior**: If a factor variable has only one choice, e.g., `(\"ReLU\",)`, SpotOptim treats it as a fixed dimension and may reduce the dimensionality. This is expected behavior.\n",
        "\n",
        "**Solution**: Use at least two choices for optimization, or remove single-choice dimensions from bounds.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Factor variables in SpotOptim enable:\n",
        "\n",
        "- ✅ **Categorical optimization**: Optimize over discrete string choices\n",
        "- ✅ **Automatic conversion**: Seamless integer↔string mapping\n",
        "- ✅ **Neural network hyperparameters**: Optimize activation functions, optimizers, etc.\n",
        "- ✅ **Mixed variable types**: Combine with continuous and integer variables\n",
        "- ✅ **Clean interface**: Objective functions work with strings directly\n",
        "- ✅ **String results**: Final results contain original string values\n",
        "\n",
        "Factor variables make categorical hyperparameter optimization as easy as continuous optimization!\n",
        "\n",
        "## See Also\n",
        "\n",
        "- [LinearRegressor Documentation](https://sequential-parameter-optimization.github.io/spotPython/reference/) - Neural network class supporting string-based activation functions\n",
        "- [Diabetes Dataset Utilities](diabetes_dataset.md) - Data loading utilities used in examples\n",
        "- [Variable Types](var_type.md) - Overview of all variable types in SpotOptim\n",
        "- [Save and Load](save_load.md) - Saving and loading optimization results with factor variables\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}