{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Unified Learning Rate Interface in SpotOptim\n",
        "sidebar_position: 5\n",
        "eval: true\n",
        "---\n",
        "\n",
        "This module provides a sophisticated unified learning rate interface for PyTorch optimizers through the `map_lr()` function and integration with `LinearRegressor`.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Different PyTorch optimizers operate on vastly different learning rate scales:\n",
        "\n",
        "- **Adam** typically uses lr ~ 0.0001-0.001\n",
        "- **SGD** typically uses lr ~ 0.01-0.1\n",
        "- **RMSprop** typically uses lr ~ 0.001-0.01\n",
        "\n",
        "This makes it difficult to:\n",
        "\n",
        "1. Compare optimizer performance fairly\n",
        "2. Optimize learning rate as a hyperparameter across different optimizers\n",
        "3. Switch between optimizers without retuning learning rates\n",
        "\n",
        "The `map_lr()` function solves this by providing a unified learning rate scale where **lr=1.0 corresponds to each optimizer's PyTorch default**.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- ✅ **Unified Interface**: Single learning rate parameter works across all optimizers\n",
        "- ✅ **Fair Comparison**: Same unified lr gives optimizer-specific optimal ranges\n",
        "- ✅ **Hyperparameter Optimization**: Optimize one learning rate for multiple optimizers\n",
        "- ✅ **Backward Compatible**: Existing code continues to work\n",
        "- ✅ **Well-tested**: 36 comprehensive tests covering all use cases\n",
        "- ✅ **Documented**: Extensive docstrings and examples\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Basic Usage with LinearRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: basic-unified-lr-example-with-linear-regressor\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "# Create model with unified lr=1.0 (gives each optimizer its default)\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n",
        "\n",
        "# Adam gets 0.001 (its default)\n",
        "optimizer_adam = model.get_optimizer(\"Adam\")\n",
        "\n",
        "# SGD gets 0.01 (its default)\n",
        "optimizer_sgd = model.get_optimizer(\"SGD\")\n",
        "\n",
        "# RMSprop gets 0.01 (its default)\n",
        "optimizer_rmsprop = model.get_optimizer(\"RMSprop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Custom Unified Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: custom-unified-lr-example-0.5\n",
        "# Using lr=0.5 scales all optimizers by 0.5\n",
        "model = LinearRegressor(input_dim=10, output_dim=1, lr=0.5)\n",
        "\n",
        "optimizer_adam = model.get_optimizer(\"Adam\")     # Gets 0.5 * 0.001 = 0.0005\n",
        "optimizer_sgd = model.get_optimizer(\"SGD\")       # Gets 0.5 * 0.01 = 0.005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Direct Use of map_lr()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: direct-map-lr-usage-unified-lr\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "\n",
        "# Map unified lr to optimizer-specific lr\n",
        "lr_adam = map_lr(1.0, \"Adam\")      # Returns 0.001\n",
        "lr_sgd = map_lr(1.0, \"SGD\")        # Returns 0.01\n",
        "lr_rmsprop = map_lr(1.0, \"RMSprop\")  # Returns 0.01\n",
        "\n",
        "# Scale by 2x\n",
        "lr_adam = map_lr(2.0, \"Adam\")      # Returns 0.002\n",
        "lr_sgd = map_lr(2.0, \"SGD\")        # Returns 0.02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: hyperparameter-optimization-unified-learning-rate\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def train_model(X):\n",
        "    \"\"\"Objective function for hyperparameter optimization.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Load data once\n",
        "    train_loader, test_loader, _ = get_diabetes_dataloaders(batch_size=32, random_state=42)\n",
        "    \n",
        "    for params in X:\n",
        "        lr_unified = 10 ** params[0]  # Log scale: [-4, 0]\n",
        "        optimizer_name = params[1]     # Factor: \"Adam\", \"SGD\", \"RMSprop\"\n",
        "        \n",
        "        # Create model with unified lr - automatically scaled per optimizer\n",
        "        torch.manual_seed(42)\n",
        "        model = LinearRegressor(input_dim=10, output_dim=1, l1=32, num_hidden_layers=2, lr=lr_unified)\n",
        "        optimizer = model.get_optimizer(optimizer_name)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # Train\n",
        "        model.train()\n",
        "        for epoch in range(30):\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(batch_X)\n",
        "                loss = criterion(predictions, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                predictions = model(batch_X)\n",
        "                test_loss += criterion(predictions, batch_y).item()\n",
        "        \n",
        "        avg_test_loss = test_loss / len(test_loader)\n",
        "        results.append(avg_test_loss)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Optimize unified lr across different optimizers\n",
        "spot_optimizer = SpotOptim(\n",
        "    fun=train_model,\n",
        "    bounds=[(-4, 0), (\"Adam\", \"SGD\", \"RMSprop\")],\n",
        "    var_type=[\"float\", \"factor\"],\n",
        "    max_iter=10,  # Small for demo\n",
        "    n_initial=5,\n",
        "    seed=42\n",
        ")\n",
        "result = spot_optimizer.optimize()\n",
        "\n",
        "print(f\"\\nBest unified lr: {10**result.x[0]:.6f}\")\n",
        "print(f\"Best optimizer: {result.x[1]}\")\n",
        "print(f\"Best test MSE: {result.fun:.4f}\")\n",
        "\n",
        "# Show actual learning rate used\n",
        "from spotoptim.utils.mapping import map_lr\n",
        "actual_lr = map_lr(10**result.x[0], result.x[1])\n",
        "print(f\"Actual {result.x[1]} learning rate: {actual_lr:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supported Optimizers\n",
        "\n",
        "All major PyTorch optimizers are supported with their default learning rates:\n",
        "\n",
        "| Optimizer | Default LR | Typical Range |\n",
        "|-----------|------------|---------------|\n",
        "| Adam      | 0.001      | 0.0001-0.01   |\n",
        "| AdamW     | 0.001      | 0.0001-0.01   |\n",
        "| Adamax    | 0.002      | 0.0001-0.01   |\n",
        "| NAdam     | 0.002      | 0.0001-0.01   |\n",
        "| RAdam     | 0.001      | 0.0001-0.01   |\n",
        "| SGD       | 0.01       | 0.001-0.1     |\n",
        "| RMSprop   | 0.01       | 0.001-0.1     |\n",
        "| Adagrad   | 0.01       | 0.001-0.1     |\n",
        "| Adadelta  | 1.0        | 0.1-10.0      |\n",
        "| ASGD      | 0.01       | 0.001-0.1     |\n",
        "| LBFGS     | 1.0        | 0.1-10.0      |\n",
        "| Rprop     | 0.01       | 0.001-0.1     |\n",
        "\n",
        "## API Reference\n",
        "\n",
        "### `map_lr(lr_unified, optimizer_name, use_default_scale=True)`\n",
        "\n",
        "Maps a unified learning rate to an optimizer-specific learning rate.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "- `lr_unified` (float): Unified learning rate multiplier. Typical range: [0.001, 100.0]\n",
        "- `optimizer_name` (str): Name of the PyTorch optimizer\n",
        "- `use_default_scale` (bool): Whether to scale by optimizer's default (default: True)\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- `float`: The optimizer-specific learning rate\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "lr = map_lr(1.0, \"Adam\")  # Returns 0.001 (Adam's default)\n",
        "lr = map_lr(0.5, \"SGD\")   # Returns 0.005 (0.5 * SGD's default)\n",
        "```\n",
        "\n",
        "### `LinearRegressor(..., lr=1.0)`\n",
        "\n",
        "**Parameter:**\n",
        "\n",
        "- `lr` (float): Unified learning rate multiplier. Default: 1.0\n",
        "\n",
        "**New Behavior in `get_optimizer()`:**\n",
        "\n",
        "- If `lr` is not specified, uses `self.lr`\n",
        "- Automatically maps unified lr to optimizer-specific lr\n",
        "- Can override model's lr by passing `lr` parameter\n",
        "\n",
        "## Design Rationale\n",
        "\n",
        "### Why Unified Learning Rates?\n",
        "\n",
        "The approach is based on spotPython's `optimizer_handler()` but improved:\n",
        "\n",
        "1. **Separation of Concerns**: Mapping logic in separate, testable module\n",
        "2. **Flexibility**: Can be used independently or integrated with models\n",
        "3. **Transparency**: Clear mapping based on PyTorch defaults\n",
        "4. **Extensibility**: Easy to add new optimizers\n",
        "5. **Type Safety**: Comprehensive error handling and validation\n",
        "\n",
        "### Comparison with spotPython\n",
        "\n",
        "| Feature | spotPython | spotoptim |\n",
        "|---------|-----------|-----------|\n",
        "| Approach | `lr_mult * default_lr` | `map_lr(lr_unified, optimizer)` |\n",
        "| Module | `optimizer_handler()` | `map_lr()` + integration |\n",
        "| Testing | Minimal | 36 comprehensive tests |\n",
        "| Documentation | Basic | Extensive with examples |\n",
        "| Reusability | Coupled | Standalone function |\n",
        "| Error Handling | Basic | Comprehensive validation |\n",
        "\n",
        "### Log-scale Optimization\n",
        "\n",
        "For hyperparameter optimization, use log-scale for unified lr:\n",
        "\n",
        "```python\n",
        "# Sample from log10 scale [-4, 0]\n",
        "log_lr = -2.5  # Sampled value\n",
        "lr_unified = 10 ** log_lr  # 0.00316\n",
        "\n",
        "# Map to optimizer-specific\n",
        "lr_adam = map_lr(lr_unified, \"Adam\")  # 0.00316 * 0.001 = 0.00000316\n",
        "lr_sgd = map_lr(lr_unified, \"SGD\")    # 0.00316 * 0.01 = 0.0000316\n",
        "```\n",
        "\n",
        "This gives a reasonable search range across all optimizers.\n",
        "\n",
        "## Examples\n",
        "\n",
        "See `examples/unified_learning_rate_demo.py` for comprehensive examples including:\n",
        "1. Basic unified interface usage\n",
        "2. Custom unified learning rates\n",
        "3. Training with different optimizers\n",
        "4. Direct use of map_lr()\n",
        "5. Log-scale hyperparameter optimization\n",
        "6. Complete hyperparameter optimization scenario\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "- [PyTorch Optimizer Documentation](https://pytorch.org/docs/stable/optim.html)\n",
        "- spotPython's `optimizer_handler()` function (inspiration)\n",
        "- [Hyperparameter Optimization Best Practices](https://arxiv.org/abs/2003.05689)\n",
        "\n",
        "## Contributing\n",
        "\n",
        "When adding new optimizers:\n",
        "\n",
        "1. Add default lr to `OPTIMIZER_DEFAULT_LR` dict in `mapping.py`\n",
        "2. Verify the default against PyTorch documentation\n",
        "3. Add tests in `test_mapping.py`\n",
        "4. Update this README\n",
        "\n",
        "## License\n",
        "\n",
        "Same as spotoptim package (see main LICENSE file)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim-cookbook/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}