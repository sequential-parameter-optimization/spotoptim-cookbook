{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Hyperparameter Tuning for Physics-Informed Neural Networks\"\n",
        "subtitle: \"Using SpotOptim to Optimize PINN Architecture and Training\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Overview\n",
        "\n",
        "This tutorial demonstrates how to use SpotOptim for hyperparameter optimization of Physics-Informed Neural Networks (PINNs). We'll optimize the network architecture and training parameters to find the best configuration for solving an ordinary differential equation.\n",
        "\n",
        "Building on the basic PINN demo, we'll now systematically search for optimal:\n",
        "\n",
        "- Number of neurons per hidden layer\n",
        "- Number of hidden layers\n",
        "- Activation function (categorical)\n",
        "- Optimizer algorithm (categorical)\n",
        "- Learning rate (log-scale)\n",
        "- Physics loss weight (log-scale)\n",
        "\n",
        "## Key Features\n",
        "\n",
        "### 1. PyTorch Dataset and DataLoader\n",
        "\n",
        "Following PyTorch best practices from the [official tutorial](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html), this tutorial implements:\n",
        "\n",
        "- **Custom Dataset Classes**: Separate classes for supervised data (`PINNDataset`) and collocation points (`CollocationDataset`)\n",
        "- **DataLoader Integration**: Efficient batch processing with configurable batch size, shuffling, and parallel loading\n",
        "- **Proper Data Separation**: Clean separation of training, validation, and collocation data\n",
        "- **Gradient Tracking**: Automatic gradient handling for collocation points needed in physics loss\n",
        "\n",
        "Benefits:\n",
        "\n",
        "- **Modularity**: Clean separation between data and model code\n",
        "- **Efficiency**: Batch processing and optional parallel data loading\n",
        "- **Scalability**: Easy to extend to larger datasets\n",
        "- **Best Practices**: Follows PyTorch conventions used across the ecosystem\n",
        "\n",
        "### 2. Automatic Transformation Handling\n",
        "\n",
        "This tutorial also showcases SpotOptim's `var_trans` feature for automatic variable transformations. Learning rates and regularization parameters are often best explored on a log scale, but manually transforming values is tedious and error-prone. With `var_trans`, you simply specify:\n",
        "\n",
        "```python\n",
        "var_trans = [None, None, \"log10\", \"log10\"]\n",
        "```\n",
        "\n",
        "SpotOptim then:\n",
        "\n",
        "- Optimizes internally in log-transformed space (efficient exploration)\n",
        "- Passes original-scale values to your objective function (no manual conversion needed)\n",
        "- Displays all results in original scale (easy interpretation)\n",
        "\n",
        "This eliminates the need for manual `10**x` conversions throughout your code!\n",
        "\n",
        "# The Problem\n",
        "\n",
        "We're solving the same ODE as in the basic PINN demo:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dt} + 0.1 y - \\sin\\left(\\frac{\\pi t}{2}\\right) = 0\n",
        "$$\n",
        "\n",
        "with initial condition $y(0) = 0$.\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: setup-pinn2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set number of epochs for training\n",
        "N_EPOCHS=10000\n",
        "\n",
        "# \n",
        "MAX_ITER = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Generation\n",
        "\n",
        "Following PyTorch best practices, we'll create custom Dataset classes for our PINN data.\n",
        "\n",
        "## Custom Dataset Classes\n",
        "\n",
        "We'll create two dataset types:\n",
        "\n",
        "1. `PINNDataset` for supervised data (training and validation)\n",
        "2. `CollocationDataset` for physics-informed collocation points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: data-generation-pinn2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def oscillator(\n",
        "    n_steps: int = 3000,\n",
        "    t_min: float = 0.0,\n",
        "    t_max: float = 30.0,\n",
        "    y0: float = 0.0,\n",
        "    alpha: float = 0.1,\n",
        "    omega: float = np.pi / 2\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Solve ODE: dy/dt + alpha*y - sin(omega*t) = 0\n",
        "    using RK2 (midpoint method).\n",
        "    \n",
        "    Returns:\n",
        "        t_tensor: Time points, shape (n_steps, 1)\n",
        "        y_tensor: Solution values, shape (n_steps, 1)\n",
        "    \"\"\"\n",
        "    t_step = (t_max - t_min) / n_steps\n",
        "    t_points = np.arange(t_min, t_min + n_steps * t_step, t_step)[:n_steps]\n",
        "    \n",
        "    y = [y0]\n",
        "    \n",
        "    for t_current_step_end in t_points[1:]:\n",
        "        t_midpoint = t_current_step_end - t_step / 2.0\n",
        "        y_prev = y[-1]\n",
        "        \n",
        "        slope_at_t_mid = -alpha * y_prev + np.sin(omega * t_midpoint)\n",
        "        y_intermediate = y_prev + (t_step / 2.0) * slope_at_t_mid\n",
        "        \n",
        "        slope_at_t_end = -alpha * y_intermediate + np.sin(omega * t_current_step_end)\n",
        "        y_next = y_prev + t_step * slope_at_t_end\n",
        "        y.append(y_next)\n",
        "    \n",
        "    t_tensor = torch.tensor(t_points, dtype=torch.float32).view(-1, 1)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "    \n",
        "    return t_tensor, y_tensor\n",
        "\n",
        "\n",
        "class PINNDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for PINN supervised data (training/validation).\n",
        "    \n",
        "    This dataset stores time-solution pairs (t, y) for supervised learning.\n",
        "    \n",
        "    Args:\n",
        "        t (torch.Tensor): Time points, shape (n_samples, 1)\n",
        "        y (torch.Tensor): Solution values, shape (n_samples, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, t: torch.Tensor, y: torch.Tensor):\n",
        "        self.t = t\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.t)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.t[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class CollocationDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for PINN collocation points.\n",
        "    \n",
        "    This dataset stores time points where physics loss is evaluated.\n",
        "    Gradients are required for computing derivatives in the PDE.\n",
        "    \n",
        "    Args:\n",
        "        t (torch.Tensor): Collocation time points, shape (n_points, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, t: torch.Tensor):\n",
        "        # Store collocation points with gradient tracking\n",
        "        self.t = t.requires_grad_(True)\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.t)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        # Return single collocation point (still requires_grad)\n",
        "        return self.t[idx].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate exact solution using RK2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_exact, y_exact = oscillator()\n",
        "\n",
        "# Create training data (sparse sampling)\n",
        "t_train = x_exact[0:3000:119]\n",
        "y_train = y_exact[0:3000:119]\n",
        "\n",
        "# Create validation data (different sampling for unbiased evaluation)\n",
        "t_val = x_exact[50:3000:120]\n",
        "y_val = y_exact[50:3000:120]\n",
        "\n",
        "# Create collocation points for physics loss\n",
        "t_physics = torch.linspace(0, 30, 50).view(-1, 1)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = PINNDataset(t_train, y_train)\n",
        "val_dataset = PINNDataset(t_val, y_val)\n",
        "collocation_dataset = CollocationDataset(t_physics)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Collocation dataset size: {len(collocation_dataset)}\")\n",
        "print(f\"\\nSample from training dataset:\")\n",
        "t_sample, y_sample = train_dataset[0]\n",
        "print(f\"  t: {t_sample.item():.4f}, y: {y_sample.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the PINN Training Function\n",
        "\n",
        "This function creates DataLoaders and trains a PINN with given hyperparameters.\n",
        "Following PyTorch best practices, we use DataLoader for efficient batch processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-training-function-pinn2\n",
        "def train_pinn(\n",
        "    l1: int,\n",
        "    num_layers: int,\n",
        "    activation: str,\n",
        "    optimizer_name: str,\n",
        "    lr_unified: float,\n",
        "    alpha: float,\n",
        "    n_epochs: int = N_EPOCHS,\n",
        "    batch_size: int = 16,\n",
        "    verbose: bool = False\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train a PINN with specified hyperparameters using DataLoaders.\n",
        "    \n",
        "    Args:\n",
        "        l1: Number of neurons per hidden layer\n",
        "        num_layers: Number of hidden layers\n",
        "        activation: Activation function (\"Tanh\", \"ReLU\", \"Sigmoid\", \"GELU\")\n",
        "        optimizer_name: Optimizer algorithm (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\")\n",
        "        lr_unified: Unified learning rate multiplier\n",
        "        alpha: Weight for physics loss\n",
        "        n_epochs: Number of training epochs\n",
        "        batch_size: Batch size for DataLoader\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        Validation mean squared error\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    \n",
        "    # For collocation points, we can use full batch since it's small\n",
        "    collocation_loader = DataLoader(\n",
        "        collocation_dataset,\n",
        "        batch_size=len(collocation_dataset),\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    # Create model\n",
        "    model = LinearRegressor(\n",
        "        input_dim=1,\n",
        "        output_dim=1,\n",
        "        l1=l1,\n",
        "        num_hidden_layers=num_layers,\n",
        "        activation=activation,\n",
        "        lr=lr_unified\n",
        "    )\n",
        "    \n",
        "    # Get optimizer\n",
        "    optimizer = model.get_optimizer(optimizer_name)\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        # Get collocation points (full batch)\n",
        "        t_physics_batch = next(iter(collocation_loader))\n",
        "        # Ensure gradients are enabled\n",
        "        t_physics_batch = t_physics_batch.requires_grad_(True)\n",
        "        \n",
        "        # Iterate over training batches\n",
        "        for batch_t, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Data Loss\n",
        "            y_pred = model(batch_t)\n",
        "            loss_data = torch.mean((y_pred - batch_y)**2)\n",
        "            \n",
        "            # Physics Loss (computed on full collocation set)\n",
        "            y_physics = model(t_physics_batch)\n",
        "            dy_dt = torch.autograd.grad(\n",
        "                y_physics,\n",
        "                t_physics_batch,\n",
        "                torch.ones_like(y_physics),\n",
        "                create_graph=True,\n",
        "                retain_graph=True\n",
        "            )[0]\n",
        "            \n",
        "            # PDE residual: dy/dt + 0.1*y - sin(pi*t/2) = 0\n",
        "            physics_residual = dy_dt + 0.1 * y_physics - torch.sin(np.pi * t_physics_batch / 2)\n",
        "            loss_physics = torch.mean(physics_residual**2)\n",
        "            \n",
        "            # Total Loss\n",
        "            loss = loss_data + alpha * loss_physics\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        if verbose and (epoch + 1) % 2000 == 0:\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"  Epoch {epoch+1}/{n_epochs}: Avg Loss = {avg_loss:.6f}\")\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_mse = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_t, batch_y in val_loader:\n",
        "            y_pred = model(batch_t)\n",
        "            val_mse += torch.mean((batch_y - y_pred)**2).item()\n",
        "    \n",
        "    val_mse /= len(val_loader)\n",
        "    return val_mse\n",
        "\n",
        "# Test the function with default parameters\n",
        "print(\"Testing PINN training function with DataLoaders...\")\n",
        "test_error = train_pinn(\n",
        "    l1=32, \n",
        "    num_layers=2,\n",
        "    activation=\"Tanh\",\n",
        "    optimizer_name=\"Adam\",\n",
        "    lr_unified=3.0, \n",
        "    alpha=0.06,\n",
        "    batch_size=16,\n",
        "    n_epochs=N_EPOCHS, \n",
        "    verbose=True\n",
        ")\n",
        "print(f\"\\nTest validation MSE: {test_error:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization with SpotOptim\n",
        "\n",
        "Now we'll use SpotOptim to find the best hyperparameters:\n",
        "\n",
        "## Define the Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-objective-function-pinn2\n",
        "def objective_pinn(X):\n",
        "    \"\"\"\n",
        "    Objective function for SpotOptim.\n",
        "    \n",
        "    Args:\n",
        "        X: Array of hyperparameter configurations, shape (n_configs, 6)\n",
        "           Each row: [l1, num_layers, activation, optimizer, lr_unified, alpha]\n",
        "           Note: SpotOptim handles log transformations and factor mapping automatically\n",
        "    \n",
        "    Returns:\n",
        "        Array of validation errors\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, params in enumerate(X):\n",
        "        # Extract parameters (already in original scale thanks to var_trans)\n",
        "        # Factor variables (activation, optimizer) are returned as strings\n",
        "        l1 = int(params[0])                    # Number of neurons\n",
        "        num_layers = int(params[1])            # Number of hidden layers\n",
        "        activation = params[2]                 # Activation function\n",
        "        optimizer_name = params[3]             # Optimizer algorithm\n",
        "        lr_unified = params[4]                 # Learning rate\n",
        "        alpha = params[5]                      # Physics weight\n",
        "        \n",
        "        print(f\"\\nConfiguration {i+1}/{len(X)}:\")\n",
        "        print(f\"  l1={l1}, num_layers={num_layers}, activation={activation}, \")\n",
        "        print(f\"  optimizer={optimizer_name}, lr_unified={lr_unified:.4f}, alpha={alpha:.4f}\")\n",
        "        \n",
        "        # Train PINN with these hyperparameters\n",
        "        val_error = train_pinn(\n",
        "            l1=l1,\n",
        "            num_layers=num_layers,\n",
        "            activation=activation,\n",
        "            optimizer_name=optimizer_name,\n",
        "            lr_unified=lr_unified,\n",
        "            alpha=alpha,\n",
        "            n_epochs=N_EPOCHS,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        print(f\"  Validation MSE: {val_error:.6f}\")\n",
        "        results.append(val_error)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Test the objective function\n",
        "print(\"Testing objective function with 2 configurations...\")\n",
        "X_test = np.array([\n",
        "    [32, 2, \"Tanh\", \"Adam\", 3.0, 0.06],    # Baseline config\n",
        "    [64, 3, \"ReLU\", \"AdamW\", 2.0, 0.04]    # Alternative config\n",
        "], dtype=object)\n",
        "test_results = objective_pinn(X_test)\n",
        "print(f\"\\nTest results: {test_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Optimization\n",
        "\n",
        "Use `tensorboard --logdir=runs` from a shell in the current directory (where this notebook is located) to visualize the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-hyperparameter-optimization-pinn2\n",
        "# Define search space with var_trans for automatic log-scale handling\n",
        "bounds = [\n",
        "    (2, 32),                                      # l1: neurons per layer (16 to 128)\n",
        "    (1, 4),                                         # num_layers: 1 to 4 hidden layers\n",
        "    (\"Tanh\", \"ReLU\", \"Sigmoid\", \"GELU\"),         # activation: activation function\n",
        "    (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),          # optimizer: optimizer algorithm\n",
        "    (0.1, 10.0),                                    # lr_unified: learning rate (0.1 to 10)\n",
        "    (0.01, 1.0)                                     # alpha: physics weight (0.01 to 1.0)\n",
        "]\n",
        "\n",
        "var_type = [\"int\", \"int\", \"factor\", \"factor\", \"num\", \"num\"]\n",
        "var_name = [\"l1\", \"num_layers\", \"activation\", \"optimizer\", \"lr_unified\", \"alpha\"]\n",
        "\n",
        "# Use var_trans to handle log-scale transformations automatically\n",
        "# Factor variables don't need transformations (None)\n",
        "var_trans = [None, None, None, None, \"log10\", \"log10\"]\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective_pinn,\n",
        "    bounds=bounds,\n",
        "    var_type=var_type,\n",
        "    var_name=var_name,\n",
        "    var_trans=var_trans,  # Automatic log-scale handling!\n",
        "    max_iter=MAX_ITER,\n",
        "    n_initial=10,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "    tensorboard_clean=True,\n",
        "    tensorboard_log=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display search space configuration.\n",
        "The `trans`column shows applied transformations. `lr_unified` and `alpha` use log10 transformation internally.\n",
        "This enables efficient exploration of log-scale parameters. All values shown are in original scale (not transformed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display search space configuration\n",
        "design_table = optimizer.print_design_table(tablefmt=\"github\")\n",
        "print(design_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-run-optimization-pinn2\n",
        "result = optimizer.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Analysis\n",
        "\n",
        "## Best Configuration\n",
        "\n",
        "Display best hyperparameters using print_best() method.\n",
        "With `var_trans`, results are already in original scale!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-best-configuration-pinn2\n",
        "optimizer.print_best(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store values for later use in visualizations.  Values are already in original scale thanks to `var_trans`.\n",
        "Factor variables are returned as strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_l1 = int(result.x[0])\n",
        "best_num_layers = int(result.x[1])\n",
        "best_activation = result.x[2]\n",
        "best_optimizer = result.x[3]\n",
        "best_lr_unified = result.x[4]\n",
        "best_alpha = result.x[5]\n",
        "best_val_error = result.fun\n",
        "\n",
        "print(f\"Best activation: {best_activation}\")\n",
        "print(f\"Best optimizer: {best_optimizer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Table with Importance Scores\n",
        "\n",
        "Display comprehensive results table with importance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-results-table-pinn2\n",
        "table = optimizer.print_results_table(show_importance=True, tablefmt=\"github\")\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-optimization-history-pinn2\n",
        "optimizer.plot_progress(log_y=True, ylabel=\"Validation MSE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Surrogate Visualization\n",
        "\n",
        "Visualize the surrogate model's learned response surface for the most important hyperparameter combinations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-surrogate-visualization-pinn2\n",
        "# Plot top 3 most important hyperparameter combinations\n",
        "optimizer.plot_important_hyperparameter_contour(max_imp=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-parameter-scatter-pinn2\n",
        "optimizer.plot_parameter_scatter(\n",
        "    result,\n",
        "    ylabel=\"Validation MSE\",\n",
        "    cmap=\"plasma\",\n",
        "    figsize=(14, 12)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Final Model with Best Hyperparameters\n",
        "\n",
        "Now let's train a final model with the optimized hyperparameters using DataLoaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-final-model-training-pinn2\n",
        "print(\"Training final model with best hyperparameters using DataLoaders...\")\n",
        "print(f\"Training for 30,000 epochs...\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create DataLoaders for final training\n",
        "final_batch_size = 16\n",
        "train_loader_final = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=final_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "collocation_loader_final = DataLoader(\n",
        "    collocation_dataset,\n",
        "    batch_size=len(collocation_dataset),\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Create model with best hyperparameters\n",
        "final_model = LinearRegressor(\n",
        "    input_dim=1,\n",
        "    output_dim=1,\n",
        "    l1=best_l1,\n",
        "    num_hidden_layers=best_num_layers,\n",
        "    activation=best_activation,\n",
        "    lr=best_lr_unified\n",
        ")\n",
        "\n",
        "optimizer_final = final_model.get_optimizer(best_optimizer)\n",
        "\n",
        "# Training with history tracking\n",
        "loss_history = []\n",
        "n_epochs_final = 30000\n",
        "\n",
        "for epoch in range(n_epochs_final):\n",
        "    final_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    # Get collocation points\n",
        "    t_physics_batch = next(iter(collocation_loader_final))\n",
        "    t_physics_batch = t_physics_batch.requires_grad_(True)\n",
        "    \n",
        "    # Iterate over training batches\n",
        "    for batch_t, batch_y in train_loader_final:\n",
        "        optimizer_final.zero_grad()\n",
        "        \n",
        "        # Data Loss\n",
        "        y_pred = final_model(batch_t)\n",
        "        loss_data = torch.mean((y_pred - batch_y)**2)\n",
        "        \n",
        "        # Physics Loss\n",
        "        y_physics = final_model(t_physics_batch)\n",
        "        dy_dt = torch.autograd.grad(\n",
        "            y_physics,\n",
        "            t_physics_batch,\n",
        "            torch.ones_like(y_physics),\n",
        "            create_graph=True,\n",
        "            retain_graph=True\n",
        "        )[0]\n",
        "        \n",
        "        physics_residual = dy_dt + 0.1 * y_physics - torch.sin(np.pi * t_physics_batch / 2)\n",
        "        loss_physics = torch.mean(physics_residual**2)\n",
        "        \n",
        "        # Total Loss\n",
        "        loss = loss_data + best_alpha * loss_physics\n",
        "        loss.backward()\n",
        "        optimizer_final.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    # Record average loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        avg_loss = epoch_loss / len(train_loader_final)\n",
        "        loss_history.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 5000 == 0:\n",
        "        avg_loss = epoch_loss / len(train_loader_final)\n",
        "        print(f\"  Epoch {epoch+1}/{n_epochs_final}: Avg Loss = {avg_loss:.6f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history, linewidth=1.5)\n",
        "plt.xlabel('Iteration (Ã—100)', fontsize=11)\n",
        "plt.ylabel('Average Total Loss per Batch', fontsize=11)\n",
        "plt.title('Final Model Training History', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-final-model-evaluation-pinn2\n",
        "# Create validation DataLoader for evaluation\n",
        "val_loader_final = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=len(val_dataset),\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Evaluate on validation set\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Validation MSE using DataLoader\n",
        "    val_mse_total = 0.0\n",
        "    for batch_t, batch_y in val_loader_final:\n",
        "        y_pred = final_model(batch_t)\n",
        "        val_mse_total += torch.mean((y_pred - batch_y)**2).item()\n",
        "    final_val_mse = val_mse_total / len(val_loader_final)\n",
        "    \n",
        "    # Predict on full domain for visualization\n",
        "    y_pred_full = final_model(x_exact)\n",
        "    full_mse = torch.mean((y_pred_full - y_exact)**2).item()\n",
        "    \n",
        "    # Compute maximum absolute error\n",
        "    max_error = torch.max(torch.abs(y_pred_full - y_exact)).item()\n",
        "\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"  Validation MSE: {final_val_mse:.6f}\")\n",
        "print(f\"  Full domain MSE: {full_mse:.6f}\")\n",
        "print(f\"  Maximum absolute error: {max_error:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Final Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-final-model-visualization-pinn2\n",
        "# Generate predictions\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = final_model(x_exact)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Plot 1: Solution comparison\n",
        "ax1 = axes[0]\n",
        "ax1.plot(x_exact.numpy(), y_exact.numpy(), 'b-', linewidth=2.5, \n",
        "         label='Exact solution', alpha=0.8)\n",
        "ax1.plot(x_exact.numpy(), y_pred.numpy(), 'r--', linewidth=2, \n",
        "         label='PINN prediction (optimized)', alpha=0.8)\n",
        "\n",
        "# Plot training data from dataset\n",
        "ax1.scatter(train_dataset.t.numpy(), train_dataset.y.numpy(), \n",
        "            color='tab:orange', s=80, label='Training data', \n",
        "            zorder=5, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Plot collocation points\n",
        "t_collocation = collocation_dataset.t.detach()\n",
        "ax1.scatter(t_collocation.numpy(), \n",
        "            final_model(t_collocation).detach().numpy(),\n",
        "            color='green', marker='x', s=50, \n",
        "            label='Collocation points', alpha=0.7, zorder=4)\n",
        "\n",
        "ax1.set_xlabel('Time t', fontsize=12)\n",
        "ax1.set_ylabel('Solution y(t)', fontsize=12)\n",
        "ax1.set_title('Optimized PINN Solution vs Exact Solution', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=11, loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Error\n",
        "ax2 = axes[1]\n",
        "error = torch.abs(y_pred - y_exact)\n",
        "ax2.plot(x_exact.numpy(), error.numpy(), 'r-', linewidth=2)\n",
        "ax2.axhline(y=max_error, color='gray', linestyle='--', linewidth=1, \n",
        "            label=f'Max error = {max_error:.6f}')\n",
        "ax2.set_xlabel('Time t', fontsize=12)\n",
        "ax2.set_ylabel('Absolute Error |y_exact - y_PINN|', fontsize=12)\n",
        "ax2.set_title('Approximation Error (Optimized Model)', fontsize=13, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison with Baseline\n",
        "\n",
        "Let's compare the optimized configuration with a baseline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-baseline-comparison-pinn2\n",
        "print(\"Training baseline model for comparison...\")\n",
        "\n",
        "# Baseline configuration (from basic PINN demo)\n",
        "baseline_config = {\n",
        "    'l1': 32,\n",
        "    'num_layers': 3,\n",
        "    'activation': 'Tanh',\n",
        "    'optimizer': 'Adam',\n",
        "    'lr_unified': 3.0,\n",
        "    'alpha': 0.06\n",
        "}\n",
        "\n",
        "print(f\"\\nBaseline Configuration:\")\n",
        "for key, val in baseline_config.items():\n",
        "    print(f\"  {key}: {val}\")\n",
        "\n",
        "# Train baseline\n",
        "baseline_error = train_pinn(\n",
        "    l1=baseline_config['l1'],\n",
        "    num_layers=baseline_config['num_layers'],\n",
        "    activation=baseline_config['activation'],\n",
        "    optimizer_name=baseline_config['optimizer'],\n",
        "    lr_unified=baseline_config['lr_unified'],\n",
        "    alpha=baseline_config['alpha'],\n",
        "    n_epochs=N_EPOCHS,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"\\nValidation MSE Comparison:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"  Baseline: {baseline_error:.6f}\")\n",
        "print(f\"  Optimized: {best_val_error:.6f}\")\n",
        "print(f\"  Improvement: {(1 - best_val_error/baseline_error)*100:.1f}%\")\n",
        "\n",
        "# Bar plot comparison\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "configs = ['Baseline', 'Optimized']\n",
        "errors = [baseline_error, best_val_error]\n",
        "colors = ['tab:blue', 'tab:green']\n",
        "\n",
        "bars = ax.bar(configs, errors, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, error in zip(bars, errors):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{error:.6f}',\n",
        "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Validation MSE', fontsize=12)\n",
        "ax.set_title('Model Comparison: Baseline vs Optimized', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Sensitivity Analysis\n",
        "\n",
        "Let's analyze how sensitive the model is to each hyperparameter using the enhanced `plot_parameter_scatter()` method with Spearman correlation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-sensitivity-analysis-pinn2\n",
        "# Use the enhanced plot_parameter_scatter method with correlation display\n",
        "optimizer.plot_parameter_scatter(\n",
        "    result,\n",
        "    ylabel=\"Validation MSE\",\n",
        "    cmap=\"plasma\",\n",
        "    figsize=(15, 10),\n",
        "    show_correlation=True,\n",
        "    log_y=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sensitivity Analysis (Spearman Correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the new sensitivity_spearman() method for tabular output\n",
        "optimizer.sensitivity_spearman()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "## Key Findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-summary-pinn2\n",
        "# Get optimization history for statistics\n",
        "history = optimizer.y_\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYPERPARAMETER OPTIMIZATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. BEST CONFIGURATION FOUND:\")\n",
        "print(f\"   - Neurons per layer (l1): {best_l1}\")\n",
        "print(f\"   - Number of hidden layers: {best_num_layers}\")\n",
        "print(f\"   - Activation function: {best_activation}\")\n",
        "print(f\"   - Optimizer: {best_optimizer}\")\n",
        "print(f\"   - Learning rate: {best_lr_unified:.4f}\")\n",
        "print(f\"   - Physics weight (alpha): {best_alpha:.4f}\")\n",
        "\n",
        "print(\"\\n2. PERFORMANCE:\")\n",
        "print(f\"   - Validation MSE: {best_val_error:.6f}\")\n",
        "print(f\"   - Full domain MSE: {full_mse:.6f}\")\n",
        "print(f\"   - Maximum absolute error: {max_error:.6f}\")\n",
        "\n",
        "print(\"\\n3. OPTIMIZATION STATISTICS:\")\n",
        "print(f\"   - Total evaluations: {result.nfev}\")\n",
        "print(f\"   - Initial best: {history[0]:.6f}\")\n",
        "print(f\"   - Final best: {best_val_error:.6f}\")\n",
        "print(f\"   - Improvement: {(1 - best_val_error/history[0])*100:.1f}%\")\n",
        "\n",
        "print(\"\\n4. COMPARISON TO BASELINE:\")\n",
        "print(f\"   - Baseline MSE: {baseline_error:.6f}\")\n",
        "print(f\"   - Optimized MSE: {best_val_error:.6f}\")\n",
        "print(f\"   - Improvement: {(1 - best_val_error/baseline_error)*100:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "Based on the hyperparameter optimization results:\n",
        "\n",
        "1. **Network Architecture**:\n",
        "   - The optimal architecture was found with `{best_l1}` neurons and `{best_num_layers}` hidden layers\n",
        "   - Best activation function: `{best_activation}`\n",
        "   - This balances model capacity with training efficiency\n",
        "\n",
        "2. **Optimizer Selection**:\n",
        "   - Best optimizer: `{best_optimizer}`\n",
        "   - Different optimizers have different convergence characteristics for PINNs\n",
        "\n",
        "3. **Learning Rate**:\n",
        "   - Optimal unified learning rate: `{best_lr_unified:.4f}`\n",
        "   - This translates to an actual Adam learning rate of `{best_lr_unified * 0.001:.6f}`\n",
        "\n",
        "4. **Physics Loss Weight**:\n",
        "   - Optimal alpha: `{best_alpha:.4f}`\n",
        "   - This balances data fitting with physics constraint satisfaction\n",
        "\n",
        "5. **Training Strategy**:\n",
        "   - Start with a broad search space to explore different architectures\n",
        "   - Use `var_trans` with \"log10\" for learning rate and physics weight parameters\n",
        "   - This enables efficient exploration of log-scale parameters without manual transformations\n",
        "   - Validate on held-out data to prevent overfitting to training points\n",
        "\n",
        "6. **Benefits of var_trans and Factor Variables**:\n",
        "   - **Factor variables**: Categorical choices (activation, optimizer) handled automatically\n",
        "   - SpotOptim maps strings to integers internally and back to strings in results\n",
        "   - **Cleaner code**: No manual `10**x` conversions in objective function\n",
        "   - **Fewer errors**: Eliminates confusion about which scale values are in\n",
        "   - **Better optimization**: Searches efficiently in transformed space\n",
        "   - **Easier interpretation**: All results displayed in original scale\n",
        "\n",
        "## Using These Results\n",
        "\n",
        "To use the optimized configuration in your own PINN problems:\n",
        "\n",
        "```python\n",
        "# Create optimized PINN\n",
        "model = LinearRegressor(\n",
        "    input_dim=1,\n",
        "    output_dim=1,\n",
        "    l1={best_l1},\n",
        "    num_hidden_layers={best_num_layers},\n",
        "    activation=\"{best_activation}\",\n",
        "    lr={best_lr_unified:.4f}\n",
        ")\n",
        "\n",
        "optimizer = model.get_optimizer(\"{best_optimizer}\")\n",
        "\n",
        "# Use alpha={best_alpha:.4f} for physics loss weight\n",
        "loss = data_loss + {best_alpha:.4f} * physics_loss\n",
        "```\n",
        "\n",
        "## Using var_trans for Your Hyperparameter Optimization\n",
        "\n",
        "When setting up optimization for your own PINN problems:\n",
        "\n",
        "```python\n",
        "from spotoptim import SpotOptim\n",
        "\n",
        "# Define search space with factor variables and log-scale parameters\n",
        "bounds = [\n",
        "    (16, 128),                                    # neurons (integer)\n",
        "    (1, 4),                                       # layers (integer)\n",
        "    (\"Tanh\", \"ReLU\", \"Sigmoid\", \"GELU\"),       # activation (factor)\n",
        "    (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),        # optimizer (factor)\n",
        "    (0.1, 10.0),                                  # learning rate (log-scale)\n",
        "    (0.01, 1.0)                                   # physics weight (log-scale)\n",
        "]\n",
        "\n",
        "var_type = [\"int\", \"int\", \"factor\", \"factor\", \"num\", \"num\"]\n",
        "var_trans = [None, None, None, None, \"log10\", \"log10\"]\n",
        "\n",
        "opt = SpotOptim(\n",
        "    fun=your_objective_function,\n",
        "    bounds=bounds,\n",
        "    var_type=var_type,\n",
        "    var_trans=var_trans,  # Automatic log-scale and factor handling!\n",
        "    max_iter=MAX_ITER,\n",
        "    n_initial=10\n",
        ")\n",
        "\n",
        "result = opt.optimize()\n",
        "```\n",
        "\n",
        "Your objective function receives parameters in **original scale** - no manual transformations needed!\n",
        "\n",
        "## Future Directions\n",
        "\n",
        "Consider exploring:\n",
        "\n",
        "1. **Adaptive physics weights** that change during training\n",
        "2. **Architecture search** including skip connections or residual blocks\n",
        "3. **Batch size optimization** as an additional hyperparameter\n",
        "4. **Multi-objective optimization** balancing accuracy and computational cost\n",
        "5. **Transfer learning** from pre-optimized configurations\n",
        "6. **Learning rate schedules** with different decay strategies\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: The specific optimal values depend on the problem, data distribution, and computational budget. Always validate results on held-out test data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spotoptim",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
