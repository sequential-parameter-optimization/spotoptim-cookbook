{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Variable Transformations for Search Space Scaling\n",
        "sidebar_position: 6\n",
        "eval: true\n",
        "---\n",
        "\n",
        "SpotOptim supports automatic variable transformations to improve optimization in scaled search spaces. Instead of manually handling transformations (e.g., log-scale for learning rates), you can specify transformations via the `var_trans` parameter, and SpotOptim handles everything internally.\n",
        "\n",
        "## Overview\n",
        "\n",
        "**What are Variable Transformations?**\n",
        "\n",
        "Variable transformations allow you to specify how search space dimensions should be scaled during optimization:\n",
        "\n",
        "1. **Original scale** (user interface): Input bounds, output results, plots\n",
        "2. **Transformed scale** (internal): Surrogate modeling, acquisition optimization\n",
        "3. **Automatic conversion**: SpotOptim handles all transformations transparently\n",
        "\n",
        "**Module**: `spotoptim.SpotOptim`\n",
        "\n",
        "**Key Features**:\n",
        "\n",
        "- Define transformations via `var_trans` parameter: `[\"log10\", \"sqrt\", None, ...]`\n",
        "- Optimization occurs in transformed space (better for surrogate models)\n",
        "- All external interfaces use original scale (intuitive for users)\n",
        "- Supported transformations: log10, log/ln, sqrt, exp, square, cube, inv/reciprocal\n",
        "- Mix transformed and non-transformed variables\n",
        "\n",
        "## Why Use Transformations?\n",
        "\n",
        "### Problem: Poorly Scaled Search Spaces\n",
        "\n",
        "Some hyperparameters span multiple orders of magnitude:\n",
        "\n",
        "- **Learning rates**: 0.0001 to 1.0 (4 orders of magnitude)\n",
        "- **Regularization**: 0.001 to 100 (5 orders of magnitude)\n",
        "- **Network sizes**: 10 to 1000 neurons\n",
        "\n",
        "Direct optimization in these spaces is inefficient because:\n",
        "\n",
        "1. Surrogate models struggle with extreme scales\n",
        "2. Uniform sampling wastes evaluations in unimportant regions\n",
        "3. Acquisition functions behave poorly with skewed distributions\n",
        "\n",
        "### Solution: Logarithmic and Other Transformations\n",
        "\n",
        "Transform the space for optimization while maintaining user-friendly interfaces:\n",
        "\n",
        "```python\n",
        "# Without transformations (manual approach)\n",
        "bounds = [(-4, 0)]  # log10(lr): awkward for users\n",
        "lr = 10 ** params[0]  # Manual transformation in objective\n",
        "\n",
        "# With transformations (automatic)\n",
        "bounds = [(0.0001, 1.0)]  # lr in natural scale\n",
        "var_trans = [\"log10\"]  # SpotOptim handles transformation\n",
        "lr = params[0]  # Already in original scale!\n",
        "```\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "### Basic Log-Scale Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def objective_function(X):\n",
        "    \"\"\"Objective receives parameters in ORIGINAL scale.\"\"\"\n",
        "    results = []\n",
        "    for params in X:\n",
        "        lr = params[0]  # Already in [0.001, 0.1] - original scale!\n",
        "        alpha = params[1]  # Already in [0.01, 1.0] - original scale!\n",
        "        \n",
        "        # Simulate model training\n",
        "        score = (lr - 0.01)**2 + (alpha - 0.1)**2 + np.random.normal(0, 0.01)\n",
        "        results.append(score)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Create optimizer with transformations\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective_function,\n",
        "    bounds=[\n",
        "        (0.001, 0.1),    # learning rate (original scale)\n",
        "        (0.01, 1.0)      # alpha (original scale)\n",
        "    ],\n",
        "    var_trans=[\"log10\", \"log10\"],  # Both use log10 transformation\n",
        "    var_name=[\"lr\", \"alpha\"],\n",
        "    max_iter=20,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Run optimization\n",
        "result = optimizer.optimize()\n",
        "\n",
        "print(f\"Best lr: {result.x[0]:.6f}\")      # In original scale\n",
        "print(f\"Best alpha: {result.x[1]:.6f}\")   # In original scale\n",
        "print(f\"Best score: {result.fun:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supported Transformations\n",
        "\n",
        "SpotOptim supports the following transformations:\n",
        "\n",
        "| Transformation | Forward (x → t) | Inverse (t → x) | Use Case |\n",
        "|----------------|-----------------|-----------------|----------|\n",
        "| `\"log10\"` | t = log₁₀(x) | x = 10^t | Learning rates, regularization |\n",
        "| `\"log\"` or `\"ln\"` | t = ln(x) | x = e^t | Natural exponential scales |\n",
        "| `\"sqrt\"` | t = √x | x = t² | Moderate scaling |\n",
        "| `\"exp\"` | t = e^x | x = ln(t) | Inverse of natural log |\n",
        "| `\"square\"` | t = x² | x = √t | Inverse of sqrt |\n",
        "| `\"cube\"` | t = x³ | x = ∛t | Strong scaling |\n",
        "| `\"inv\"` or `\"reciprocal\"` | t = 1/x | x = 1/t | Reciprocal relationships |\n",
        "| `None` or `\"id\"` | t = x | x = t | No transformation |\n",
        "\n",
        "### Transformation Guidelines\n",
        "\n",
        "**When to use `\"log10\"` or `\"log\"`:**\n",
        "\n",
        "- Parameters spanning multiple orders of magnitude\n",
        "- Learning rates: `(1e-5, 1e-1)` → uniform sampling in log space\n",
        "- Regularization parameters: `(1e-6, 1e2)`\n",
        "- Batch sizes, hidden units when range is large\n",
        "\n",
        "**When to use `\"sqrt\"`:**\n",
        "\n",
        "- Moderate scaling (1-2 orders of magnitude)\n",
        "- Batch sizes: `(16, 512)`\n",
        "- Number of neurons: `(32, 256)`\n",
        "\n",
        "**When to use `\"inv\"` (reciprocal):**\n",
        "\n",
        "- Inverse relationships (e.g., 1/temperature)\n",
        "- When smaller values are more important\n",
        "\n",
        "**When to use `None`:**\n",
        "\n",
        "- Parameters with narrow ranges\n",
        "- Already well-scaled parameters\n",
        "- Categorical indices (use with `var_type=[\"factor\"]`)\n",
        "\n",
        "## Detailed Examples\n",
        "\n",
        "### Example 1: Neural Network Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.data import get_diabetes_dataloaders\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "import numpy as np\n",
        "\n",
        "def train_neural_network(X):\n",
        "    \"\"\"Train neural network with hyperparameters in original scale.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        # All parameters in original scale\n",
        "        hidden_size = int(params[0])  # [16, 256]\n",
        "        num_layers = int(params[1])   # [1, 4]\n",
        "        lr = params[2]                # [0.0001, 0.1]\n",
        "        weight_decay = params[3]      # [1e-6, 0.01]\n",
        "        \n",
        "        print(f\"Training: hidden={hidden_size}, layers={num_layers}, \"\n",
        "              f\"lr={lr:.6f}, wd={weight_decay:.6f}\")\n",
        "        \n",
        "        # Load data\n",
        "        train_loader, test_loader, _ = get_diabetes_dataloaders(batch_size=32)\n",
        "        \n",
        "        # Create model\n",
        "        model = LinearRegressor(\n",
        "            input_dim=10,\n",
        "            output_dim=1,\n",
        "            l1=hidden_size,\n",
        "            num_hidden_layers=num_layers,\n",
        "            activation=\"ReLU\",\n",
        "            lr=lr\n",
        "        )\n",
        "        \n",
        "        # Get optimizer with weight decay\n",
        "        optimizer = torch.optim.Adam(model.parameters(), \n",
        "                                     lr=lr, \n",
        "                                     weight_decay=weight_decay)\n",
        "        \n",
        "        # Train\n",
        "        model.train()\n",
        "        for epoch in range(50):\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_x)\n",
        "                loss = nn.MSELoss()(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader:\n",
        "                outputs = model(batch_x)\n",
        "                loss = nn.MSELoss()(outputs, batch_y)\n",
        "                total_loss += loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / len(test_loader)\n",
        "        results.append(avg_loss)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Create optimizer with appropriate transformations\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_neural_network,\n",
        "    bounds=[\n",
        "        (16, 256),           # hidden_size: moderate range\n",
        "        (1, 4),              # num_layers: small range\n",
        "        (0.0001, 0.1),       # lr: 3 orders of magnitude\n",
        "        (1e-6, 0.01)         # weight_decay: 4 orders of magnitude\n",
        "    ],\n",
        "    var_trans=[\n",
        "        \"sqrt\",              # sqrt for hidden_size\n",
        "        None,                # no transformation for num_layers\n",
        "        \"log10\",             # log10 for learning rate\n",
        "        \"log10\"              # log10 for weight_decay\n",
        "    ],\n",
        "    var_type=[\"int\", \"int\", \"float\", \"float\"],\n",
        "    var_name=[\"hidden_size\", \"num_layers\", \"lr\", \"weight_decay\"],\n",
        "    max_iter=30,\n",
        "    n_initial=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "\n",
        "print(\"\\nBest Configuration:\")\n",
        "print(f\"  Hidden Size: {int(result.x[0])}\")\n",
        "print(f\"  Num Layers: {int(result.x[1])}\")\n",
        "print(f\"  Learning Rate: {result.x[2]:.6f}\")\n",
        "print(f\"  Weight Decay: {result.x[3]:.8f}\")\n",
        "print(f\"  Best Loss: {result.fun:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Physics-Informed Neural Networks (PINNs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def train_pinn(X):\n",
        "    \"\"\"Train PINN with hyperparameters in original scale.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        neurons = int(params[0])      # [16, 128]\n",
        "        layers = int(params[1])       # [1, 4]\n",
        "        lr = params[2]                # [0.1, 10.0]\n",
        "        alpha = params[3]             # [0.01, 1.0]\n",
        "        \n",
        "        # Simulate PINN training\n",
        "        # In practice, this would train an actual PINN model\n",
        "        val_error = 0.1 * (1/lr) + 0.05 * alpha + np.random.normal(0, 0.01)\n",
        "        results.append(val_error)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Define optimization with transformations\n",
        "optimizer = SpotOptim(\n",
        "    fun=train_pinn,\n",
        "    bounds=[\n",
        "        (16, 128),           # neurons\n",
        "        (1, 4),              # layers\n",
        "        (0.1, 10.0),         # lr: covers 2 orders of magnitude\n",
        "        (0.01, 1.0)          # alpha: covers 2 orders of magnitude\n",
        "    ],\n",
        "    var_trans=[None, None, \"log10\", \"log10\"],\n",
        "    var_type=[\"int\", \"int\", \"float\", \"float\"],\n",
        "    var_name=[\"neurons\", \"layers\", \"lr\", \"alpha\"],\n",
        "    max_iter=20,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "optimizer.print_best(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Mixing Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "def complex_objective(X):\n",
        "    \"\"\"Objective with multiple transformation types.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for params in X:\n",
        "        # All in original scale\n",
        "        x1 = params[0]  # sqrt-transformed: [10, 1000]\n",
        "        x2 = params[1]  # log10-transformed: [0.001, 1.0]\n",
        "        x3 = params[2]  # no transformation: [-5, 5]\n",
        "        x4 = params[3]  # reciprocal: [0.1, 10]\n",
        "        \n",
        "        # Complex function\n",
        "        result = (\n",
        "            (x1/100 - 5)**2 + \n",
        "            (np.log10(x2) + 1.5)**2 + \n",
        "            x3**2 + \n",
        "            (1/x4 - 0.2)**2\n",
        "        )\n",
        "        results.append(result)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "optimizer = SpotOptim(\n",
        "    fun=complex_objective,\n",
        "    bounds=[\n",
        "        (10, 1000),      # x1: moderate range\n",
        "        (0.001, 1.0),    # x2: log scale\n",
        "        (-5, 5),         # x3: symmetric range\n",
        "        (0.1, 10)        # x4: for reciprocal\n",
        "    ],\n",
        "    var_trans=[\"sqrt\", \"log10\", None, \"inv\"],\n",
        "    var_name=[\"x1_sqrt\", \"x2_log\", \"x3_linear\", \"x4_inv\"],\n",
        "    max_iter=30,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing Transformations in Tables\n",
        "\n",
        "The transformation type is displayed in the \"trans\" column of both design and results tables:\n",
        "\n",
        "### Design Table (Before Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from spotoptim import SpotOptim\n",
        "import numpy as np\n",
        "\n",
        "optimizer = SpotOptim(\n",
        "    fun=lambda X: np.sum(X**2, axis=1),\n",
        "    bounds=[\n",
        "        (0.001, 1.0),\n",
        "        (0.01, 10.0),\n",
        "        (10, 1000),\n",
        "        (-5, 5)\n",
        "    ],\n",
        "    var_trans=[\"log10\", \"log\", \"sqrt\", None],\n",
        "    var_name=[\"lr\", \"alpha\", \"neurons\", \"bias\"],\n",
        "    max_iter=10\n",
        ")\n",
        "\n",
        "# Display design table\n",
        "print(optimizer.print_design_table())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "```\n",
        "| name    | type   |    lower |    upper |   default | trans   |\n",
        "|---------|--------|----------|----------|-----------|---------|\n",
        "| lr      | num    |   0.0010 |   1.0000 |    0.5005 | log10   |\n",
        "| alpha   | num    |   0.0100 |  10.0000 |    5.0050 | log     |\n",
        "| neurons | num    |  10.0000 | 1000.0000 |  505.0000 | sqrt    |\n",
        "| bias    | num    |  -5.0000 |   5.0000 |    0.0000 | -       |\n",
        "```\n",
        "\n",
        "### Results Table (After Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = optimizer.optimize()\n",
        "\n",
        "# Display results with transformations\n",
        "print(optimizer.print_results_table())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output shows the \"trans\" column with transformation types, helping you understand which parameters were optimized in which scale.\n",
        "\n",
        "## Internal Architecture\n",
        "\n",
        "Understanding how transformations work internally can help debug issues and understand behavior:\n",
        "\n",
        "### Flow Diagram\n",
        "\n",
        "```\n",
        "User Input (Original Scale)\n",
        "    ↓\n",
        "[Transform to Internal Scale]\n",
        "    ↓\n",
        "Optimization (Transformed Scale)\n",
        "  • Initial design generation\n",
        "  • Surrogate model fitting\n",
        "  • Acquisition function optimization\n",
        "    ↓\n",
        "[Inverse Transform to Original Scale]\n",
        "    ↓\n",
        "Objective Function Evaluation (Original Scale)\n",
        "    ↓\n",
        "Storage & Results (Original Scale)\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **Bounds Transformation** (`_transform_bounds()`):\n",
        "\n",
        "   - Called during initialization\n",
        "   - Transforms `_original_lower` and `_original_upper` → `lower` and `upper`\n",
        "   - Updates `self.bounds` for internal use\n",
        "\n",
        "2. **Forward Transformation** (`_transform_X()`):\n",
        "\n",
        "   - Converts from original scale to transformed scale\n",
        "   - Used before surrogate fitting\n",
        "   - Used when comparing distances\n",
        "\n",
        "3. **Inverse Transformation** (`_inverse_transform_X()`):\n",
        "\n",
        "   - Converts from transformed scale to original scale\n",
        "   - Used before function evaluation\n",
        "   - Used when storing results\n",
        "\n",
        "4. **Storage**:\n",
        "\n",
        "   - `self.X_` stores in **original scale**\n",
        "   - `self.best_x_` stores in **original scale**\n",
        "   - All external-facing data in original scale\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "### 1. Choose Appropriate Transformations\n",
        "\n",
        "```python\n",
        "# Good: Log scale for learning rate\n",
        "bounds = [(1e-5, 1e-1)]\n",
        "var_trans = [\"log10\"]\n",
        "\n",
        "# Bad: No transformation for wide range\n",
        "bounds = [(1e-5, 1e-1)]\n",
        "var_trans = [None]  # Poor sampling distribution\n",
        "```\n",
        "\n",
        "### 2. Match Transformation to Range\n",
        "\n",
        "```python\n",
        "# Wide range (>3 orders of magnitude): use log\n",
        "bounds = [(1e-6, 1e-2)]\n",
        "var_trans = [\"log10\"]\n",
        "\n",
        "# Moderate range (1-2 orders): use sqrt\n",
        "bounds = [(10, 500)]\n",
        "var_trans = [\"sqrt\"]\n",
        "\n",
        "# Narrow range (<1 order): no transformation\n",
        "bounds = [(-1, 1)]\n",
        "var_trans = [None]\n",
        "```\n",
        "\n",
        "### 3. Validate Transformation Choice\n",
        "\n",
        "```python\n",
        "# Check if transformation makes sense\n",
        "import numpy as np\n",
        "\n",
        "# Original space\n",
        "x_orig = np.linspace(0.001, 1.0, 10)\n",
        "print(\"Original:\", x_orig)\n",
        "\n",
        "# Log10 transformed space\n",
        "x_trans = np.log10(x_orig)\n",
        "print(\"Transformed:\", x_trans)\n",
        "print(\"Range ratio:\", np.ptp(x_trans) / np.ptp(x_orig))\n",
        "# Should be much more uniform distribution\n",
        "```\n",
        "\n",
        "### 4. Combine with Variable Types\n",
        "\n",
        "```python\n",
        "# Mix transformations with variable types\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective,\n",
        "    bounds=[\n",
        "        (10, 200),                          # int with sqrt\n",
        "        (\"ReLU\", \"Tanh\", \"Sigmoid\"),        # factor (no transform)\n",
        "        (0.0001, 0.1),                      # num with log10\n",
        "        (0.01, 1.0)                         # num with log10\n",
        "    ],\n",
        "    var_type=[\"int\", \"factor\", \"float\", \"float\"],\n",
        "    var_trans=[\"sqrt\", None, \"log10\", \"log10\"],\n",
        "    var_name=[\"neurons\", \"activation\", \"lr\", \"dropout\"]\n",
        ")\n",
        "```\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Issue: Values Out of Bounds\n",
        "\n",
        "**Problem**: Objective function receives values outside specified bounds.\n",
        "\n",
        "**Solution**: This should not happen with transformations. If it does:\n",
        "\n",
        "```python\n",
        "# Check transformation is applied correctly\n",
        "print(f\"Original bounds: {optimizer._original_lower} to {optimizer._original_upper}\")\n",
        "print(f\"Transformed bounds: {optimizer.lower} to {optimizer.upper}\")\n",
        "print(f\"Transformations: {optimizer.var_trans}\")\n",
        "```\n",
        "\n",
        "### Issue: Poor Optimization Performance\n",
        "\n",
        "**Problem**: Optimization doesn't find good solutions.\n",
        "\n",
        "**Possible causes**:\n",
        "\n",
        "1. Wrong transformation type for the parameter scale\n",
        "2. Transformation not needed (adding unnecessary complexity)\n",
        "3. Bounds too wide or too narrow\n",
        "\n",
        "**Solution**:\n",
        "```python\n",
        "# Try different transformations\n",
        "for trans in [None, \"log10\", \"sqrt\"]:\n",
        "    optimizer = SpotOptim(\n",
        "        fun=objective,\n",
        "        bounds=[(0.001, 1.0)],\n",
        "        var_trans=[trans],\n",
        "        max_iter=20,\n",
        "        seed=42\n",
        "    )\n",
        "    result = optimizer.optimize()\n",
        "    print(f\"Transformation: {trans}, Best: {result.fun:.6f}\")\n",
        "```\n",
        "\n",
        "### Issue: Transformation Not Applied\n",
        "\n",
        "**Problem**: Transformation doesn't seem to affect optimization.\n",
        "\n",
        "**Check**:\n",
        "```python\n",
        "# Verify var_trans length matches dimensions\n",
        "print(f\"Number of dimensions: {len(optimizer.bounds)}\")\n",
        "print(f\"Number of transformations: {len(optimizer.var_trans)}\")\n",
        "# These must match!\n",
        "\n",
        "# Check transformation is not None/\"id\"\n",
        "print(f\"Transformations: {optimizer.var_trans}\")\n",
        "```\n",
        "\n",
        "## Comparison: Manual vs Automatic Transformations\n",
        "\n",
        "### Manual Approach (Old Way)\n",
        "\n",
        "```python\n",
        "def objective_manual(X):\n",
        "    \"\"\"Manual transformation - error-prone!\"\"\"\n",
        "    results = []\n",
        "    for params in X:\n",
        "        # Must remember to transform\n",
        "        lr = 10 ** params[0]  # Was in log scale\n",
        "        alpha = 10 ** params[1]  # Was in log scale\n",
        "        \n",
        "        # Use parameters\n",
        "        score = compute_score(lr, alpha)\n",
        "        results.append(score)\n",
        "    return np.array(results)\n",
        "\n",
        "# Bounds in log scale - confusing!\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective_manual,\n",
        "    bounds=[(-4, -1), (-2, 0)],  # log10 scale\n",
        "    var_name=[\"log10_lr\", \"log10_alpha\"]  # Confusing names\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "# Must transform back for interpretation\n",
        "best_lr = 10 ** result.x[0]\n",
        "best_alpha = 10 ** result.x[1]\n",
        "```\n",
        "\n",
        "### Automatic Approach (New Way)\n",
        "\n",
        "```python\n",
        "def objective_auto(X):\n",
        "    \"\"\"Automatic transformation - clean!\"\"\"\n",
        "    results = []\n",
        "    for params in X:\n",
        "        # Already in original scale\n",
        "        lr = params[0]\n",
        "        alpha = params[1]\n",
        "        \n",
        "        # Use parameters directly\n",
        "        score = compute_score(lr, alpha)\n",
        "        results.append(score)\n",
        "    return np.array(results)\n",
        "\n",
        "# Bounds in natural scale - intuitive!\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective_auto,\n",
        "    bounds=[(0.0001, 0.1), (0.01, 1.0)],\n",
        "    var_trans=[\"log10\", \"log10\"],  # Specify transformation\n",
        "    var_name=[\"lr\", \"alpha\"]  # Natural names\n",
        ")\n",
        "\n",
        "result = optimizer.optimize()\n",
        "# Results already in original scale\n",
        "best_lr = result.x[0]\n",
        "best_alpha = result.x[1]\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "**Key Takeaways**:\n",
        "\n",
        "1. ✅ Use `var_trans` to specify transformations for each dimension\n",
        "2. ✅ Transformations improve optimization for poorly scaled spaces\n",
        "3. ✅ All user interfaces (bounds, results, plots) use original scale\n",
        "4. ✅ Optimization happens internally in transformed space\n",
        "5. ✅ Common transformations: `\"log10\"` for learning rates, `\"sqrt\"` for moderate scaling\n",
        "6. ✅ View transformations in tables with \"trans\" column\n",
        "\n",
        "**When to Use**:\n",
        "\n",
        "- Parameters spanning multiple orders of magnitude → `\"log10\"` or `\"log\"`\n",
        "- Moderate scaling (1-2 orders) → `\"sqrt\"`\n",
        "- Reciprocal relationships → `\"inv\"`\n",
        "- Well-scaled parameters → `None`\n",
        "\n",
        "**Benefits**:\n",
        "\n",
        "- Better surrogate model performance\n",
        "- More efficient sampling\n",
        "- Improved optimization convergence\n",
        "- User-friendly interface (no manual transformations in objective function)\n",
        "\n",
        "---\n",
        "\n",
        "**See Also**:\n",
        "\n",
        "- [Variable Types Manual](var_type.md) - Integer, numeric, and factor types\n",
        "- [Factor Variables Manual](factor_variables.md) - Categorical optimization\n",
        "- [Reproducibility Manual](reproducibility.md) - Setting seeds for consistent results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/bartz/workspace/spotoptim-cookbook/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}